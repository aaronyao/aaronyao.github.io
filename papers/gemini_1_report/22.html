<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p22" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_22{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_22{left:115px;bottom:1132px;letter-spacing:-0.14px;}
#t3_22{left:137px;bottom:1132px;letter-spacing:-0.21px;}
#t4_22{left:203px;bottom:1132px;letter-spacing:-0.19px;word-spacing:1.1px;}
#t5_22{left:137px;bottom:1111px;letter-spacing:-0.21px;word-spacing:0.61px;}
#t6_22{left:137px;bottom:1090px;letter-spacing:-0.17px;word-spacing:0.35px;}
#t7_22{left:793px;bottom:1090px;letter-spacing:-0.22px;}
#t8_22{left:137px;bottom:1070px;letter-spacing:-0.22px;word-spacing:0.68px;}
#t9_22{left:177px;bottom:1070px;}
#ta_22{left:184px;bottom:1070px;letter-spacing:-0.19px;}
#tb_22{left:221px;bottom:1070px;letter-spacing:-0.2px;word-spacing:0.6px;}
#tc_22{left:137px;bottom:1049px;letter-spacing:-0.22px;word-spacing:0.62px;}
#td_22{left:95px;bottom:1011px;letter-spacing:-0.19px;word-spacing:0.56px;}
#te_22{left:95px;bottom:991px;letter-spacing:-0.19px;word-spacing:0.63px;}
#tf_22{left:95px;bottom:970px;letter-spacing:-0.16px;word-spacing:0.59px;}
#tg_22{left:539px;bottom:970px;letter-spacing:-0.21px;word-spacing:0.68px;}
#th_22{left:630px;bottom:970px;}
#ti_22{left:640px;bottom:970px;letter-spacing:-0.19px;}
#tj_22{left:676px;bottom:970px;}
#tk_22{left:686px;bottom:970px;letter-spacing:-0.26px;word-spacing:0.79px;}
#tl_22{left:760px;bottom:970px;}
#tm_22{left:769px;bottom:970px;letter-spacing:-0.19px;}
#tn_22{left:806px;bottom:970px;letter-spacing:-0.12px;}
#to_22{left:95px;bottom:949px;letter-spacing:-0.27px;word-spacing:0.72px;}
#tp_22{left:115px;bottom:912px;letter-spacing:-0.14px;}
#tq_22{left:137px;bottom:912px;letter-spacing:-0.2px;word-spacing:1.56px;}
#tr_22{left:257px;bottom:912px;letter-spacing:-0.16px;word-spacing:1.23px;}
#ts_22{left:136px;bottom:891px;letter-spacing:-0.17px;word-spacing:0.57px;}
#tt_22{left:137px;bottom:870px;letter-spacing:-0.16px;word-spacing:0.49px;}
#tu_22{left:115px;bottom:849px;letter-spacing:-0.14px;}
#tv_22{left:137px;bottom:849px;letter-spacing:-0.21px;word-spacing:0.77px;}
#tw_22{left:262px;bottom:849px;letter-spacing:-0.18px;word-spacing:0.51px;}
#tx_22{left:137px;bottom:829px;letter-spacing:-0.18px;word-spacing:0.8px;}
#ty_22{left:137px;bottom:808px;letter-spacing:-0.18px;word-spacing:0.48px;}
#tz_22{left:713px;bottom:808px;letter-spacing:-0.23px;word-spacing:0.58px;}
#t10_22{left:812px;bottom:808px;}
#t11_22{left:136px;bottom:787px;letter-spacing:-0.19px;}
#t12_22{left:174px;bottom:787px;letter-spacing:-0.12px;}
#t13_22{left:115px;bottom:767px;letter-spacing:-0.14px;}
#t14_22{left:137px;bottom:767px;letter-spacing:-0.17px;word-spacing:0.55px;}
#t15_22{left:240px;bottom:767px;letter-spacing:-0.21px;word-spacing:0.46px;}
#t16_22{left:137px;bottom:746px;letter-spacing:-0.36px;}
#t17_22{left:95px;bottom:708px;letter-spacing:-0.2px;word-spacing:0.57px;}
#t18_22{left:95px;bottom:688px;letter-spacing:-0.31px;word-spacing:0.84px;}
#t19_22{left:297px;bottom:688px;letter-spacing:-0.19px;}
#t1a_22{left:315px;bottom:688px;letter-spacing:-0.18px;word-spacing:0.66px;}
#t1b_22{left:95px;bottom:667px;letter-spacing:-0.19px;word-spacing:0.52px;}
#t1c_22{left:95px;bottom:646px;letter-spacing:-0.19px;word-spacing:0.55px;}
#t1d_22{left:359px;bottom:609px;letter-spacing:0.04px;word-spacing:0.53px;}
#t1e_22{left:358px;bottom:596px;letter-spacing:0.09px;word-spacing:0.32px;}
#t1f_22{left:507px;bottom:609px;letter-spacing:0.03px;word-spacing:0.55px;}
#t1g_22{left:506px;bottom:596px;letter-spacing:-0.06px;}
#t1h_22{left:655px;bottom:609px;letter-spacing:0.11px;word-spacing:0.43px;}
#t1i_22{left:654px;bottom:596px;letter-spacing:0.01px;}
#t1j_22{left:124px;bottom:568px;letter-spacing:0.11px;word-spacing:0.43px;}
#t1k_22{left:124px;bottom:555px;letter-spacing:0.07px;word-spacing:0.36px;}
#t1l_22{left:359px;bottom:568px;letter-spacing:0.12px;}
#t1m_22{left:358px;bottom:555px;letter-spacing:0.09px;word-spacing:0.34px;}
#t1n_22{left:507px;bottom:568px;letter-spacing:0.12px;}
#t1o_22{left:507px;bottom:555px;letter-spacing:0.09px;word-spacing:0.34px;}
#t1p_22{left:655px;bottom:568px;letter-spacing:0.16px;}
#t1q_22{left:124px;bottom:521px;letter-spacing:0.11px;word-spacing:0.43px;}
#t1r_22{left:124px;bottom:507px;letter-spacing:0.04px;word-spacing:0.4px;}
#t1s_22{left:359px;bottom:521px;letter-spacing:0.12px;}
#t1t_22{left:358px;bottom:507px;letter-spacing:0.09px;word-spacing:0.34px;}
#t1u_22{left:507px;bottom:521px;letter-spacing:0.12px;}
#t1v_22{left:507px;bottom:507px;letter-spacing:0.09px;word-spacing:0.34px;}
#t1w_22{left:655px;bottom:521px;letter-spacing:0.12px;}
#t1x_22{left:95px;bottom:466px;letter-spacing:-0.6px;word-spacing:1.37px;}
#t1y_22{left:161px;bottom:466px;}
#t1z_22{left:170px;bottom:466px;letter-spacing:-0.23px;word-spacing:0.72px;}
#t20_22{left:95px;bottom:445px;letter-spacing:-0.19px;word-spacing:0.55px;}
#t21_22{left:95px;bottom:412px;letter-spacing:-0.19px;word-spacing:4.19px;}
#t22_22{left:95px;bottom:378px;letter-spacing:-0.24px;word-spacing:0.58px;}
#t23_22{left:95px;bottom:357px;letter-spacing:-0.17px;word-spacing:0.53px;}
#t24_22{left:95px;bottom:337px;letter-spacing:-0.23px;word-spacing:0.63px;}
#t25_22{left:95px;bottom:288px;letter-spacing:-0.31px;word-spacing:2.71px;}
#t26_22{left:95px;bottom:254px;letter-spacing:-0.18px;word-spacing:0.6px;}
#t27_22{left:95px;bottom:234px;letter-spacing:-0.15px;word-spacing:0.42px;}
#t28_22{left:484px;bottom:242px;letter-spacing:0.1px;}
#t29_22{left:503px;bottom:234px;letter-spacing:-0.16px;word-spacing:0.45px;}
#t2a_22{left:95px;bottom:213px;letter-spacing:-0.17px;word-spacing:2.14px;}
#t2b_22{left:95px;bottom:192px;letter-spacing:-0.21px;word-spacing:1.84px;}
#t2c_22{left:95px;bottom:171px;letter-spacing:-0.19px;word-spacing:0.86px;}
#t2d_22{left:95px;bottom:151px;letter-spacing:-0.09px;word-spacing:0.38px;}
#t2e_22{left:107px;bottom:130px;letter-spacing:0.06px;}
#t2f_22{left:120px;bottom:124px;letter-spacing:0.23px;}
#t2g_22{left:801px;bottom:68px;letter-spacing:0.1px;}

.s0_22{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_22{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s2_22{font-size:17px;font-family:XCharter-Bold_vg;color:#000;}
.s3_22{font-size:17px;font-family:XCharter-Roman_vh;color:#00F;}
.s4_22{font-size:15px;font-family:XCharter-Roman_vh;color:#000;}
.s5_22{font-size:17px;font-family:txsys_6y;color:#000;}
.s6_22{font-size:12px;font-family:XCharter-Roman_vh;color:#00F;}
.s7_22{font-size:11px;font-family:XCharter-Roman_vh;color:#000;}
.s8_22{font-size:13px;font-family:LMMono8-Regular_6u;color:#00F;}
.t.v0_22{transform:scaleX(1.02);}
.t.v1_22{transform:scaleX(1.019);}
.t.v2_22{transform:scaleX(0.979);}
.t.v3_22{transform:scaleX(0.993);}
.t.v4_22{transform:scaleX(0.982);}
.t.v5_22{transform:scaleX(0.987);}
.t.v6_22{transform:scaleX(1.007);}
.t.v7_22{transform:scaleX(1.013);}
.t.v8_22{transform:scaleX(0.99);}
.t.v9_22{transform:scaleX(1.005);}
.t.v10_22{transform:scaleX(1.014);}
.t.v11_22{transform:scaleX(0.983);}
.t.v12_22{transform:scaleX(1.008);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts22" type="text/css" >

@font-face {
	font-family: LMMono8-Regular_6u;
	src: url("fonts/LMMono8-Regular_6u.woff") format("woff");
}

@font-face {
	font-family: XCharter-Bold_vg;
	src: url("fonts/XCharter-Bold_vg.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

@font-face {
	font-family: txsys_6y;
	src: url("fonts/txsys_6y.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg22Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg22" style="-webkit-user-select: none;"><object width="909" height="1286" data="22/22.svg" type="image/svg+xml" id="pdf22" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_22" class="t s0_22">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_22" class="t s1_22">3. </span><span id="t3_22" class="t v0_22 s2_22">Hedging</span><span id="t4_22" class="t v0_22 s1_22">: If prompted with an input such that it is “unanswerable”, Gemini should not hal- </span>
<span id="t5_22" class="t v1_22 s1_22">lucinate. Rather, it should acknowledge that it cannot provide a response by hedging. These </span>
<span id="t6_22" class="t v2_22 s1_22">include scenarios where the input prompt contains false-premise questions (see examples in </span><span id="t7_22" class="t v2_22 s3_22">Hu </span>
<span id="t8_22" class="t v3_22 s3_22">et al. </span><span id="t9_22" class="t v3_22 s1_22">(</span><span id="ta_22" class="t v3_22 s3_22">2023</span><span id="tb_22" class="t v3_22 s1_22">)), the input prompt instructs the model to perform open-book QA, but the answer </span>
<span id="tc_22" class="t s1_22">is not derivable from the given context, and so forth. </span>
<span id="td_22" class="t v3_22 s1_22" data-mappings='[[87,"fi"]]'>We elicited these desired behaviors from Gemini models by curating targeted supervised-ﬁne tuning </span>
<span id="te_22" class="t v4_22 s1_22">datasets and performing RLHF. Note that the results produced here do not include endowing Gemini </span>
<span id="tf_22" class="t v2_22 s1_22">with tools or retrieval that purportedly could boost factuality (</span><span id="tg_22" class="t v2_22 s3_22">Menick et al.</span><span id="th_22" class="t v2_22 s1_22">, </span><span id="ti_22" class="t v2_22 s3_22">2022</span><span id="tj_22" class="t v2_22 s1_22">; </span><span id="tk_22" class="t v2_22 s3_22">Peng et al.</span><span id="tl_22" class="t v2_22 s1_22">, </span><span id="tm_22" class="t v2_22 s3_22">2023</span><span id="tn_22" class="t v2_22 s1_22">). </span>
<span id="to_22" class="t s1_22">We provide three key results on respective challenge sets below. </span>
<span id="tp_22" class="t s1_22">1. </span><span id="tq_22" class="t v0_22 s2_22">Factuality Set: </span><span id="tr_22" class="t v0_22 s1_22">An evaluation set containing fact-seeking prompts (primarily closed-book). </span>
<span id="ts_22" class="t v5_22 s1_22">This is evaluated via human annotators who fact-check each response manually; we report the </span>
<span id="tt_22" class="t s1_22">percentage of factually-inaccurate responses as judged by annotators. </span>
<span id="tu_22" class="t s1_22">2. </span><span id="tv_22" class="t v6_22 s2_22">Attribution Set: </span><span id="tw_22" class="t v6_22 s1_22">An evaluation set containing a variety of prompts that require attribution to </span>
<span id="tx_22" class="t v0_22 s1_22">sources in the prompt. This is evaluated via human annotators who check for attribution to </span>
<span id="ty_22" class="t v7_22 s1_22">sources in the prompt for each response manually; the reported metric is AIS (</span><span id="tz_22" class="t v7_22 s3_22">Rashkin et al.</span><span id="t10_22" class="t v7_22 s1_22">, </span>
<span id="t11_22" class="t s3_22">2023</span><span id="t12_22" class="t s1_22">). </span>
<span id="t13_22" class="t s1_22">3. </span><span id="t14_22" class="t v2_22 s2_22">Hedging Set: </span><span id="t15_22" class="t v2_22 s1_22">An automatic evaluation setup where we measure whether Gemini models hedge </span>
<span id="t16_22" class="t s1_22">accurately. </span>
<span id="t17_22" class="t v2_22 s1_22">We compare Gemini Pro with a version of instruction-tuned Gemini Pro model without any factuality- </span>
<span id="t18_22" class="t v8_22 s1_22">focused adaptation in Table </span><span id="t19_22" class="t v8_22 s3_22">14</span><span id="t1a_22" class="t v8_22 s1_22">. We observe that the rate of inaccuracy is halved in the factuality set, </span>
<span id="t1b_22" class="t v9_22 s1_22">the accuracy of attribution is increased by 50% from the attribution set, and the model successfully </span>
<span id="t1c_22" class="t s1_22">hedges 70% (up from 0%) in the provided hedging set task. </span>
<span id="t1d_22" class="t s4_22">Factuality Set </span>
<span id="t1e_22" class="t s0_22">(Inaccurate Rate) </span>
<span id="t1f_22" class="t s4_22">Attribution Set </span>
<span id="t1g_22" class="t s0_22">(AIS) </span>
<span id="t1h_22" class="t s4_22">Hedging Set </span>
<span id="t1i_22" class="t s0_22">(Accuracy) </span>
<span id="t1j_22" class="t s4_22">Gemini Pro </span>
<span id="t1k_22" class="t s0_22">No factuality-focused adaptation </span>
<span id="t1l_22" class="t s4_22">7.9% </span>
<span id="t1m_22" class="t s0_22">[7%, 9%] </span>
<span id="t1n_22" class="t s4_22">40.2% </span>
<span id="t1o_22" class="t s0_22">[37.9%, 42.4%] </span>
<span id="t1p_22" class="t s4_22">0% </span>
<span id="t1q_22" class="t s4_22">Gemini Pro </span>
<span id="t1r_22" class="t s0_22">Final stage of instruction tuning </span>
<span id="t1s_22" class="t s4_22">3.4% </span>
<span id="t1t_22" class="t s0_22">[2.8%, 4.1%] </span>
<span id="t1u_22" class="t s4_22">59.7% </span>
<span id="t1v_22" class="t s0_22">[57.2%, 61.9%] </span>
<span id="t1w_22" class="t s4_22">69.30% </span>
<span id="t1x_22" class="t s1_22">Table 14 </span><span id="t1y_22" class="t s5_22">| </span><span id="t1z_22" class="t s1_22">Factuality mitigations: Impact of instruction-tuning on the rate of inaccuracy, presence of </span>
<span id="t20_22" class="t s1_22" data-mappings='[[72,"fi"]]'>attribution and the rate of accurate hedging (with corresponding 95% conﬁdence intervals). </span>
<span id="t21_22" class="t s2_22">6.5. Deployment </span>
<span id="t22_22" class="t v10_22 s1_22">Following the completion of reviews, model cards for each approved Gemini model are created for </span>
<span id="t23_22" class="t s1_22">structured and consistent internal documentation of critical performance and responsibility metrics </span>
<span id="t24_22" class="t s1_22">as well as to inform appropriate external communication of these metrics over time. </span>
<span id="t25_22" class="t s2_22">6.6. Responsible Governance </span>
<span id="t26_22" class="t v11_22 s1_22">Across the responsible development process, we undertake ethics and safety reviews with the Google </span>
<span id="t27_22" class="t v12_22 s1_22">DeepMind’s Responsibility and Safety Council (RSC), </span>
<span id="t28_22" class="t s6_22">10 </span>
<span id="t29_22" class="t v12_22 s1_22">an interdisciplinary group which evaluates </span>
<span id="t2a_22" class="t v0_22 s1_22">Google DeepMind’s projects, papers and collaborations against Google’s AI Principles. The RSC </span>
<span id="t2b_22" class="t v0_22 s1_22" data-mappings='[[89,"ff"]]'>provides input and feedback on impact assessments, policies, evaluations and mitigation eﬀorts. </span>
<span id="t2c_22" class="t v0_22 s1_22" data-mappings='[[44,"fi"]]'>During the Gemini project, the RSC set speciﬁc evaluation targets across key policy domains (e.g. </span>
<span id="t2d_22" class="t s1_22">child safety). </span>
<span id="t2e_22" class="t s7_22">10 </span>
<span id="t2f_22" class="t s8_22">https://deepmind.google/about/responsibility-safety/ </span>
<span id="t2g_22" class="t s0_22">22 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
