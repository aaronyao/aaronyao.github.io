<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p8" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_8{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_8{left:121px;bottom:1132px;letter-spacing:-0.23px;word-spacing:0.73px;}
#t3_8{left:95px;bottom:1111px;letter-spacing:-0.21px;word-spacing:1.25px;}
#t4_8{left:95px;bottom:1090px;letter-spacing:-0.22px;word-spacing:0.56px;}
#t5_8{left:95px;bottom:1070px;letter-spacing:-0.22px;}
#t6_8{left:151px;bottom:1078px;}
#t7_8{left:159px;bottom:1070px;letter-spacing:-0.17px;word-spacing:2.27px;}
#t8_8{left:95px;bottom:1049px;letter-spacing:-0.16px;word-spacing:0.51px;}
#t9_8{left:95px;bottom:1003px;letter-spacing:-0.22px;word-spacing:2px;}
#ta_8{left:95px;bottom:969px;letter-spacing:-0.2px;word-spacing:0.88px;}
#tb_8{left:95px;bottom:948px;letter-spacing:-0.2px;word-spacing:1.02px;}
#tc_8{left:95px;bottom:927px;letter-spacing:-0.21px;word-spacing:2.14px;}
#td_8{left:95px;bottom:907px;letter-spacing:-0.21px;word-spacing:0.74px;}
#te_8{left:95px;bottom:886px;letter-spacing:-0.2px;word-spacing:1.52px;}
#tf_8{left:95px;bottom:865px;letter-spacing:-0.2px;word-spacing:0.65px;}
#tg_8{left:95px;bottom:844px;letter-spacing:-0.19px;word-spacing:-0.65px;}
#th_8{left:95px;bottom:824px;letter-spacing:-0.19px;word-spacing:0.59px;}
#ti_8{left:95px;bottom:803px;letter-spacing:-0.34px;}
#tj_8{left:95px;bottom:480px;letter-spacing:-0.15px;word-spacing:1.73px;}
#tk_8{left:165px;bottom:480px;}
#tl_8{left:175px;bottom:480px;letter-spacing:-0.18px;word-spacing:1.78px;}
#tm_8{left:95px;bottom:460px;letter-spacing:-0.19px;word-spacing:0.55px;}
#tn_8{left:121px;bottom:419px;letter-spacing:-0.18px;word-spacing:0.48px;}
#to_8{left:633px;bottom:419px;}
#tp_8{left:642px;bottom:419px;letter-spacing:-0.18px;word-spacing:0.51px;}
#tq_8{left:95px;bottom:398px;letter-spacing:-0.18px;word-spacing:0.58px;}
#tr_8{left:95px;bottom:378px;letter-spacing:-0.18px;word-spacing:0.57px;}
#ts_8{left:95px;bottom:357px;letter-spacing:-0.22px;word-spacing:0.61px;}
#tt_8{left:95px;bottom:311px;letter-spacing:-0.23px;word-spacing:4.29px;}
#tu_8{left:95px;bottom:277px;letter-spacing:-0.23px;word-spacing:1.38px;}
#tv_8{left:95px;bottom:256px;letter-spacing:-0.19px;word-spacing:1.49px;}
#tw_8{left:95px;bottom:235px;letter-spacing:-0.16px;word-spacing:0.76px;}
#tx_8{left:329px;bottom:235px;}
#ty_8{left:343px;bottom:235px;letter-spacing:-0.22px;word-spacing:0.59px;}
#tz_8{left:95px;bottom:215px;letter-spacing:-0.26px;word-spacing:1.44px;}
#t10_8{left:476px;bottom:215px;}
#t11_8{left:491px;bottom:215px;letter-spacing:-0.21px;word-spacing:1.38px;}
#t12_8{left:95px;bottom:194px;letter-spacing:-0.19px;word-spacing:2.77px;}
#t13_8{left:376px;bottom:194px;letter-spacing:-0.2px;word-spacing:2.58px;}
#t14_8{left:95px;bottom:173px;letter-spacing:-0.23px;word-spacing:0.79px;}
#t15_8{left:95px;bottom:153px;letter-spacing:-0.19px;word-spacing:0.72px;}
#t16_8{left:113px;bottom:130px;}
#t17_8{left:120px;bottom:124px;letter-spacing:-0.22px;word-spacing:0.53px;}
#t18_8{left:255px;bottom:123px;letter-spacing:0.04px;}
#t19_8{left:492px;bottom:124px;}
#t1a_8{left:807px;bottom:68px;}

.s0_8{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_8{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s2_8{font-size:12px;font-family:XCharter-Roman_vh;color:#00F;}
.s3_8{font-size:17px;font-family:XCharter-BoldItalic_6q;color:#000;}
.s4_8{font-size:17px;font-family:txsys_6y;color:#000;}
.s5_8{font-size:17px;font-family:XCharter-Roman_vh;color:#00F;}
.s6_8{font-size:11px;font-family:XCharter-Roman_vh;color:#000;}
.s7_8{font-size:14px;font-family:XCharter-Roman_vh;color:#000;}
.s8_8{font-size:15px;font-family:LMMono9-Regular_6v;color:#00F;}
.t.v0_8{transform:scaleX(0.981);}
.t.v1_8{transform:scaleX(1.02);}
.t.v2_8{transform:scaleX(1.008);}
.t.v3_8{transform:scaleX(0.98);}
.t.v4_8{transform:scaleX(0.979);}
.t.v5_8{transform:scaleX(0.984);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts8" type="text/css" >

@font-face {
	font-family: LMMono9-Regular_6v;
	src: url("fonts/LMMono9-Regular_6v.woff") format("woff");
}

@font-face {
	font-family: XCharter-BoldItalic_6q;
	src: url("fonts/XCharter-BoldItalic_6q.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

@font-face {
	font-family: txsys_6y;
	src: url("fonts/txsys_6y.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg8Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg8" style="-webkit-user-select: none;"><object width="909" height="1286" data="8/8.svg" type="image/svg+xml" id="pdf8" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_8" class="t s0_8">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_8" class="t v0_8 s1_8">Even so, model performance on these benchmarks gives us an indication of the model capabilities </span>
<span id="t3_8" class="t v1_8 s1_8">and where they may provide impact on real-world tasks. For example, Gemini Ultra’s impressive </span>
<span id="t4_8" class="t v2_8 s1_8">reasoning and STEM competencies pave the way for advancements in LLMs within the educational </span>
<span id="t5_8" class="t v1_8 s1_8">domain </span>
<span id="t6_8" class="t s2_8">4 </span>
<span id="t7_8" class="t v1_8 s1_8" data-mappings='[[56,"fi"]]'>. The ability to tackle complex mathematical and scientiﬁc concepts opens up exciting </span>
<span id="t8_8" class="t s1_8">possibilities for personalized learning and intelligent tutoring systems. </span>
<span id="t9_8" class="t s3_8">5.1.2. Trends in Capabilities </span>
<span id="ta_8" class="t v1_8 s1_8">We investigate the trends in capabilities across the Gemini model family by evaluating them on a </span>
<span id="tb_8" class="t v1_8 s1_8" data-mappings='[[53,"ff"]]'>holistic harness of more than 50 benchmarks in six diﬀerent capabilities, noting that some of the </span>
<span id="tc_8" class="t v1_8 s1_8">most notable benchmarks were discussed in the last section. These capabilities are: “Factuality” </span>
<span id="td_8" class="t v1_8 s1_8">covering open/closed-book retrieval and question answering tasks; “Long-Context” covering long- </span>
<span id="te_8" class="t v1_8 s1_8">form summarization, retrieval and question answering tasks; “Math/Science” including tasks for </span>
<span id="tf_8" class="t v3_8 s1_8" data-mappings='[[58,"fi"]]'>mathematical problem solving, theorem proving, and scientiﬁc exams; “Reasoning” tasks that require </span>
<span id="tg_8" class="t v4_8 s1_8" data-mappings='[[19,"fi"]]'>arithmetic, scientiﬁc, and commonsense reasoning; “Multilingual” tasks for translation, summarization, </span>
<span id="th_8" class="t v4_8 s1_8">and reasoning in multiple languages. Please see appendix for a detailed list of tasks included for each </span>
<span id="ti_8" class="t s1_8">capability. </span>
<span id="tj_8" class="t v1_8 s1_8">Figure 3 </span><span id="tk_8" class="t s4_8">| </span><span id="tl_8" class="t v1_8 s1_8">Language understanding and generation performance of Gemini model family across </span>
<span id="tm_8" class="t s1_8" data-mappings='[[2,"ff"]]'>diﬀerent capabilities (normalized by the Gemini Pro model). </span>
<span id="tn_8" class="t v4_8 s1_8">We observe consistent quality gains with increased model size in Figure </span><span id="to_8" class="t v4_8 s5_8">3</span><span id="tp_8" class="t v4_8 s1_8">, especially in reasoning, </span>
<span id="tq_8" class="t s1_8">math/science, summarization and long-context. Gemini Ultra is the best model across the board for </span>
<span id="tr_8" class="t v4_8 s1_8">all six capabilities. Gemini Pro, the second-largest model in the Gemini family of models, is also quite </span>
<span id="ts_8" class="t s1_8" data-mappings='[[36,"ffi"]]'>competitive while being a lot more eﬃcient to serve. </span>
<span id="tt_8" class="t s3_8">5.1.3. Nano </span>
<span id="tu_8" class="t v1_8 s1_8">Bringing AI closer to the user, we discuss the Gemini Nano 1 and Nano 2 models engineered for </span>
<span id="tv_8" class="t v1_8 s1_8">on-device deployments. These models excel in summarization and reading comprehension tasks </span>
<span id="tw_8" class="t s1_8" data-mappings='[[14,"fi"]]'>with per-task ﬁnetuning. Figure </span><span id="tx_8" class="t s5_8">3 </span><span id="ty_8" class="t s1_8">shows the performance of these pretrained models in comparison </span>
<span id="tz_8" class="t v1_8 s1_8">to the much larger Gemini Pro model, while Table </span><span id="t10_8" class="t v1_8 s5_8">3 </span><span id="t11_8" class="t v1_8 s1_8" data-mappings='[[23,"fi"]]'>dives deeper into speciﬁc factuality, coding, </span>
<span id="t12_8" class="t v1_8 s1_8">Math/Science, and reasoning tasks. </span><span id="t13_8" class="t v1_8 s1_8">Nano-1 and Nano-2 model sizes are only 1.8B and 3.25B </span>
<span id="t14_8" class="t v5_8 s1_8">parameters respectively. Despite their size, they show exceptionally strong performance on factuality, </span>
<span id="t15_8" class="t v5_8 s1_8" data-mappings='[[39,"fi"]]'>i.e. retrieval-related tasks, and signiﬁcant performance on reasoning, STEM, coding, multimodal and </span>
<span id="t16_8" class="t s6_8">4 </span>
<span id="t17_8" class="t s7_8">See demos on website </span><span id="t18_8" class="t s8_8">https://deepmind.google/gemini</span><span id="t19_8" class="t s7_8">. </span>
<span id="t1a_8" class="t s0_8">8 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
