<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p10" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_10{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_10{left:95px;bottom:1132px;letter-spacing:-0.19px;word-spacing:0.37px;}
#t3_10{left:95px;bottom:1111px;letter-spacing:-0.2px;word-spacing:0.57px;}
#t4_10{left:95px;bottom:1057px;letter-spacing:-0.18px;word-spacing:0.64px;}
#t5_10{left:411px;bottom:1057px;letter-spacing:-0.23px;word-spacing:0.56px;}
#t6_10{left:95px;bottom:1036px;letter-spacing:-0.2px;word-spacing:0.64px;}
#t7_10{left:95px;bottom:1015px;letter-spacing:-0.19px;word-spacing:0.52px;}
#t8_10{left:198px;bottom:1015px;letter-spacing:-0.25px;word-spacing:0.76px;}
#t9_10{left:261px;bottom:1015px;}
#ta_10{left:270px;bottom:1015px;letter-spacing:-0.19px;}
#tb_10{left:307px;bottom:1015px;letter-spacing:-0.17px;word-spacing:0.56px;}
#tc_10{left:769px;bottom:1015px;letter-spacing:-0.26px;}
#td_10{left:95px;bottom:995px;letter-spacing:-0.23px;word-spacing:0.73px;}
#te_10{left:131px;bottom:995px;}
#tf_10{left:140px;bottom:995px;letter-spacing:-0.19px;}
#tg_10{left:177px;bottom:995px;letter-spacing:-0.24px;word-spacing:0.74px;}
#th_10{left:95px;bottom:974px;letter-spacing:-0.2px;word-spacing:0.69px;}
#ti_10{left:95px;bottom:953px;letter-spacing:-0.17px;word-spacing:0.51px;}
#tj_10{left:495px;bottom:953px;letter-spacing:-0.21px;word-spacing:0.62px;}
#tk_10{left:580px;bottom:953px;}
#tl_10{left:589px;bottom:953px;letter-spacing:-0.19px;}
#tm_10{left:626px;bottom:953px;letter-spacing:-0.16px;word-spacing:0.5px;}
#tn_10{left:763px;bottom:953px;letter-spacing:-0.34px;}
#to_10{left:95px;bottom:933px;letter-spacing:-0.22px;word-spacing:0.72px;}
#tp_10{left:131px;bottom:933px;}
#tq_10{left:140px;bottom:933px;letter-spacing:-0.19px;}
#tr_10{left:176px;bottom:933px;letter-spacing:-0.18px;word-spacing:0.64px;}
#ts_10{left:95px;bottom:912px;letter-spacing:-0.21px;word-spacing:0.59px;}
#tt_10{left:95px;bottom:891px;letter-spacing:-0.33px;word-spacing:1.21px;}
#tu_10{left:217px;bottom:891px;}
#tv_10{left:231px;bottom:891px;letter-spacing:-0.2px;word-spacing:0.61px;}
#tw_10{left:95px;bottom:870px;letter-spacing:-0.2px;word-spacing:1.54px;}
#tx_10{left:95px;bottom:850px;letter-spacing:-0.18px;word-spacing:0.53px;}
#ty_10{left:385px;bottom:814px;letter-spacing:0.08px;word-spacing:0.34px;}
#tz_10{left:472px;bottom:814px;letter-spacing:0.09px;word-spacing:0.34px;}
#t10_10{left:551px;bottom:814px;letter-spacing:-0.27px;}
#t11_10{left:601px;bottom:814px;letter-spacing:-0.01px;word-spacing:0.54px;}
#t12_10{left:259px;bottom:791px;letter-spacing:0.13px;}
#t13_10{left:308px;bottom:792px;letter-spacing:0.05px;}
#t14_10{left:385px;bottom:791px;letter-spacing:0.09px;}
#t15_10{left:472px;bottom:791px;letter-spacing:0.09px;}
#t16_10{left:555px;bottom:791px;letter-spacing:0.09px;}
#t17_10{left:614px;bottom:791px;letter-spacing:0.09px;}
#t18_10{left:259px;bottom:776px;letter-spacing:0.1px;}
#t19_10{left:307px;bottom:778px;letter-spacing:0.05px;}
#t1a_10{left:385px;bottom:776px;letter-spacing:0.09px;}
#t1b_10{left:472px;bottom:776px;letter-spacing:0.09px;}
#t1c_10{left:561px;bottom:776px;}
#t1d_10{left:614px;bottom:776px;letter-spacing:0.09px;}
#t1e_10{left:259px;bottom:762px;letter-spacing:0.07px;}
#t1f_10{left:385px;bottom:762px;letter-spacing:0.09px;}
#t1g_10{left:472px;bottom:762px;letter-spacing:0.09px;}
#t1h_10{left:561px;bottom:762px;}
#t1i_10{left:613px;bottom:762px;letter-spacing:0.09px;}
#t1j_10{left:95px;bottom:722px;letter-spacing:-0.71px;word-spacing:1.59px;}
#t1k_10{left:152px;bottom:722px;}
#t1l_10{left:161px;bottom:722px;letter-spacing:-0.21px;word-spacing:0.58px;}
#t1m_10{left:95px;bottom:654px;letter-spacing:-0.16px;word-spacing:2.43px;}
#t1n_10{left:95px;bottom:620px;letter-spacing:-0.2px;word-spacing:0.56px;}
#t1o_10{left:95px;bottom:599px;letter-spacing:-0.22px;word-spacing:0.71px;}
#t1p_10{left:95px;bottom:578px;letter-spacing:-0.2px;word-spacing:0.65px;}
#t1q_10{left:95px;bottom:558px;letter-spacing:-0.23px;word-spacing:0.99px;}
#t1r_10{left:95px;bottom:537px;letter-spacing:-0.19px;word-spacing:0.58px;}
#t1s_10{left:95px;bottom:516px;letter-spacing:-0.18px;word-spacing:1.05px;}
#t1t_10{left:770px;bottom:516px;}
#t1u_10{left:779px;bottom:516px;letter-spacing:-1.29px;word-spacing:5.17px;}
#t1v_10{left:95px;bottom:496px;letter-spacing:-0.18px;word-spacing:1.07px;}
#t1w_10{left:95px;bottom:475px;letter-spacing:-0.22px;word-spacing:0.53px;}
#t1x_10{left:95px;bottom:454px;letter-spacing:-0.16px;word-spacing:0.49px;}
#t1y_10{left:353px;bottom:454px;letter-spacing:-0.15px;}
#t1z_10{left:390px;bottom:454px;}
#t20_10{left:95px;bottom:227px;letter-spacing:-0.15px;word-spacing:0.17px;}
#t21_10{left:159px;bottom:227px;}
#t22_10{left:167px;bottom:227px;letter-spacing:-0.19px;word-spacing:0.26px;}
#t23_10{left:95px;bottom:206px;letter-spacing:-0.2px;word-spacing:0.58px;}
#t24_10{left:801px;bottom:68px;letter-spacing:0.1px;}

.s0_10{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_10{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s2_10{font-size:17px;font-family:XCharter-Bold_vg;color:#000;}
.s3_10{font-size:17px;font-family:XCharter-Roman_vh;color:#00F;}
.s4_10{font-size:9px;font-family:XCharter-Roman_vh;color:#000;}
.s5_10{font-size:12px;font-family:XCharter-Bold_vg;color:#00F;}
.s6_10{font-size:17px;font-family:txsys_6y;color:#000;}
.s7_10{font-size:17px;font-family:XCharter-BoldItalic_6q;color:#000;}
.t.v0_10{transform:scaleX(0.979);}
.t.v1_10{transform:scaleX(1.011);}
.t.v2_10{transform:scaleX(0.99);}
.t.v3_10{transform:scaleX(0.988);}
.t.v4_10{transform:scaleX(0.983);}
.t.v5_10{transform:scaleX(0.993);}
.t.v6_10{transform:scaleX(0.984);}
.t.v7_10{transform:scaleX(1.008);}
.t.v8_10{transform:scaleX(1.02);}
.t.v9_10{transform:scaleX(0.995);}
.t.v10_10{transform:scaleX(1.005);}
.t.v11_10{transform:scaleX(0.985);}
.t.v12_10{transform:scaleX(1.014);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts10" type="text/css" >

@font-face {
	font-family: XCharter-BoldItalic_6q;
	src: url("fonts/XCharter-BoldItalic_6q.woff") format("woff");
}

@font-face {
	font-family: XCharter-Bold_vg;
	src: url("fonts/XCharter-Bold_vg.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

@font-face {
	font-family: txsys_6y;
	src: url("fonts/txsys_6y.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg10Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg10" style="-webkit-user-select: none;"><object width="909" height="1286" data="10/10.svg" type="image/svg+xml" id="pdf10" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_10" class="t s0_10">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_10" class="t v0_10 s1_10">For these languages, both from and into English, Gemini Ultra achieved an average chrF score of 27.0 </span>
<span id="t3_10" class="t s1_10">in 1-shot setup, while the next-best model, PaLM 2-L, achieved a score of 25.3. </span>
<span id="t4_10" class="t v1_10 s2_10">Multilingual Math and Summarization </span><span id="t5_10" class="t v1_10 s1_10">Beyond translation, we evaluated how well Gemini per- </span>
<span id="t6_10" class="t s1_10" data-mappings='[[64,"fi"]]'>forms in challenging tasks across a range of languages. We speciﬁcally investigated the math bench- </span>
<span id="t7_10" class="t v2_10 s1_10">mark MGSM (</span><span id="t8_10" class="t v2_10 s3_10">Shi et al.</span><span id="t9_10" class="t v2_10 s1_10">, </span><span id="ta_10" class="t v2_10 s3_10">2023</span><span id="tb_10" class="t v2_10 s1_10">), which is a translated variant of the math benchmark GSM8K (</span><span id="tc_10" class="t v2_10 s3_10">Cobbe </span>
<span id="td_10" class="t v3_10 s3_10">et al.</span><span id="te_10" class="t v3_10 s1_10">, </span><span id="tf_10" class="t v3_10 s3_10">2021</span><span id="tg_10" class="t v3_10 s1_10" data-mappings='[[6,"fi"]]'>). We ﬁnd Gemini Ultra achieves an accuracy of 79.0%, an advance over PaLM 2-L which </span>
<span id="th_10" class="t v4_10 s1_10">scores 74.7%, when averaged across all languages in an 8-shot setup. We also benchmark Gemini on </span>
<span id="ti_10" class="t v5_10 s1_10">the multilingual summarization benchmarks – XLSum (</span><span id="tj_10" class="t v5_10 s3_10">Hasan et al.</span><span id="tk_10" class="t v5_10 s1_10">, </span><span id="tl_10" class="t v5_10 s3_10">2021</span><span id="tm_10" class="t v5_10 s1_10">) and WikiLingua (</span><span id="tn_10" class="t v5_10 s3_10">Ladhak </span>
<span id="to_10" class="t v6_10 s3_10">et al.</span><span id="tp_10" class="t v6_10 s1_10">, </span><span id="tq_10" class="t v6_10 s3_10">2020</span><span id="tr_10" class="t v6_10 s1_10">). In XLSum, Gemini Ultra reached an average of 17.6 rougeL score compared to 15.4 for </span>
<span id="ts_10" class="t v7_10 s1_10">PaLM 2. For Wikilingua, Gemini Ultra (5-shot) trails behind PaLM 2 (3-shot) measured in BLEURT </span>
<span id="tt_10" class="t v7_10 s1_10">score. See Table </span><span id="tu_10" class="t v7_10 s3_10">5 </span><span id="tv_10" class="t v7_10 s1_10">for the full results. Overall the diverse set of multilingual benchmarks show that </span>
<span id="tw_10" class="t v8_10 s1_10">Gemini family models have a broad language coverage, enabling them to also reach locales and </span>
<span id="tx_10" class="t s1_10">regions with low-resource languages. </span>
<span id="ty_10" class="t s0_10">Gemini Ultra </span><span id="tz_10" class="t s0_10">Gemini Pro </span><span id="t10_10" class="t s0_10">GPT-4 </span><span id="t11_10" class="t s0_10">PaLM 2-L </span>
<span id="t12_10" class="t s0_10">MGSM </span><span id="t13_10" class="t s4_10">(8-shot) </span><span id="t14_10" class="t s5_10">79.0 </span><span id="t15_10" class="t s0_10">63.5 </span><span id="t16_10" class="t s0_10">74.5 </span><span id="t17_10" class="t s0_10">74.7 </span>
<span id="t18_10" class="t s0_10">XLsum </span><span id="t19_10" class="t s4_10">(3-shot) </span><span id="t1a_10" class="t s5_10">17.6 </span><span id="t1b_10" class="t s0_10">16.2 </span><span id="t1c_10" class="t s0_10">— </span><span id="t1d_10" class="t s0_10">15.4 </span>
<span id="t1e_10" class="t s0_10">Wikilingua </span><span id="t1f_10" class="t s0_10">48.9 </span><span id="t1g_10" class="t s0_10">47.8 </span><span id="t1h_10" class="t s0_10">— </span><span id="t1i_10" class="t s5_10">50.4 </span>
<span id="t1j_10" class="t s1_10">Table 5 </span><span id="t1k_10" class="t s6_10">| </span><span id="t1l_10" class="t s1_10">Performance of Gemini models on multilingual math and summarization. </span>
<span id="t1m_10" class="t s7_10">5.1.5. Long Context </span>
<span id="t1n_10" class="t v9_10 s1_10" data-mappings='[[73,"fi"]]'>Gemini models are trained with a sequence length of 32,768 tokens and we ﬁnd that they make use </span>
<span id="t1o_10" class="t v10_10 s1_10" data-mappings='[[25,"ff"],[39,"fi"]]'>of their context length eﬀectively. We ﬁrst verify this by running a synthetic retrieval test: we place </span>
<span id="t1p_10" class="t v11_10 s1_10" data-mappings='[[63,"fi"]]'>key-value pairs at the beginning of the context, then add long ﬁller text, and ask for value associated </span>
<span id="t1q_10" class="t v8_10 s1_10" data-mappings='[[26,"fi"]]'>with a particular key. We ﬁnd that the Ultra model retrieves the correct value with 98% accuracy </span>
<span id="t1r_10" class="t s1_10">when queried across the full context length. We further investigate this by plotting the negative log </span>
<span id="t1s_10" class="t v8_10 s1_10">likelihood (NLL) versus the token index across a held-out set of long documents in Figure </span><span id="t1t_10" class="t v8_10 s3_10">4</span><span id="t1u_10" class="t v8_10 s1_10">. We </span>
<span id="t1v_10" class="t v8_10 s1_10" data-mappings='[[0,"fi"]]'>ﬁnd that the NLL decreases with sequence position up to the full 32K context length. The longer </span>
<span id="t1w_10" class="t v12_10 s1_10">context length of Gemini models enable new use cases such as retrieval over documents and video </span>
<span id="t1x_10" class="t s1_10">understanding discussed in section </span><span id="t1y_10" class="t s3_10">5.2.2</span><span id="t1z_10" class="t s1_10">. </span>
<span id="t20_10" class="t v0_10 s1_10">Figure 4 </span><span id="t21_10" class="t s6_10">| </span><span id="t22_10" class="t v0_10 s1_10">Negative log likelihood as a function of token index across 32K context length on a held-out </span>
<span id="t23_10" class="t s1_10">set of long documents. </span>
<span id="t24_10" class="t s0_10">10 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
