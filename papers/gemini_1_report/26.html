<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p26" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_26{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_26{left:95px;bottom:1132px;letter-spacing:-0.21px;word-spacing:0.66px;}
#t3_26{left:111px;bottom:1111px;letter-spacing:-0.19px;word-spacing:0.91px;}
#t4_26{left:717px;bottom:1111px;letter-spacing:-0.18px;word-spacing:0.53px;}
#t5_26{left:112px;bottom:1090px;letter-spacing:-0.19px;word-spacing:0.58px;}
#t6_26{left:112px;bottom:1070px;letter-spacing:-0.25px;word-spacing:0.57px;}
#t7_26{left:586px;bottom:1070px;letter-spacing:-0.16px;word-spacing:0.84px;}
#t8_26{left:112px;bottom:1048px;letter-spacing:0.2px;}
#t9_26{left:430px;bottom:1049px;}
#ta_26{left:95px;bottom:1015px;letter-spacing:-0.23px;word-spacing:2.67px;}
#tb_26{left:111px;bottom:994px;letter-spacing:-0.21px;word-spacing:0.74px;}
#tc_26{left:112px;bottom:973px;letter-spacing:-0.18px;word-spacing:0.3px;}
#td_26{left:311px;bottom:973px;letter-spacing:-0.2px;word-spacing:0.34px;}
#te_26{left:729px;bottom:973px;letter-spacing:-0.16px;word-spacing:1.05px;}
#tf_26{left:112px;bottom:952px;letter-spacing:0.18px;}
#tg_26{left:574px;bottom:953px;}
#th_26{left:95px;bottom:918px;letter-spacing:-0.21px;word-spacing:1.63px;}
#ti_26{left:112px;bottom:897px;letter-spacing:-0.19px;word-spacing:2.68px;}
#tj_26{left:336px;bottom:897px;letter-spacing:-0.24px;word-spacing:2.63px;}
#tk_26{left:712px;bottom:897px;letter-spacing:-0.15px;word-spacing:2.48px;}
#tl_26{left:112px;bottom:877px;letter-spacing:-0.17px;}
#tm_26{left:242px;bottom:877px;letter-spacing:-0.16px;word-spacing:1.38px;}
#tn_26{left:335px;bottom:876px;letter-spacing:0.18px;}
#to_26{left:643px;bottom:877px;}
#tp_26{left:95px;bottom:842px;letter-spacing:-0.23px;word-spacing:0.61px;}
#tq_26{left:111px;bottom:822px;letter-spacing:-0.21px;word-spacing:0.84px;}
#tr_26{left:112px;bottom:801px;letter-spacing:-0.22px;word-spacing:1.6px;}
#ts_26{left:209px;bottom:801px;letter-spacing:-0.31px;word-spacing:0.83px;}
#tt_26{left:605px;bottom:801px;letter-spacing:-0.12px;word-spacing:0.44px;}
#tu_26{left:95px;bottom:767px;letter-spacing:-0.16px;word-spacing:-0.54px;}
#tv_26{left:111px;bottom:746px;letter-spacing:-0.27px;word-spacing:1.04px;}
#tw_26{left:730px;bottom:746px;letter-spacing:-0.2px;word-spacing:0.69px;}
#tx_26{left:112px;bottom:725px;letter-spacing:-0.2px;word-spacing:0.57px;}
#ty_26{left:378px;bottom:725px;letter-spacing:-0.14px;word-spacing:0.47px;}
#tz_26{left:95px;bottom:691px;letter-spacing:-0.27px;word-spacing:0.67px;}
#t10_26{left:112px;bottom:670px;letter-spacing:-0.21px;word-spacing:0.91px;}
#t11_26{left:573px;bottom:670px;letter-spacing:-0.16px;word-spacing:0.52px;}
#t12_26{left:812px;bottom:670px;}
#t13_26{left:111px;bottom:649px;letter-spacing:-0.17px;}
#t14_26{left:95px;bottom:615px;letter-spacing:-0.29px;word-spacing:1.35px;}
#t15_26{left:112px;bottom:594px;letter-spacing:-0.26px;word-spacing:0.74px;}
#t16_26{left:112px;bottom:574px;letter-spacing:-0.25px;word-spacing:1.07px;}
#t17_26{left:112px;bottom:553px;letter-spacing:-0.24px;}
#t18_26{left:145px;bottom:553px;letter-spacing:-0.15px;word-spacing:0.47px;}
#t19_26{left:95px;bottom:518px;letter-spacing:-0.27px;word-spacing:0.81px;}
#t1a_26{left:112px;bottom:498px;letter-spacing:-0.22px;word-spacing:1.83px;}
#t1b_26{left:112px;bottom:477px;letter-spacing:-0.2px;word-spacing:-0.3px;}
#t1c_26{left:112px;bottom:456px;letter-spacing:-0.23px;word-spacing:0.64px;}
#t1d_26{left:672px;bottom:456px;letter-spacing:-0.16px;word-spacing:0.4px;}
#t1e_26{left:111px;bottom:436px;letter-spacing:-0.18px;word-spacing:2.3px;}
#t1f_26{left:195px;bottom:435px;letter-spacing:0.2px;}
#t1g_26{left:514px;bottom:436px;}
#t1h_26{left:95px;bottom:401px;letter-spacing:-0.25px;word-spacing:3.8px;}
#t1i_26{left:488px;bottom:401px;letter-spacing:-0.22px;word-spacing:3.48px;}
#t1j_26{left:112px;bottom:381px;letter-spacing:-0.2px;word-spacing:2.15px;}
#t1k_26{left:342px;bottom:381px;letter-spacing:-0.15px;}
#t1l_26{left:364px;bottom:381px;letter-spacing:-0.21px;word-spacing:2.19px;}
#t1m_26{left:112px;bottom:360px;letter-spacing:-0.28px;}
#t1n_26{left:188px;bottom:360px;letter-spacing:-0.19px;word-spacing:2.8px;}
#t1o_26{left:783px;bottom:360px;letter-spacing:-0.21px;}
#t1p_26{left:112px;bottom:338px;letter-spacing:0.19px;}
#t1q_26{left:507px;bottom:339px;}
#t1r_26{left:95px;bottom:305px;letter-spacing:-0.19px;}
#t1s_26{left:180px;bottom:305px;letter-spacing:-0.18px;}
#t1t_26{left:253px;bottom:305px;letter-spacing:-0.16px;}
#t1u_26{left:281px;bottom:305px;letter-spacing:-0.17px;}
#t1v_26{left:385px;bottom:305px;letter-spacing:-0.17px;}
#t1w_26{left:456px;bottom:305px;letter-spacing:-0.21px;}
#t1x_26{left:498px;bottom:304px;letter-spacing:0.18px;}
#t1y_26{left:112px;bottom:283px;letter-spacing:0.18px;}
#t1z_26{left:218px;bottom:284px;}
#t20_26{left:95px;bottom:250px;letter-spacing:-0.28px;word-spacing:0.87px;}
#t21_26{left:112px;bottom:229px;letter-spacing:-0.18px;word-spacing:0.8px;}
#t22_26{left:715px;bottom:229px;letter-spacing:-0.18px;word-spacing:0.57px;}
#t23_26{left:112px;bottom:208px;letter-spacing:-0.17px;word-spacing:0.53px;}
#t24_26{left:555px;bottom:208px;letter-spacing:-0.16px;word-spacing:0.47px;}
#t25_26{left:95px;bottom:174px;letter-spacing:-0.26px;word-spacing:0.74px;}
#t26_26{left:112px;bottom:153px;letter-spacing:-0.21px;word-spacing:0.16px;}
#t27_26{left:112px;bottom:132px;letter-spacing:-0.15px;word-spacing:0.99px;}
#t28_26{left:266px;bottom:132px;letter-spacing:-0.19px;word-spacing:0.56px;}
#t29_26{left:812px;bottom:132px;}
#t2a_26{left:801px;bottom:68px;letter-spacing:0.1px;}

.s0_26{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_26{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s2_26{font-size:17px;font-family:XCharter-Italic_6r;color:#000;}
.s3_26{font-size:18px;font-family:LMMono10-Regular_6t;color:#00F;}
.t.v0_26{transform:scaleX(1.02);}
.t.v1_26{transform:scaleX(0.979);}
.t.v2_26{transform:scaleX(0.994);}
.t.v3_26{transform:scaleX(1.014);}
.t.v4_26{transform:scaleX(1.005);}
.t.v5_26{transform:scaleX(0.988);}
.t.v6_26{transform:scaleX(0.991);}
.t.v7_26{transform:scaleX(1.016);}
.t.v8_26{transform:scaleX(0.987);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts26" type="text/css" >

@font-face {
	font-family: LMMono10-Regular_6t;
	src: url("fonts/LMMono10-Regular_6t.woff") format("woff");
}

@font-face {
	font-family: XCharter-Italic_6r;
	src: url("fonts/XCharter-Italic_6r.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg26Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg26" style="-webkit-user-select: none;"><object width="909" height="1286" data="26/26.svg" type="image/svg+xml" id="pdf26" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_26" class="t s0_26">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_26" class="t v0_26 s1_26">Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina </span>
<span id="t3_26" class="t v1_26 s1_26" data-mappings='[[45,"ffi"]]'>Toutanova. BoolQ: Exploring the surprising diﬃculty of natural yes/no questions. In </span><span id="t4_26" class="t v1_26 s2_26">Proceedings of </span>
<span id="t5_26" class="t v2_26 s2_26">the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: </span>
<span id="t6_26" class="t v3_26 s2_26">Human Language Technologies, Volume 1 (Long and Short Papers)</span><span id="t7_26" class="t v3_26 s1_26">, pages 2924–2936, 2019. URL </span>
<span id="t8_26" class="t s3_26">https://aclanthology.org/N19-1300</span><span id="t9_26" class="t s1_26">. </span>
<span id="ta_26" class="t v0_26 s1_26">Jon Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and </span>
<span id="tb_26" class="t s1_26">Jennimaria Palomaki. TydiQA: A benchmark for information-seeking question answering in typo- </span>
<span id="tc_26" class="t v1_26 s1_26">logically diverse languages. </span><span id="td_26" class="t v1_26 s2_26">Transactions of the Association for Computational Linguistics</span><span id="te_26" class="t v1_26 s1_26">, 2020. URL </span>
<span id="tf_26" class="t s3_26">https://storage.googleapis.com/tydiqa/tydiqa.pdf</span><span id="tg_26" class="t s1_26">. </span>
<span id="th_26" class="t v0_26 s1_26">Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher </span>
<span id="ti_26" class="t v0_26 s1_26">Hesse, and John Schulman. </span><span id="tj_26" class="t v0_26 s1_26" data-mappings='[[13,"fi"]]'>Training veriﬁers to solve math word problems. </span><span id="tk_26" class="t v0_26 s2_26">arXiv preprint </span>
<span id="tl_26" class="t s2_26">arXiv:2110.14168</span><span id="tm_26" class="t s1_26">, 2021. URL </span><span id="tn_26" class="t s3_26">https://arxiv.org/abs/2110.14168</span><span id="to_26" class="t s1_26">. </span>
<span id="tp_26" class="t v4_26 s1_26">Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, </span>
<span id="tq_26" class="t s1_26">Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representations </span>
<span id="tr_26" class="t v5_26 s1_26">of speech. In </span><span id="ts_26" class="t v5_26 s2_26">2022 IEEE Spoken Language Technology Workshop (SLT)</span><span id="tt_26" class="t v5_26 s1_26">, pages 798–805. IEEE, 2023. </span>
<span id="tu_26" class="t v1_26 s1_26" data-mappings='[[2,"ff"]]'>Jeﬀrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio Ranzato, </span>
<span id="tv_26" class="t v0_26 s1_26">Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. </span><span id="tw_26" class="t v0_26 s2_26">Advances in </span>
<span id="tx_26" class="t s2_26">neural information processing systems</span><span id="ty_26" class="t s1_26">, 25, 2012. </span>
<span id="tz_26" class="t v1_26 s1_26">Harish Dattatraya Dixit, Sneha Pendharkar, Matt Beadon, Chris Mason, Tejasvi Chakravarthy, Bharath </span>
<span id="t10_26" class="t v0_26 s1_26">Muthiah, and Sriram Sankar. Silent data corruptions at scale. </span><span id="t11_26" class="t v0_26 s2_26">arXiv preprint arXiv:2102.11245</span><span id="t12_26" class="t v0_26 s1_26">, </span>
<span id="t13_26" class="t s1_26">2021. </span>
<span id="t14_26" class="t v0_26 s1_26">Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas </span>
<span id="t15_26" class="t v6_26 s1_26">Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, </span>
<span id="t16_26" class="t v1_26 s1_26">and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In </span>
<span id="t17_26" class="t s2_26">ICLR</span><span id="t18_26" class="t s1_26">, 2020. </span>
<span id="t19_26" class="t v0_26 s1_26">Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. </span>
<span id="t1a_26" class="t v0_26 s1_26">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In </span>
<span id="t1b_26" class="t v1_26 s2_26">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational </span>
<span id="t1c_26" class="t v7_26 s2_26">Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</span><span id="t1d_26" class="t v7_26 s1_26">, pages 2368–2378, </span>
<span id="t1e_26" class="t s1_26">2019. URL </span><span id="t1f_26" class="t s3_26">https://aclanthology.org/N19-1246</span><span id="t1g_26" class="t s1_26">. </span>
<span id="t1h_26" class="t v0_26 s1_26">Christian Federmann, Tom Kocmi, and Ying Xin. </span><span id="t1i_26" class="t v0_26 s1_26">NTREX-128 – news test references for MT </span>
<span id="t1j_26" class="t v0_26 s1_26">evaluation of 128 languages. </span><span id="t1k_26" class="t v0_26 s1_26">In </span><span id="t1l_26" class="t v0_26 s2_26">Proceedings of the First Workshop on Scaling Up Multilingual </span>
<span id="t1m_26" class="t v0_26 s2_26">Evaluation</span><span id="t1n_26" class="t v0_26 s1_26">, pages 21–24, Online, nov 2022. Association for Computational Linguistics. </span><span id="t1o_26" class="t v0_26 s1_26">URL </span>
<span id="t1p_26" class="t s3_26">https://aclanthology.org/2022.sumeval-1.4</span><span id="t1q_26" class="t s1_26">. </span>
<span id="t1r_26" class="t v0_26 s1_26">Google. </span><span id="t1s_26" class="t v0_26 s1_26">Google’s </span><span id="t1t_26" class="t v0_26 s1_26">AI </span><span id="t1u_26" class="t v0_26 s1_26">Principles. </span><span id="t1v_26" class="t v0_26 s1_26">2023. </span><span id="t1w_26" class="t v0_26 s1_26">URL </span><span id="t1x_26" class="t s3_26">https://ai.google/responsibility/ </span>
<span id="t1y_26" class="t s3_26">principles/</span><span id="t1z_26" class="t s1_26">. </span>
<span id="t20_26" class="t v2_26 s1_26">Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA </span>
<span id="t21_26" class="t s1_26">matter: Elevating the role of image understanding in visual question answering. In </span><span id="t22_26" class="t s2_26">Proceedings of </span>
<span id="t23_26" class="t s2_26">the IEEE conference on computer vision and pattern recognition</span><span id="t24_26" class="t s1_26">, pages 6904–6913, 2017. </span>
<span id="t25_26" class="t v8_26 s1_26">Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, </span>
<span id="t26_26" class="t v1_26 s1_26">M. Sohel Rahman, and Rifat Shahriyar. XL-sum: Large-scale multilingual abstractive summarization </span>
<span id="t27_26" class="t v0_26 s1_26">for 44 languages. In </span><span id="t28_26" class="t v0_26 s2_26">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</span><span id="t29_26" class="t v0_26 s1_26">, </span>
<span id="t2a_26" class="t s0_26">26 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
