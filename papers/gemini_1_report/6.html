<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p6" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_6{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_6{left:95px;bottom:1130px;letter-spacing:-0.24px;word-spacing:5.22px;}
#t3_6{left:95px;bottom:1093px;letter-spacing:-0.2px;word-spacing:0.98px;}
#t4_6{left:95px;bottom:1072px;letter-spacing:-0.18px;word-spacing:0.54px;}
#t5_6{left:95px;bottom:1052px;letter-spacing:-0.2px;word-spacing:1.32px;}
#t6_6{left:95px;bottom:1031px;letter-spacing:-0.18px;word-spacing:0.62px;}
#t7_6{left:95px;bottom:1010px;letter-spacing:-0.18px;word-spacing:0.53px;}
#t8_6{left:95px;bottom:959px;letter-spacing:-0.31px;word-spacing:4.47px;}
#t9_6{left:95px;bottom:925px;letter-spacing:-0.21px;word-spacing:2.51px;}
#ta_6{left:95px;bottom:891px;letter-spacing:-0.22px;word-spacing:0.82px;}
#tb_6{left:95px;bottom:870px;letter-spacing:-0.2px;word-spacing:0.93px;}
#tc_6{left:95px;bottom:850px;letter-spacing:-0.24px;word-spacing:1.78px;}
#td_6{left:488px;bottom:850px;}
#te_6{left:498px;bottom:850px;letter-spacing:-0.32px;word-spacing:2.02px;}
#tf_6{left:95px;bottom:829px;letter-spacing:-0.23px;word-spacing:0.69px;}
#tg_6{left:95px;bottom:808px;letter-spacing:-0.23px;word-spacing:0.61px;}
#th_6{left:95px;bottom:788px;letter-spacing:-0.2px;word-spacing:0.58px;}
#ti_6{left:121px;bottom:757px;letter-spacing:-0.26px;word-spacing:0.66px;}
#tj_6{left:207px;bottom:757px;letter-spacing:-0.19px;word-spacing:0.62px;}
#tk_6{left:322px;bottom:757px;}
#tl_6{left:331px;bottom:757px;letter-spacing:-0.18px;}
#tm_6{left:376px;bottom:757px;letter-spacing:-0.18px;word-spacing:0.6px;}
#tn_6{left:95px;bottom:736px;letter-spacing:-0.2px;word-spacing:0.55px;}
#to_6{left:95px;bottom:715px;letter-spacing:-0.18px;word-spacing:0.48px;}
#tp_6{left:95px;bottom:694px;letter-spacing:-0.16px;word-spacing:0.35px;}
#tq_6{left:95px;bottom:674px;letter-spacing:-0.27px;word-spacing:0.66px;}
#tr_6{left:95px;bottom:653px;letter-spacing:-0.23px;word-spacing:0.54px;}
#ts_6{left:95px;bottom:632px;letter-spacing:-0.19px;word-spacing:0.55px;}
#tt_6{left:695px;bottom:632px;letter-spacing:-0.42px;word-spacing:1.06px;}
#tu_6{left:762px;bottom:632px;}
#tv_6{left:772px;bottom:632px;letter-spacing:-0.19px;}
#tw_6{left:809px;bottom:632px;}
#tx_6{left:95px;bottom:612px;letter-spacing:-0.22px;word-spacing:1.4px;}
#ty_6{left:95px;bottom:591px;letter-spacing:-0.2px;word-spacing:0.55px;}
#tz_6{left:95px;bottom:570px;letter-spacing:-0.2px;word-spacing:0.52px;}
#t10_6{left:95px;bottom:550px;letter-spacing:-0.22px;word-spacing:0.62px;}
#t11_6{left:95px;bottom:529px;letter-spacing:-0.18px;word-spacing:0.53px;}
#t12_6{left:121px;bottom:498px;letter-spacing:-0.21px;word-spacing:0.22px;}
#t13_6{left:95px;bottom:477px;letter-spacing:-0.21px;word-spacing:0.68px;}
#t14_6{left:95px;bottom:456px;letter-spacing:-0.17px;word-spacing:0.46px;}
#t15_6{left:412px;bottom:456px;letter-spacing:-0.23px;word-spacing:0.63px;}
#t16_6{left:496px;bottom:456px;}
#t17_6{left:505px;bottom:456px;letter-spacing:-0.19px;}
#t18_6{left:542px;bottom:456px;letter-spacing:-0.17px;word-spacing:0.49px;}
#t19_6{left:95px;bottom:436px;letter-spacing:-0.19px;word-spacing:1.26px;}
#t1a_6{left:576px;bottom:436px;letter-spacing:-0.42px;word-spacing:1.72px;}
#t1b_6{left:660px;bottom:436px;}
#t1c_6{left:670px;bottom:436px;letter-spacing:-0.19px;}
#t1d_6{left:708px;bottom:436px;letter-spacing:-0.19px;word-spacing:1.24px;}
#t1e_6{left:95px;bottom:415px;letter-spacing:-0.18px;word-spacing:0.5px;}
#t1f_6{left:95px;bottom:394px;letter-spacing:-0.2px;word-spacing:-0.64px;}
#t1g_6{left:94px;bottom:374px;letter-spacing:-0.21px;word-spacing:0.6px;}
#t1h_6{left:95px;bottom:353px;letter-spacing:-0.19px;word-spacing:0.65px;}
#t1i_6{left:95px;bottom:332px;letter-spacing:-0.21px;word-spacing:0.56px;}
#t1j_6{left:95px;bottom:311px;letter-spacing:-0.19px;word-spacing:0.58px;}
#t1k_6{left:95px;bottom:291px;letter-spacing:-0.25px;word-spacing:0.68px;}
#t1l_6{left:121px;bottom:260px;letter-spacing:-0.19px;word-spacing:0.53px;}
#t1m_6{left:95px;bottom:239px;letter-spacing:-0.22px;word-spacing:0.52px;}
#t1n_6{left:95px;bottom:218px;letter-spacing:-0.2px;word-spacing:0.54px;}
#t1o_6{left:543px;bottom:218px;letter-spacing:-0.15px;}
#t1p_6{left:585px;bottom:218px;letter-spacing:-0.21px;word-spacing:0.55px;}
#t1q_6{left:95px;bottom:198px;letter-spacing:-0.22px;word-spacing:0.56px;}
#t1r_6{left:613px;bottom:198px;letter-spacing:-0.19px;word-spacing:0.54px;}
#t1s_6{left:692px;bottom:198px;}
#t1t_6{left:701px;bottom:198px;letter-spacing:-0.19px;}
#t1u_6{left:738px;bottom:198px;letter-spacing:-0.22px;word-spacing:0.55px;}
#t1v_6{left:95px;bottom:177px;letter-spacing:-0.18px;word-spacing:-0.38px;}
#t1w_6{left:95px;bottom:156px;letter-spacing:-0.22px;word-spacing:1.77px;}
#t1x_6{left:95px;bottom:136px;letter-spacing:-0.2px;word-spacing:0.56px;}
#t1y_6{left:807px;bottom:68px;}

.s0_6{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_6{font-size:20px;font-family:XCharter-Bold_vg;color:#000;}
.s2_6{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s3_6{font-size:17px;font-family:XCharter-Bold_vg;color:#000;}
.s4_6{font-size:17px;font-family:XCharter-BoldItalic_6q;color:#000;}
.s5_6{font-size:17px;font-family:XCharter-Roman_vh;color:#00F;}
.t.v0_6{transform:scaleX(1.02);}
.t.v1_6{transform:scaleX(1.01);}
.t.v2_6{transform:scaleX(0.988);}
.t.v3_6{transform:scaleX(1.016);}
.t.v4_6{transform:scaleX(0.984);}
.t.v5_6{transform:scaleX(0.979);}
.t.v6_6{transform:scaleX(1.011);}
.t.v7_6{transform:scaleX(1.014);}
.t.v8_6{transform:scaleX(0.987);}
.t.v9_6{transform:scaleX(1.015);}
.t.v10_6{transform:scaleX(1.018);}
.t.v11_6{transform:scaleX(1.006);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts6" type="text/css" >

@font-face {
	font-family: XCharter-BoldItalic_6q;
	src: url("fonts/XCharter-BoldItalic_6q.woff") format("woff");
}

@font-face {
	font-family: XCharter-Bold_vg;
	src: url("fonts/XCharter-Bold_vg.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg6Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg6" style="-webkit-user-select: none;"><object width="909" height="1286" data="6/6.svg" type="image/svg+xml" id="pdf6" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_6" class="t s0_6">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_6" class="t s1_6">5. Evaluation </span>
<span id="t3_6" class="t v0_6 s2_6">The Gemini models are natively multimodal, as they are trained jointly across text, image, audio, </span>
<span id="t4_6" class="t v1_6 s2_6">and video. One open question is whether this joint training can result in a model which has strong </span>
<span id="t5_6" class="t v0_6 s2_6">capabilities in each domain – even when compared to models and approaches that are narrowly </span>
<span id="t6_6" class="t v0_6 s2_6" data-mappings='[[31,"fi"]]'>tailored to single domains. We ﬁnd this to be the case: Gemini sets a new state of the art across a </span>
<span id="t7_6" class="t s2_6">wide range of text, image, audio, and video benchmarks. </span>
<span id="t8_6" class="t s3_6">5.1. Text </span>
<span id="t9_6" class="t s4_6">5.1.1. Academic Benchmarks </span>
<span id="ta_6" class="t v0_6 s2_6">We compare Gemini Pro and Ultra to a suite of external LLMs and our previous best model PaLM </span>
<span id="tb_6" class="t v0_6 s2_6">2 across a series of text-based academic benchmarks covering reasoning, reading comprehension, </span>
<span id="tc_6" class="t v0_6 s2_6">STEM, and coding. We report these results in Table </span><span id="td_6" class="t v0_6 s5_6">2</span><span id="te_6" class="t v0_6 s2_6" data-mappings='[[14,"fi"]]'>. Broadly, we ﬁnd that the performance of </span>
<span id="tf_6" class="t v2_6 s2_6">Gemini Pro outperforms inference-optimized models such as GPT-3.5 and performs comparably with </span>
<span id="tg_6" class="t v3_6 s2_6">several of the most capable models available, and Gemini Ultra outperforms all current models. In </span>
<span id="th_6" class="t s2_6" data-mappings='[[39,"fi"]]'>this section, we examine some of these ﬁndings. </span>
<span id="ti_6" class="t v4_6 s2_6">On MMLU (</span><span id="tj_6" class="t v4_6 s5_6">Hendrycks et al.</span><span id="tk_6" class="t v4_6 s2_6">, </span><span id="tl_6" class="t v4_6 s5_6">2021a</span><span id="tm_6" class="t v4_6 s2_6">), Gemini Ultra can outperform all existing models, achieving </span>
<span id="tn_6" class="t v3_6 s2_6">an accuracy of 90.04%. MMLU is a holistic exam benchmark, which measures knowledge across a </span>
<span id="to_6" class="t v3_6 s2_6">set of 57 subjects. Human expert performance is gauged at 89.8% by the benchmark authors, and </span>
<span id="tp_6" class="t v5_6 s2_6" data-mappings='[[20,"fi"]]'>Gemini Ultra is the ﬁrst model to exceed this threshold, with the prior state-of-the-art result at 86.4%. </span>
<span id="tq_6" class="t v3_6 s2_6">Achieving high performance requires specialist knowledge across many domains (e.g. law, biology, </span>
<span id="tr_6" class="t v5_6 s2_6" data-mappings='[[66,"fi"]]'>history, etc.), alongside reading comprehension and reasoning. We ﬁnd Gemini Ultra achieves highest </span>
<span id="ts_6" class="t s2_6">accuracy when used in combination with a chain-of-thought prompting approach (</span><span id="tt_6" class="t s5_6">Wei et al.</span><span id="tu_6" class="t s2_6">, </span><span id="tv_6" class="t s5_6">2022</span><span id="tw_6" class="t s2_6">) </span>
<span id="tx_6" class="t v0_6 s2_6">that accounts for model uncertainty. The model produces a chain of thought with k samples, for </span>
<span id="ty_6" class="t v6_6 s2_6">example 8 or 32. If there is a consensus above a preset threshold (selected based on the validation </span>
<span id="tz_6" class="t v3_6 s2_6">split), it selects this answer, otherwise it reverts to a greedy sample based on maximum likelihood </span>
<span id="t10_6" class="t v6_6 s2_6">choice without chain of thought. We refer the reader to appendix for a detailed breakdown of how </span>
<span id="t11_6" class="t s2_6">this approach compares with only chain-of-thought prompting or only greedy sampling. </span>
<span id="t12_6" class="t v5_6 s2_6" data-mappings='[[18,"fi"]]'>In mathematics, a ﬁeld commonly used to benchmark the analytical capabilities of models, Gemini </span>
<span id="t13_6" class="t v2_6 s2_6">Ultra shows strong performance on both elementary exams and competition-grade problem sets. For </span>
<span id="t14_6" class="t v5_6 s2_6">the grade-school math benchmark, GSM8K (</span><span id="t15_6" class="t v5_6 s5_6">Cobbe et al.</span><span id="t16_6" class="t v5_6 s2_6">, </span><span id="t17_6" class="t v5_6 s5_6">2021</span><span id="t18_6" class="t v5_6 s2_6" data-mappings='[[6,"fi"]]'>), we ﬁnd Gemini Ultra reaches 94.4% </span>
<span id="t19_6" class="t v0_6 s2_6">accuracy with chain-of-thought prompting and self-consistency (</span><span id="t1a_6" class="t v0_6 s5_6">Wang et al.</span><span id="t1b_6" class="t v0_6 s2_6">, </span><span id="t1c_6" class="t v0_6 s5_6">2022</span><span id="t1d_6" class="t v0_6 s2_6">) compared to </span>
<span id="t1e_6" class="t v7_6 s2_6">the previous best accuracy of 92% with the same prompting technique. Similar positive trends are </span>
<span id="t1f_6" class="t v5_6 s2_6" data-mappings='[[24,"ffi"]]'>observed in increased diﬃculty math problems drawn from middle- and high-school math competitions </span>
<span id="t1g_6" class="t v0_6 s2_6">(MATH benchmark), with the Gemini Ultra model outperforming all competitor models, reaching </span>
<span id="t1h_6" class="t v8_6 s2_6">53.2% using 4-shot prompting. The model also outperforms the state of the art on even harder tasks </span>
<span id="t1i_6" class="t v9_6 s2_6">derived from American Mathematical Competitions (150 questions from 2022 and 2023). Smaller </span>
<span id="t1j_6" class="t s2_6">models perform poorly on this challenging task scoring close to random, but Gemini Ultra can solve </span>
<span id="t1k_6" class="t s2_6">32% of the questions, compared to the 30% solve rate for GPT-4. </span>
<span id="t1l_6" class="t v7_6 s2_6">Gemini Ultra also excels in coding, a popular use case of current LLMs. We evaluate the model </span>
<span id="t1m_6" class="t v10_6 s2_6">on many conventional and internal benchmarks and also measure its performance as part of more </span>
<span id="t1n_6" class="t v11_6 s2_6">complex reasoning systems such as AlphaCode 2 (see section </span><span id="t1o_6" class="t v11_6 s5_6">5.1.7 </span><span id="t1p_6" class="t v11_6 s2_6">on complex reasoning systems). </span>
<span id="t1q_6" class="t v1_6 s2_6">For example, on HumanEval, a standard code-completion benchmark (</span><span id="t1r_6" class="t v1_6 s5_6">Chen et al.</span><span id="t1s_6" class="t v1_6 s2_6">, </span><span id="t1t_6" class="t v1_6 s5_6">2021</span><span id="t1u_6" class="t v1_6 s2_6">) mapping </span>
<span id="t1v_6" class="t v5_6 s2_6">function descriptions to Python implementations, instruction-tuned Gemini Ultra correctly implements </span>
<span id="t1w_6" class="t v0_6 s2_6">74.4% of problems. On a new held-out evaluation benchmark for python code generation tasks, </span>
<span id="t1x_6" class="t s2_6">Natural2Code, where we ensure no web leakage, Gemini Ultra achieves the highest score of 74.9%. </span>
<span id="t1y_6" class="t s0_6">6 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
