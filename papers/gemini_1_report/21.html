<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p21" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_21{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_21{left:95px;bottom:1132px;letter-spacing:-0.19px;word-spacing:2.52px;}
#t3_21{left:95px;bottom:1098px;letter-spacing:-0.19px;word-spacing:0.49px;}
#t4_21{left:95px;bottom:1077px;letter-spacing:-0.2px;word-spacing:2.53px;}
#t5_21{left:472px;bottom:1077px;letter-spacing:-0.23px;word-spacing:2.61px;}
#t6_21{left:95px;bottom:1056px;letter-spacing:-0.18px;word-spacing:1.62px;}
#t7_21{left:95px;bottom:1036px;letter-spacing:-0.16px;word-spacing:0.5px;}
#t8_21{left:671px;bottom:1036px;letter-spacing:-0.19px;word-spacing:0.59px;}
#t9_21{left:735px;bottom:1036px;}
#ta_21{left:744px;bottom:1036px;letter-spacing:-0.18px;}
#tb_21{left:790px;bottom:1036px;letter-spacing:-0.12px;}
#tc_21{left:121px;bottom:1005px;letter-spacing:-0.19px;word-spacing:0.63px;}
#td_21{left:95px;bottom:984px;letter-spacing:-0.2px;word-spacing:1.19px;}
#te_21{left:95px;bottom:963px;letter-spacing:-0.19px;word-spacing:-0.19px;}
#tf_21{left:95px;bottom:942px;letter-spacing:-0.17px;word-spacing:0.03px;}
#tg_21{left:617px;bottom:942px;letter-spacing:-0.32px;word-spacing:0.35px;}
#th_21{left:714px;bottom:942px;}
#ti_21{left:723px;bottom:942px;letter-spacing:-0.18px;}
#tj_21{left:768px;bottom:942px;}
#tk_21{left:777px;bottom:942px;letter-spacing:-0.17px;}
#tl_21{left:95px;bottom:922px;letter-spacing:-0.22px;word-spacing:0.98px;}
#tm_21{left:132px;bottom:922px;}
#tn_21{left:142px;bottom:922px;letter-spacing:-0.19px;}
#to_21{left:180px;bottom:922px;letter-spacing:-0.22px;word-spacing:1.08px;}
#tp_21{left:95px;bottom:901px;letter-spacing:-0.21px;word-spacing:0.54px;}
#tq_21{left:95px;bottom:880px;letter-spacing:-0.19px;word-spacing:1.29px;}
#tr_21{left:95px;bottom:860px;letter-spacing:-0.2px;word-spacing:0.68px;}
#ts_21{left:95px;bottom:839px;letter-spacing:-0.21px;word-spacing:0.57px;}
#tt_21{left:121px;bottom:808px;letter-spacing:-0.2px;word-spacing:0.59px;}
#tu_21{left:95px;bottom:787px;letter-spacing:-0.19px;word-spacing:1.35px;}
#tv_21{left:95px;bottom:767px;letter-spacing:-0.21px;word-spacing:0.21px;}
#tw_21{left:95px;bottom:746px;letter-spacing:-0.17px;word-spacing:1.81px;}
#tx_21{left:95px;bottom:725px;letter-spacing:-0.18px;word-spacing:0.53px;}
#ty_21{left:121px;bottom:694px;letter-spacing:-0.21px;word-spacing:0.62px;}
#tz_21{left:95px;bottom:673px;letter-spacing:-0.21px;word-spacing:0.59px;}
#t10_21{left:95px;bottom:653px;letter-spacing:-0.17px;word-spacing:0.51px;}
#t11_21{left:95px;bottom:632px;letter-spacing:-0.21px;word-spacing:1.17px;}
#t12_21{left:95px;bottom:611px;letter-spacing:-0.19px;word-spacing:0.5px;}
#t13_21{left:747px;bottom:611px;letter-spacing:-0.19px;word-spacing:0.53px;}
#t14_21{left:812px;bottom:611px;}
#t15_21{left:95px;bottom:591px;letter-spacing:-0.18px;}
#t16_21{left:141px;bottom:591px;letter-spacing:-0.17px;word-spacing:0.53px;}
#t17_21{left:95px;bottom:570px;letter-spacing:-0.18px;word-spacing:0.49px;}
#t18_21{left:494px;bottom:570px;letter-spacing:-0.22px;word-spacing:0.59px;}
#t19_21{left:586px;bottom:570px;}
#t1a_21{left:596px;bottom:570px;letter-spacing:-0.19px;}
#t1b_21{left:633px;bottom:570px;letter-spacing:-0.17px;word-spacing:0.48px;}
#t1c_21{left:95px;bottom:549px;letter-spacing:-0.22px;word-spacing:0.44px;}
#t1d_21{left:95px;bottom:528px;letter-spacing:-0.19px;word-spacing:0.48px;}
#t1e_21{left:95px;bottom:508px;letter-spacing:-0.18px;word-spacing:0.53px;}
#t1f_21{left:95px;bottom:463px;letter-spacing:-0.18px;word-spacing:4.23px;}
#t1g_21{left:95px;bottom:429px;letter-spacing:-0.16px;word-spacing:0.44px;}
#t1h_21{left:95px;bottom:408px;letter-spacing:-0.22px;word-spacing:0.59px;}
#t1i_21{left:95px;bottom:387px;letter-spacing:-0.2px;word-spacing:0.58px;}
#t1j_21{left:115px;bottom:350px;letter-spacing:-0.14px;}
#t1k_21{left:137px;bottom:350px;letter-spacing:-0.25px;}
#t1l_21{left:223px;bottom:350px;letter-spacing:-0.18px;word-spacing:1.69px;}
#t1m_21{left:137px;bottom:329px;letter-spacing:-0.17px;word-spacing:-0.11px;}
#t1n_21{left:137px;bottom:309px;letter-spacing:-0.17px;word-spacing:1.74px;}
#t1o_21{left:255px;bottom:309px;letter-spacing:-0.23px;word-spacing:1.85px;}
#t1p_21{left:357px;bottom:309px;}
#t1q_21{left:368px;bottom:309px;letter-spacing:-0.19px;}
#t1r_21{left:406px;bottom:309px;letter-spacing:-0.19px;word-spacing:2.24px;}
#t1s_21{left:137px;bottom:288px;letter-spacing:-0.2px;word-spacing:0.65px;}
#t1t_21{left:763px;bottom:288px;letter-spacing:-0.22px;}
#t1u_21{left:137px;bottom:267px;letter-spacing:-0.23px;word-spacing:2.33px;}
#t1v_21{left:182px;bottom:267px;}
#t1w_21{left:189px;bottom:267px;letter-spacing:-0.19px;}
#t1x_21{left:227px;bottom:267px;letter-spacing:-0.12px;}
#t1y_21{left:246px;bottom:267px;letter-spacing:-0.26px;word-spacing:2.38px;}
#t1z_21{left:333px;bottom:267px;}
#t20_21{left:340px;bottom:267px;letter-spacing:-0.19px;}
#t21_21{left:378px;bottom:267px;letter-spacing:-0.23px;word-spacing:2.34px;}
#t22_21{left:137px;bottom:246px;letter-spacing:-0.16px;word-spacing:0.08px;}
#t23_21{left:183px;bottom:246px;letter-spacing:-0.27px;word-spacing:0.34px;}
#t24_21{left:287px;bottom:246px;}
#t25_21{left:295px;bottom:246px;letter-spacing:-0.19px;}
#t26_21{left:332px;bottom:246px;letter-spacing:-0.21px;word-spacing:0.32px;}
#t27_21{left:137px;bottom:226px;letter-spacing:-0.19px;word-spacing:0.55px;}
#t28_21{left:115px;bottom:205px;letter-spacing:-0.14px;}
#t29_21{left:137px;bottom:205px;letter-spacing:-0.27px;word-spacing:0.95px;}
#t2a_21{left:395px;bottom:205px;letter-spacing:-0.17px;word-spacing:0.67px;}
#t2b_21{left:137px;bottom:184px;letter-spacing:-0.2px;word-spacing:1.21px;}
#t2c_21{left:718px;bottom:184px;letter-spacing:-0.24px;word-spacing:1.29px;}
#t2d_21{left:136px;bottom:164px;}
#t2e_21{left:142px;bottom:164px;letter-spacing:-0.19px;}
#t2f_21{left:179px;bottom:164px;letter-spacing:-0.21px;word-spacing:0.26px;}
#t2g_21{left:137px;bottom:143px;letter-spacing:-0.18px;word-spacing:0.7px;}
#t2h_21{left:136px;bottom:122px;letter-spacing:-0.27px;word-spacing:0.77px;}
#t2i_21{left:801px;bottom:68px;letter-spacing:0.1px;}

.s0_21{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_21{font-size:17px;font-family:XCharter-BoldItalic_6q;color:#000;}
.s2_21{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s3_21{font-size:17px;font-family:XCharter-Roman_vh;color:#00F;}
.s4_21{font-size:17px;font-family:XCharter-Bold_vg;color:#000;}
.t.v0_21{transform:scaleX(1.02);}
.t.v1_21{transform:scaleX(0.987);}
.t.v2_21{transform:scaleX(0.979);}
.t.v3_21{transform:scaleX(1.01);}
.t.v4_21{transform:scaleX(0.98);}
.t.v5_21{transform:scaleX(1.015);}
.t.v6_21{transform:scaleX(0.99);}
.t.v7_21{transform:scaleX(1.008);}
.t.v8_21{transform:scaleX(1.018);}
.t.v9_21{transform:scaleX(0.984);}
.t.v10_21{transform:scaleX(0.982);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts21" type="text/css" >

@font-face {
	font-family: XCharter-BoldItalic_6q;
	src: url("fonts/XCharter-BoldItalic_6q.woff") format("woff");
}

@font-face {
	font-family: XCharter-Bold_vg;
	src: url("fonts/XCharter-Bold_vg.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg21Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg21" style="-webkit-user-select: none;"><object width="909" height="1286" data="21/21.svg" type="image/svg+xml" id="pdf21" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_21" class="t s0_21">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_21" class="t s1_21">6.4.2. Instruction Tuning </span>
<span id="t3_21" class="t v0_21 s2_21" data-mappings='[[42,"fi"]]'>Instruction tuning encompasses supervised ﬁne tuning (SFT) and reinforcement learning through </span>
<span id="t4_21" class="t v0_21 s2_21">human feedback (RLHF) using a reward model. </span><span id="t5_21" class="t v0_21 s2_21">We apply instruction tuning in both text and </span>
<span id="t6_21" class="t v0_21 s2_21">multimodal settings. Instruction tuning recipes are carefully designed to balance the increase in </span>
<span id="t7_21" class="t s2_21">helpfulness with decrease in model harms related to safety and hallucinations (</span><span id="t8_21" class="t s3_21">Bai et al.</span><span id="t9_21" class="t s2_21">, </span><span id="ta_21" class="t s3_21">2022a</span><span id="tb_21" class="t s2_21">). </span>
<span id="tc_21" class="t v1_21 s2_21">Curation of “quality” data is critical for SFT, reward model training, and RLHF. The data mixture </span>
<span id="td_21" class="t v0_21 s2_21">ratios are ablated with smaller models to balance the metrics on helpfulness (such as instruction </span>
<span id="te_21" class="t v2_21 s2_21">following, creativity) and reduction of model harms, and these results generalize well to larger models. </span>
<span id="tf_21" class="t v2_21 s2_21">We have also observed that data quality is more important than quantity (</span><span id="tg_21" class="t v2_21 s3_21">Touvron et al.</span><span id="th_21" class="t v2_21 s2_21">, </span><span id="ti_21" class="t v2_21 s3_21">2023b</span><span id="tj_21" class="t v2_21 s2_21">; </span><span id="tk_21" class="t v2_21 s3_21">Zhou </span>
<span id="tl_21" class="t v0_21 s3_21">et al.</span><span id="tm_21" class="t v0_21 s2_21">, </span><span id="tn_21" class="t v0_21 s3_21">2023</span><span id="to_21" class="t v0_21 s2_21" data-mappings='[[74,"fi"]]'>), especially for larger models. Similarly, for reward model training, we ﬁnd it critical </span>
<span id="tp_21" class="t v3_21 s2_21">to balance the dataset with examples where the model prefers to say, “I cannot help with that,” for </span>
<span id="tq_21" class="t v0_21 s2_21">safety reasons and examples where the model outputs helpful responses. We use multi-objective </span>
<span id="tr_21" class="t v4_21 s2_21">optimization with a weighted sum of reward scores from helpfulness, factuality, and safety, to train a </span>
<span id="ts_21" class="t s2_21">multi-headed reward model. </span>
<span id="tt_21" class="t v5_21 s2_21">We further elaborate our approach to mitigate risks of harmful text generation. We enumerate </span>
<span id="tu_21" class="t v0_21 s2_21">approximately 20 harm types (e.g. hate speech, providing medical advice, suggesting dangerous </span>
<span id="tv_21" class="t v2_21 s2_21">behavior) across a wide variety of use cases. We generate a dataset of potential harm-inducing queries </span>
<span id="tw_21" class="t v0_21 s2_21">in these categories, either manually by policy experts and ML engineers, or via prompting high </span>
<span id="tx_21" class="t s2_21">capability language models with topical keywords as seeds. </span>
<span id="ty_21" class="t v6_21 s2_21">Given the harm-inducing queries, we probe our Gemini models and analyze the model responses </span>
<span id="tz_21" class="t v7_21 s2_21">via side-by-side evaluation. As discussed above, we balance the objective of model output response </span>
<span id="t10_21" class="t v7_21 s2_21">being harmless versus being helpful. From the detected risk areas, we create additional supervised </span>
<span id="t11_21" class="t v0_21 s2_21" data-mappings='[[0,"fi"]]'>ﬁne-tuning data to demonstrate the desirable responses. To generate such responses at scale, we </span>
<span id="t12_21" class="t v0_21 s2_21">heavily rely on a custom data generation recipe loosely inspired from Constitutional AI (</span><span id="t13_21" class="t v0_21 s3_21">Bai et al.</span><span id="t14_21" class="t v0_21 s2_21">, </span>
<span id="t15_21" class="t s3_21">2022b</span><span id="t16_21" class="t s2_21">), where we inject variants of Google’s content policy language as “constitutions”, and utilize </span>
<span id="t17_21" class="t v3_21 s2_21">language model’s strong zero-shot reasoning abilities (</span><span id="t18_21" class="t v3_21 s3_21">Kojima et al.</span><span id="t19_21" class="t v3_21 s2_21">, </span><span id="t1a_21" class="t v3_21 s3_21">2022</span><span id="t1b_21" class="t v3_21 s2_21">) to revise responses and </span>
<span id="t1c_21" class="t v2_21 s2_21" data-mappings='[[78,"ff"]]'>choose between multiple response candidates. We have found this recipe to be eﬀective – for example </span>
<span id="t1d_21" class="t v8_21 s2_21" data-mappings='[[80,"fi"]]'>in Gemini Pro, this overall recipe was able to mitigate a majority of our identiﬁed text harm cases, </span>
<span id="t1e_21" class="t s2_21">without any perceptible decrease on response helpfulness. </span>
<span id="t1f_21" class="t s1_21">6.4.3. Factuality </span>
<span id="t1g_21" class="t v3_21 s2_21">It is important that our models generate responses that are factual in a variety of scenarios, and to </span>
<span id="t1h_21" class="t v0_21 s2_21" data-mappings='[[71,"ff"]]'>reduce the frequency of hallucinations. We focused instruction tuning eﬀorts on three key desired </span>
<span id="t1i_21" class="t s2_21" data-mappings='[[13,"fl"]]'>behaviors, reﬂecting real-world scenarios: </span>
<span id="t1j_21" class="t s2_21">1. </span><span id="t1k_21" class="t v0_21 s4_21">Attribution</span><span id="t1l_21" class="t v0_21 s2_21">: If instructed to generate a response that should be fully attributed to a given </span>
<span id="t1m_21" class="t v2_21 s2_21">context in the prompt, Gemini should produce a response with the highest degree of faithfulness </span>
<span id="t1n_21" class="t v0_21 s2_21">to the context (</span><span id="t1o_21" class="t v0_21 s3_21">Rashkin et al.</span><span id="t1p_21" class="t v0_21 s2_21">, </span><span id="t1q_21" class="t v0_21 s3_21">2023</span><span id="t1r_21" class="t v0_21 s2_21">). This includes the summarization of a user-provided </span>
<span id="t1s_21" class="t v2_21 s2_21" data-mappings='[[19,"fi"]]'>source, generating ﬁne-grained citations given a question and provided snippets akin to </span><span id="t1t_21" class="t v2_21 s3_21">Menick </span>
<span id="t1u_21" class="t v0_21 s3_21">et al. </span><span id="t1v_21" class="t v0_21 s2_21">(</span><span id="t1w_21" class="t v0_21 s3_21">2022</span><span id="t1x_21" class="t v0_21 s2_21">); </span><span id="t1y_21" class="t v0_21 s3_21">Peng et al. </span><span id="t1z_21" class="t v0_21 s2_21">(</span><span id="t20_21" class="t v0_21 s3_21">2023</span><span id="t21_21" class="t v0_21 s2_21">), answering questions from a long-form source such as a </span>
<span id="t22_21" class="t v2_21 s2_21">book (</span><span id="t23_21" class="t v2_21 s3_21">Mihaylov et al.</span><span id="t24_21" class="t v2_21 s2_21">, </span><span id="t25_21" class="t v2_21 s3_21">2018</span><span id="t26_21" class="t v2_21 s2_21">), and transforming a given source to a desired output (e.g. an email </span>
<span id="t27_21" class="t s2_21">from a portion of a meeting transcript). </span>
<span id="t28_21" class="t s2_21">2. </span><span id="t29_21" class="t v9_21 s4_21">Closed-Book Response Generation</span><span id="t2a_21" class="t v9_21 s2_21">: If provided with a fact-seeking prompt without any given </span>
<span id="t2b_21" class="t v0_21 s2_21">source, Gemini should not hallucinate incorrect information (see Section 2 of </span><span id="t2c_21" class="t v0_21 s3_21">Roberts et al. </span>
<span id="t2d_21" class="t v2_21 s2_21">(</span><span id="t2e_21" class="t v2_21 s3_21">2020</span><span id="t2f_21" class="t v2_21 s2_21" data-mappings='[[10,"fi"]]'>) for a deﬁnition). These prompts can range from information-seeking prompts (e.g. “Who </span>
<span id="t2g_21" class="t v10_21 s2_21">is the prime minister of India?”) to semi-creative prompts that may request factual information </span>
<span id="t2h_21" class="t s2_21">(e.g. “Write a 500-word speech in favor of the adoption of renewable energy”). </span>
<span id="t2i_21" class="t s0_21">21 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
