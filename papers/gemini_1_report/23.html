<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p23" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_23{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_23{left:95px;bottom:1130px;letter-spacing:-0.19px;word-spacing:2.36px;}
#t3_23{left:95px;bottom:1093px;letter-spacing:-0.24px;word-spacing:1.72px;}
#t4_23{left:95px;bottom:1072px;letter-spacing:-0.18px;word-spacing:1.35px;}
#t5_23{left:95px;bottom:1052px;letter-spacing:-0.22px;word-spacing:1.21px;}
#t6_23{left:95px;bottom:1031px;letter-spacing:-0.17px;word-spacing:0.55px;}
#t7_23{left:95px;bottom:1010px;letter-spacing:-0.18px;word-spacing:0.53px;}
#t8_23{left:95px;bottom:990px;letter-spacing:-0.19px;word-spacing:0.13px;}
#t9_23{left:95px;bottom:969px;letter-spacing:-0.2px;word-spacing:1.3px;}
#ta_23{left:95px;bottom:948px;letter-spacing:-0.21px;word-spacing:1.57px;}
#tb_23{left:95px;bottom:928px;letter-spacing:-0.17px;word-spacing:0.53px;}
#tc_23{left:95px;bottom:907px;letter-spacing:-0.2px;word-spacing:0.47px;}
#td_23{left:95px;bottom:886px;letter-spacing:-0.2px;word-spacing:1.11px;}
#te_23{left:95px;bottom:865px;letter-spacing:-0.21px;word-spacing:0.58px;}
#tf_23{left:246px;bottom:865px;letter-spacing:-0.37px;word-spacing:1.02px;}
#tg_23{left:312px;bottom:865px;}
#th_23{left:321px;bottom:865px;letter-spacing:-0.19px;}
#ti_23{left:358px;bottom:865px;letter-spacing:-0.19px;word-spacing:0.64px;}
#tj_23{left:95px;bottom:845px;letter-spacing:-0.19px;word-spacing:0.55px;}
#tk_23{left:121px;bottom:814px;letter-spacing:-0.2px;word-spacing:-0.31px;}
#tl_23{left:95px;bottom:793px;letter-spacing:-0.22px;word-spacing:0.69px;}
#tm_23{left:95px;bottom:772px;letter-spacing:-0.21px;word-spacing:0.52px;}
#tn_23{left:95px;bottom:752px;letter-spacing:-0.2px;word-spacing:0.38px;}
#to_23{left:95px;bottom:731px;letter-spacing:-0.2px;word-spacing:1.2px;}
#tp_23{left:95px;bottom:710px;letter-spacing:-0.21px;word-spacing:0.64px;}
#tq_23{left:95px;bottom:690px;letter-spacing:-0.23px;word-spacing:0.71px;}
#tr_23{left:95px;bottom:669px;letter-spacing:-0.22px;word-spacing:0.61px;}
#ts_23{left:121px;bottom:638px;letter-spacing:-0.19px;word-spacing:0.65px;}
#tt_23{left:95px;bottom:617px;letter-spacing:-0.2px;word-spacing:0.49px;}
#tu_23{left:95px;bottom:596px;letter-spacing:-0.2px;word-spacing:0.6px;}
#tv_23{left:95px;bottom:576px;letter-spacing:-0.19px;word-spacing:-0.75px;}
#tw_23{left:95px;bottom:555px;letter-spacing:-0.2px;word-spacing:0.72px;}
#tx_23{left:95px;bottom:534px;letter-spacing:-0.2px;word-spacing:0.8px;}
#ty_23{left:95px;bottom:514px;letter-spacing:-0.17px;word-spacing:0.5px;}
#tz_23{left:121px;bottom:483px;letter-spacing:-0.19px;word-spacing:0.47px;}
#t10_23{left:95px;bottom:462px;letter-spacing:-0.22px;word-spacing:0.72px;}
#t11_23{left:95px;bottom:441px;letter-spacing:-0.23px;word-spacing:0.8px;}
#t12_23{left:95px;bottom:420px;letter-spacing:-0.23px;word-spacing:0.62px;}
#t13_23{left:95px;bottom:400px;letter-spacing:-0.19px;word-spacing:0.55px;}
#t14_23{left:95px;bottom:379px;letter-spacing:-0.2px;word-spacing:0.58px;}
#t15_23{left:801px;bottom:68px;letter-spacing:0.1px;}

.s0_23{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_23{font-size:20px;font-family:XCharter-Bold_vg;color:#000;}
.s2_23{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s3_23{font-size:17px;font-family:XCharter-Roman_vh;color:#00F;}
.t.v0_23{transform:scaleX(1.02);}
.t.v1_23{transform:scaleX(0.979);}
.t.v2_23{transform:scaleX(0.995);}
.t.v3_23{transform:scaleX(0.988);}
.t.v4_23{transform:scaleX(1.016);}
.t.v5_23{transform:scaleX(0.992);}
.t.v6_23{transform:scaleX(0.983);}
.t.v7_23{transform:scaleX(1.011);}
.t.v8_23{transform:scaleX(0.987);}
.t.v9_23{transform:scaleX(1.018);}
.t.v10_23{transform:scaleX(1.006);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts23" type="text/css" >

@font-face {
	font-family: XCharter-Bold_vg;
	src: url("fonts/XCharter-Bold_vg.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg23Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg23" style="-webkit-user-select: none;"><object width="909" height="1286" data="23/23.svg" type="image/svg+xml" id="pdf23" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_23" class="t s0_23">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_23" class="t s1_23">7. Discussion and Conclusion </span>
<span id="t3_23" class="t v0_23 s2_23">We have presented Gemini, a new family of models that advance multimodal model capabilities </span>
<span id="t4_23" class="t v0_23 s2_23">in text, code, image, audio, and video. This technical report evaluates the capabilities of Gemini </span>
<span id="t5_23" class="t v0_23 s2_23">on a diverse set of widely-studied benchmarks, and our most capable model Gemini Ultra makes </span>
<span id="t6_23" class="t s2_23" data-mappings='[[5,"fi"]]'>signiﬁcant advances across the board. In the natural language domain, the performance gains from </span>
<span id="t7_23" class="t s2_23">careful developments in data and model training at scale continue to deliver quality improvements, </span>
<span id="t8_23" class="t v1_23 s2_23">setting new state of the art in several benchmarks. In particular, Gemini Ultra surpasses human-expert </span>
<span id="t9_23" class="t v0_23 s2_23">performance on the exam benchmark MMLU, scoring 90.0%, which has been a defacto measure </span>
<span id="ta_23" class="t v0_23 s2_23" data-mappings='[[39,"fi"]]'>of progress for LLMs ever since it was ﬁrst released in 2020. In the multimodal domain, Gemini </span>
<span id="tb_23" class="t v2_23 s2_23">Ultra sets new state of the art on most of the image understanding, video understanding, and audio </span>
<span id="tc_23" class="t v1_23 s2_23" data-mappings='[[43,"fi"],[50,"fi"]]'>understanding benchmarks without task-speciﬁc modiﬁcations or tuning. In particular, Gemini Ultra’s </span>
<span id="td_23" class="t v0_23 s2_23">multimodal reasoning capabilities are evident from its state-of-the-art performance on the recent </span>
<span id="te_23" class="t v3_23 s2_23">MMMU benchmark (</span><span id="tf_23" class="t v3_23 s3_23">Yue et al.</span><span id="tg_23" class="t v3_23 s2_23">, </span><span id="th_23" class="t v3_23 s3_23">2023</span><span id="ti_23" class="t v3_23 s2_23">), that comprises questions about images requiring college-level </span>
<span id="tj_23" class="t s2_23">subject knowledge and deliberate reasoning. </span>
<span id="tk_23" class="t v1_23 s2_23">Beyond the state-of-art results on benchmarks, what we are most excited about is the new use cases </span>
<span id="tl_23" class="t v3_23 s2_23">enabled by Gemini models. The new capabilities of Gemini models to parse complex images, such as </span>
<span id="tm_23" class="t v4_23 s2_23">charts or infographics, reason over interleaved sequences of images, audio, and text, and generate </span>
<span id="tn_23" class="t v1_23 s2_23" data-mappings='[[94,"fi"]]'>interleaved text and images as responses open a wide variety of new applications. As shown in ﬁgures </span>
<span id="to_23" class="t v0_23 s2_23">throughout the report and appendix, Gemini can enable new approaches in areas like education, </span>
<span id="tp_23" class="t v5_23 s2_23">everyday problem solving, multilingual communication, information summarization, extraction, and </span>
<span id="tq_23" class="t v2_23 s2_23" data-mappings='[[58,"fi"],[79,"fi"]]'>creativity. We expect that the users of these models will ﬁnd all kinds of beneﬁcial new uses that we </span>
<span id="tr_23" class="t s2_23">have only scratched the surface of in our own investigations. </span>
<span id="ts_23" class="t v6_23 s2_23">Despite their impressive capabilities, we should note that there are limitations to the use of LLMs. </span>
<span id="tt_23" class="t v7_23 s2_23">There is a continued need for ongoing research and development on “hallucinations” generated by </span>
<span id="tu_23" class="t v0_23 s2_23" data-mappings='[[60,"fi"]]'>LLMs to ensure that model outputs are more reliable and veriﬁable. LLMs also struggle with tasks </span>
<span id="tv_23" class="t v1_23 s2_23">requiring high-level reasoning abilities like causal understanding, logical deduction, and counterfactual </span>
<span id="tw_23" class="t v8_23 s2_23">reasoning even though they achieve impressive performance on exam benchmarks. This underscores </span>
<span id="tx_23" class="t v0_23 s2_23">the need for more challenging and robust evaluations to measure their true understanding as the </span>
<span id="ty_23" class="t s2_23">current state-of-the-art LLMs saturate many benchmarks. </span>
<span id="tz_23" class="t v9_23 s2_23" data-mappings='[[92,"fi"]]'>Gemini is a further step towards our mission to solve intelligence, advance science and beneﬁt </span>
<span id="t10_23" class="t v1_23 s2_23">humanity, and we are enthusiastic to see how these models are used by our colleagues at Google and </span>
<span id="t11_23" class="t v0_23 s2_23">beyond. We build on many innovations in machine learning, data, infrastructure, and responsible </span>
<span id="t12_23" class="t v1_23 s2_23">development – areas that we have been pursuing at Google for over a decade. The models we present </span>
<span id="t13_23" class="t v10_23 s2_23">in this report provide a strong foundation towards our broader future goal to develop a large-scale, </span>
<span id="t14_23" class="t s2_23">modularized system that will have broad generalization capabilities across many modalities. </span>
<span id="t15_23" class="t s0_23">23 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
