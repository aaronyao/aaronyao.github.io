<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p5" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_5{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_5{left:95px;bottom:1132px;letter-spacing:-0.19px;word-spacing:0.61px;}
#t3_5{left:95px;bottom:1111px;letter-spacing:-0.17px;word-spacing:0.51px;}
#t4_5{left:121px;bottom:1080px;letter-spacing:-0.2px;word-spacing:0.61px;}
#t5_5{left:485px;bottom:1080px;letter-spacing:-0.29px;word-spacing:0.81px;}
#t6_5{left:591px;bottom:1080px;}
#t7_5{left:600px;bottom:1080px;letter-spacing:-0.19px;}
#t8_5{left:637px;bottom:1080px;letter-spacing:-0.3px;word-spacing:0.8px;}
#t9_5{left:758px;bottom:1080px;letter-spacing:-0.19px;}
#ta_5{left:95px;bottom:1059px;letter-spacing:-0.22px;word-spacing:2.17px;}
#tb_5{left:133px;bottom:1059px;}
#tc_5{left:144px;bottom:1059px;letter-spacing:-0.19px;}
#td_5{left:182px;bottom:1059px;letter-spacing:-0.18px;word-spacing:2.09px;}
#te_5{left:95px;bottom:1039px;letter-spacing:-0.24px;word-spacing:0.81px;}
#tf_5{left:557px;bottom:1039px;letter-spacing:-0.21px;word-spacing:0.7px;}
#tg_5{left:616px;bottom:1039px;}
#th_5{left:625px;bottom:1039px;letter-spacing:-0.19px;}
#ti_5{left:661px;bottom:1039px;letter-spacing:-0.18px;word-spacing:0.6px;}
#tj_5{left:95px;bottom:1018px;letter-spacing:-0.18px;word-spacing:0.39px;}
#tk_5{left:632px;bottom:1018px;letter-spacing:-0.2px;}
#tl_5{left:661px;bottom:1018px;}
#tm_5{left:670px;bottom:1018px;letter-spacing:-0.19px;}
#tn_5{left:706px;bottom:1018px;letter-spacing:-0.18px;word-spacing:0.43px;}
#to_5{left:95px;bottom:997px;letter-spacing:-0.19px;word-spacing:0.38px;}
#tp_5{left:95px;bottom:977px;letter-spacing:-0.17px;word-spacing:0.54px;}
#tq_5{left:121px;bottom:946px;letter-spacing:-0.18px;word-spacing:0.87px;}
#tr_5{left:329px;bottom:954px;}
#ts_5{left:341px;bottom:946px;letter-spacing:-0.24px;word-spacing:1.02px;}
#tt_5{left:95px;bottom:925px;letter-spacing:-0.2px;word-spacing:0.61px;}
#tu_5{left:95px;bottom:904px;letter-spacing:-0.19px;word-spacing:-0.12px;}
#tv_5{left:95px;bottom:883px;letter-spacing:-0.22px;word-spacing:0.66px;}
#tw_5{left:785px;bottom:883px;letter-spacing:-0.15px;}
#tx_5{left:95px;bottom:863px;letter-spacing:-0.21px;word-spacing:0.7px;}
#ty_5{left:132px;bottom:863px;}
#tz_5{left:141px;bottom:863px;letter-spacing:-0.19px;}
#t10_5{left:179px;bottom:863px;letter-spacing:-0.17px;word-spacing:0.59px;}
#t11_5{left:95px;bottom:842px;letter-spacing:-0.18px;word-spacing:2.05px;}
#t12_5{left:95px;bottom:821px;letter-spacing:-0.21px;word-spacing:0.56px;}
#t13_5{left:121px;bottom:790px;letter-spacing:-0.18px;word-spacing:0.54px;}
#t14_5{left:95px;bottom:770px;letter-spacing:-0.2px;word-spacing:-0.04px;}
#t15_5{left:94px;bottom:749px;letter-spacing:-0.16px;word-spacing:0.43px;}
#t16_5{left:158px;bottom:749px;letter-spacing:-0.17px;word-spacing:0.53px;}
#t17_5{left:234px;bottom:749px;}
#t18_5{left:243px;bottom:749px;letter-spacing:-0.19px;}
#t19_5{left:280px;bottom:749px;}
#t1a_5{left:290px;bottom:749px;letter-spacing:-0.18px;word-spacing:0.52px;}
#t1b_5{left:412px;bottom:749px;}
#t1c_5{left:421px;bottom:749px;letter-spacing:-0.19px;}
#t1d_5{left:458px;bottom:749px;}
#t1e_5{left:468px;bottom:749px;letter-spacing:-0.2px;word-spacing:0.54px;}
#t1f_5{left:613px;bottom:749px;}
#t1g_5{left:622px;bottom:749px;letter-spacing:-0.19px;}
#t1h_5{left:659px;bottom:749px;letter-spacing:-0.16px;word-spacing:0.72px;}
#t1i_5{left:95px;bottom:728px;letter-spacing:-0.18px;word-spacing:0.46px;}
#t1j_5{left:95px;bottom:707px;letter-spacing:-0.2px;word-spacing:1.96px;}
#t1k_5{left:95px;bottom:687px;letter-spacing:-0.17px;word-spacing:1.04px;}
#t1l_5{left:95px;bottom:666px;letter-spacing:-0.2px;word-spacing:1.33px;}
#t1m_5{left:95px;bottom:645px;letter-spacing:-0.17px;word-spacing:0.52px;}
#t1n_5{left:95px;bottom:625px;letter-spacing:-0.2px;word-spacing:0.58px;}
#t1o_5{left:95px;bottom:568px;letter-spacing:-0.12px;word-spacing:2.92px;}
#t1p_5{left:95px;bottom:530px;letter-spacing:-0.19px;word-spacing:0.53px;}
#t1q_5{left:95px;bottom:510px;letter-spacing:-0.16px;word-spacing:0.54px;}
#t1r_5{left:121px;bottom:479px;letter-spacing:-0.2px;word-spacing:1.38px;}
#t1s_5{left:402px;bottom:479px;letter-spacing:-0.22px;word-spacing:1.4px;}
#t1t_5{left:563px;bottom:479px;}
#t1u_5{left:573px;bottom:479px;letter-spacing:-0.19px;}
#t1v_5{left:611px;bottom:479px;letter-spacing:-0.15px;word-spacing:1.29px;}
#t1w_5{left:95px;bottom:458px;letter-spacing:-0.19px;word-spacing:2.01px;}
#t1x_5{left:95px;bottom:437px;letter-spacing:-0.23px;word-spacing:2.01px;}
#t1y_5{left:95px;bottom:416px;letter-spacing:-0.17px;word-spacing:0.57px;}
#t1z_5{left:95px;bottom:396px;letter-spacing:-0.19px;}
#t20_5{left:121px;bottom:365px;letter-spacing:-0.2px;word-spacing:0.57px;}
#t21_5{left:95px;bottom:344px;letter-spacing:-0.14px;}
#t22_5{left:114px;bottom:344px;letter-spacing:-0.27px;word-spacing:0.67px;}
#t23_5{left:232px;bottom:344px;}
#t24_5{left:239px;bottom:344px;letter-spacing:-0.19px;}
#t25_5{left:277px;bottom:344px;letter-spacing:-0.21px;word-spacing:0.66px;}
#t26_5{left:95px;bottom:323px;letter-spacing:-0.2px;word-spacing:-0.16px;}
#t27_5{left:653px;bottom:323px;letter-spacing:-0.32px;word-spacing:0.08px;}
#t28_5{left:754px;bottom:323px;}
#t29_5{left:761px;bottom:323px;letter-spacing:-0.18px;}
#t2a_5{left:806px;bottom:323px;letter-spacing:-0.12px;}
#t2b_5{left:121px;bottom:292px;letter-spacing:-0.18px;word-spacing:1.38px;}
#t2c_5{left:95px;bottom:272px;letter-spacing:-0.22px;word-spacing:0.57px;}
#t2d_5{left:95px;bottom:251px;letter-spacing:-0.19px;word-spacing:0.63px;}
#t2e_5{left:95px;bottom:230px;letter-spacing:-0.21px;word-spacing:0.75px;}
#t2f_5{left:95px;bottom:209px;letter-spacing:-0.17px;word-spacing:0.66px;}
#t2g_5{left:95px;bottom:189px;letter-spacing:-0.19px;word-spacing:0.46px;}
#t2h_5{left:95px;bottom:168px;letter-spacing:-0.18px;word-spacing:0.56px;}
#t2i_5{left:113px;bottom:144px;}
#t2j_5{left:120px;bottom:138px;letter-spacing:-0.18px;word-spacing:0.48px;}
#t2k_5{left:807px;bottom:68px;}

.s0_5{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_5{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s2_5{font-size:17px;font-family:XCharter-Roman_vh;color:#00F;}
.s3_5{font-size:12px;font-family:XCharter-Roman_vh;color:#00F;}
.s4_5{font-size:20px;font-family:XCharter-Bold_vg;color:#000;}
.s5_5{font-size:11px;font-family:XCharter-Roman_vh;color:#000;}
.s6_5{font-size:14px;font-family:XCharter-Roman_vh;color:#000;}
.t.v0_5{transform:scaleX(0.984);}
.t.v1_5{transform:scaleX(0.988);}
.t.v2_5{transform:scaleX(1.02);}
.t.v3_5{transform:scaleX(0.983);}
.t.v4_5{transform:scaleX(0.979);}
.t.v5_5{transform:scaleX(1.005);}
.t.v6_5{transform:scaleX(1.007);}
.t.v7_5{transform:scaleX(1.014);}
.t.v8_5{transform:scaleX(0.993);}
.t.v9_5{transform:scaleX(1.016);}
.t.v10_5{transform:scaleX(0.987);}
.t.v11_5{transform:scaleX(1.011);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts5" type="text/css" >

@font-face {
	font-family: XCharter-Bold_vg;
	src: url("fonts/XCharter-Bold_vg.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg5Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg5" style="-webkit-user-select: none;"><object width="909" height="1286" data="5/5.svg" type="image/svg+xml" id="pdf5" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_5" class="t s0_5">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_5" class="t v0_5 s1_5" data-mappings='[[39,"ffi"]]'>network latencies and bandwidths are suﬃcient to support the commonly used synchronous training </span>
<span id="t3_5" class="t s1_5">paradigm, exploiting model parallelism within superpods and data-parallelism across superpods. </span>
<span id="t4_5" class="t v1_5 s1_5">The ‘single controller’ programming model of Jax (</span><span id="t5_5" class="t v1_5 s2_5">Bradbury et al.</span><span id="t6_5" class="t v1_5 s1_5">, </span><span id="t7_5" class="t v1_5 s2_5">2018</span><span id="t8_5" class="t v1_5 s1_5">) and Pathways (</span><span id="t9_5" class="t v1_5 s2_5">Barham </span>
<span id="ta_5" class="t v2_5 s2_5">et al.</span><span id="tb_5" class="t v2_5 s1_5">, </span><span id="tc_5" class="t v2_5 s2_5">2022</span><span id="td_5" class="t v2_5 s1_5">) allows a single Python process to orchestrate the entire training run, dramatically </span>
<span id="te_5" class="t v3_5 s1_5" data-mappings='[[32,"fl"]]'>simplifying the development workﬂow. The GSPMD partitioner (</span><span id="tf_5" class="t v3_5 s2_5">Xu et al.</span><span id="tg_5" class="t v3_5 s1_5">, </span><span id="th_5" class="t v3_5 s2_5">2021</span><span id="ti_5" class="t v3_5 s1_5">) in the XLA compiler </span>
<span id="tj_5" class="t v4_5 s1_5">partitions the training step computation, and the MegaScale XLA compiler (</span><span id="tk_5" class="t v4_5 s2_5">XLA</span><span id="tl_5" class="t v4_5 s1_5">, </span><span id="tm_5" class="t v4_5 s2_5">2019</span><span id="tn_5" class="t v4_5 s1_5">) pass statically </span>
<span id="to_5" class="t v4_5 s1_5">schedules appropriate collectives so that they maximally overlap with the computation with very little </span>
<span id="tp_5" class="t s1_5">variation in step time. </span>
<span id="tq_5" class="t v2_5 s1_5">Maintaining a high goodput </span>
<span id="tr_5" class="t s3_5">3 </span>
<span id="ts_5" class="t v2_5 s1_5">at this scale would have been impossible using the conventional </span>
<span id="tt_5" class="t v5_5 s1_5">approach of periodic checkpointing of weights to persistent cluster storage. For Gemini, we instead </span>
<span id="tu_5" class="t v4_5 s1_5">made use of redundant in-memory copies of the model state, and on any unplanned hardware failures, </span>
<span id="tv_5" class="t s1_5">we rapidly recover directly from an intact model replica. Compared to both PaLM and PaLM-2 (</span><span id="tw_5" class="t s2_5">Anil </span>
<span id="tx_5" class="t v2_5 s2_5">et al.</span><span id="ty_5" class="t v2_5 s1_5">, </span><span id="tz_5" class="t v2_5 s2_5">2023</span><span id="t10_5" class="t v2_5 s1_5" data-mappings='[[74,"fi"]]'>), this provided a substantial speedup in recovery time, despite the signiﬁcantly larger </span>
<span id="t11_5" class="t v2_5 s1_5">training resources being used. As a result, the overall goodput for the largest-scale training job </span>
<span id="t12_5" class="t s1_5">increased from 85% to 97%. </span>
<span id="t13_5" class="t s1_5">Training at unprecedented scale invariably surfaces new and interesting systems failure modes - </span>
<span id="t14_5" class="t v4_5 s1_5">and in this instance one of the problems that we needed to address was that of “Silent Data Corruption </span>
<span id="t15_5" class="t v6_5 s1_5">(SDC)” (</span><span id="t16_5" class="t v6_5 s2_5">Dixit et al.</span><span id="t17_5" class="t v6_5 s1_5">, </span><span id="t18_5" class="t v6_5 s2_5">2021</span><span id="t19_5" class="t v6_5 s1_5">; </span><span id="t1a_5" class="t v6_5 s2_5">Hochschild et al.</span><span id="t1b_5" class="t v6_5 s1_5">, </span><span id="t1c_5" class="t v6_5 s2_5">2021</span><span id="t1d_5" class="t v6_5 s1_5">; </span><span id="t1e_5" class="t v6_5 s2_5">Vishwanathan et al.</span><span id="t1f_5" class="t v6_5 s1_5">, </span><span id="t1g_5" class="t v6_5 s2_5">2015</span><span id="t1h_5" class="t v6_5 s1_5">). Although these are </span>
<span id="t1i_5" class="t v7_5 s1_5">extremely rare, the scale of Gemini means that we can expect SDC events to impact training every </span>
<span id="t1j_5" class="t v2_5 s1_5">week or two. Rapidly detecting and removing faulty hardware required several new techniques </span>
<span id="t1k_5" class="t v2_5 s1_5">that exploit deterministic replay to isolate incorrect computations, combined with proactive SDC </span>
<span id="t1l_5" class="t v2_5 s1_5">scanners on idle machines and hot standbys. Our fully deterministic infrastructure allowed us to </span>
<span id="t1m_5" class="t v8_5 s1_5">quickly identify root causes (including hardware failures) during the development leading up to the </span>
<span id="t1n_5" class="t s1_5">Ultra model, and this was a crucial ingredient towards stable training. </span>
<span id="t1o_5" class="t s4_5">4. Training Dataset </span>
<span id="t1p_5" class="t v9_5 s1_5">Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining </span>
<span id="t1q_5" class="t v10_5 s1_5">dataset uses data from web documents, books, and code, and includes image, audio, and video data. </span>
<span id="t1r_5" class="t v2_5 s1_5">We use the SentencePiece tokenizer (</span><span id="t1s_5" class="t v2_5 s2_5">Kudo and Richardson</span><span id="t1t_5" class="t v2_5 s1_5">, </span><span id="t1u_5" class="t v2_5 s2_5">2018</span><span id="t1v_5" class="t v2_5 s1_5" data-mappings='[[6,"fi"]]'>) and ﬁnd that training the </span>
<span id="t1w_5" class="t v2_5 s1_5">tokenizer on a large sample of the entire training corpus improves the inferred vocabulary and </span>
<span id="t1x_5" class="t v2_5 s1_5" data-mappings='[[57,"fi"],[80,"ffi"]]'>subsequently improves model performance. For example, we ﬁnd Gemini models can eﬃciently </span>
<span id="t1y_5" class="t v10_5 s1_5" data-mappings='[[51,"fi"]]'>tokenize non-Latin scripts which can, in turn, beneﬁt model quality as well as training and inference </span>
<span id="t1z_5" class="t s1_5">speed. </span>
<span id="t20_5" class="t s1_5">The number of tokens used to train the largest models were determined following the approach </span>
<span id="t21_5" class="t s1_5">in </span><span id="t22_5" class="t s2_5" data-mappings='[[2,"ff"]]'>Hoﬀmann et al. </span><span id="t23_5" class="t s1_5">(</span><span id="t24_5" class="t s2_5">2022</span><span id="t25_5" class="t s1_5" data-mappings='[[43,"fi"]]'>). The smaller models are trained for signiﬁcantly more tokens to improve </span>
<span id="t26_5" class="t v4_5 s1_5">performance for a given inference budget, similar to the approach advocated in </span><span id="t27_5" class="t v4_5 s2_5">Touvron et al. </span><span id="t28_5" class="t v4_5 s1_5">(</span><span id="t29_5" class="t v4_5 s2_5">2023a</span><span id="t2a_5" class="t v4_5 s1_5">). </span>
<span id="t2b_5" class="t v2_5 s1_5" data-mappings='[[17,"fi"],[90,"fi"]]'>We apply quality ﬁlters to all datasets, using both heuristic rules and model-based classiﬁers. </span>
<span id="t2c_5" class="t v2_5 s1_5" data-mappings='[[23,"fi"],[62,"fi"]]'>We also perform safety ﬁltering to remove harmful content. We ﬁlter our evaluation sets from our </span>
<span id="t2d_5" class="t s1_5" data-mappings='[[21,"fi"]]'>training corpus. The ﬁnal data mixtures and weights were determined through ablations on smaller </span>
<span id="t2e_5" class="t v4_5 s1_5">models. We stage training to alter the mixture composition during training – increasing the weight of </span>
<span id="t2f_5" class="t v2_5 s1_5" data-mappings='[[53,"fi"]]'>domain-relevant data towards the end of training. We ﬁnd that data quality is critical to a highly- </span>
<span id="t2g_5" class="t v11_5 s1_5" data-mappings='[[76,"fi"]]'>performing model, and believe that many interesting questions remain around ﬁnding the optimal </span>
<span id="t2h_5" class="t s1_5">dataset distribution for pretraining. </span>
<span id="t2i_5" class="t s5_5">3 </span>
<span id="t2j_5" class="t s6_5" data-mappings='[[5,"fi"]]'>We deﬁne goodput as the time spent computing useful new steps over the elapsed time of the training job. </span>
<span id="t2k_5" class="t s0_5">5 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
