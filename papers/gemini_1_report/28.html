<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p28" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_28{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_28{left:112px;bottom:1132px;letter-spacing:-0.2px;word-spacing:0.04px;}
#t3_28{left:112px;bottom:1111px;letter-spacing:-0.15px;}
#t4_28{left:132px;bottom:1111px;letter-spacing:-0.21px;word-spacing:1.29px;}
#t5_28{left:641px;bottom:1111px;letter-spacing:-0.17px;word-spacing:2.42px;}
#t6_28{left:112px;bottom:1090px;letter-spacing:0.2px;}
#t7_28{left:469px;bottom:1090px;}
#t8_28{left:95px;bottom:1056px;letter-spacing:-0.26px;word-spacing:2.23px;}
#t9_28{left:773px;bottom:1056px;letter-spacing:-0.1px;}
#ta_28{left:112px;bottom:1035px;letter-spacing:-0.18px;word-spacing:0.63px;}
#tb_28{left:413px;bottom:1035px;letter-spacing:-0.1px;}
#tc_28{left:469px;bottom:1035px;letter-spacing:-0.16px;word-spacing:1.52px;}
#td_28{left:561px;bottom:1035px;letter-spacing:0.18px;}
#te_28{left:111px;bottom:1014px;letter-spacing:0.18px;}
#tf_28{left:159px;bottom:1015px;}
#tg_28{left:95px;bottom:980px;letter-spacing:-0.22px;word-spacing:2.34px;}
#th_28{left:112px;bottom:960px;letter-spacing:-0.16px;word-spacing:0.49px;}
#ti_28{left:501px;bottom:960px;letter-spacing:-0.32px;word-spacing:0.78px;}
#tj_28{left:731px;bottom:960px;letter-spacing:-0.18px;word-spacing:1.45px;}
#tk_28{left:110px;bottom:939px;letter-spacing:-0.14px;word-spacing:2.23px;}
#tl_28{left:333px;bottom:938px;letter-spacing:0.2px;}
#tm_28{left:652px;bottom:939px;}
#tn_28{left:95px;bottom:904px;letter-spacing:-0.22px;word-spacing:0.68px;}
#to_28{left:112px;bottom:884px;letter-spacing:-0.21px;word-spacing:2.36px;}
#tp_28{left:112px;bottom:863px;letter-spacing:-0.3px;word-spacing:1.71px;}
#tq_28{left:112px;bottom:842px;letter-spacing:-0.21px;word-spacing:0.62px;}
#tr_28{left:597px;bottom:842px;letter-spacing:-0.22px;word-spacing:0.56px;}
#ts_28{left:112px;bottom:822px;letter-spacing:-0.18px;word-spacing:1.65px;}
#tt_28{left:325px;bottom:822px;letter-spacing:-0.16px;word-spacing:3.91px;}
#tu_28{left:739px;bottom:821px;letter-spacing:0.18px;}
#tv_28{left:112px;bottom:800px;letter-spacing:0.21px;}
#tw_28{left:353px;bottom:801px;}
#tx_28{left:95px;bottom:767px;letter-spacing:-0.22px;word-spacing:0.83px;}
#ty_28{left:112px;bottom:746px;letter-spacing:-0.18px;word-spacing:0.63px;}
#tz_28{left:502px;bottom:746px;letter-spacing:-0.19px;word-spacing:0.33px;}
#t10_28{left:112px;bottom:725px;letter-spacing:-0.17px;word-spacing:0.12px;}
#t11_28{left:284px;bottom:725px;letter-spacing:-0.2px;word-spacing:-0.41px;}
#t12_28{left:112px;bottom:704px;letter-spacing:-0.16px;}
#t13_28{left:208px;bottom:704px;letter-spacing:-0.15px;word-spacing:5.06px;}
#t14_28{left:556px;bottom:704px;letter-spacing:-0.21px;}
#t15_28{left:595px;bottom:704px;letter-spacing:0.18px;}
#t16_28{left:112px;bottom:683px;letter-spacing:0.2px;}
#t17_28{left:430px;bottom:684px;}
#t18_28{left:95px;bottom:649px;letter-spacing:-0.23px;word-spacing:1.27px;}
#t19_28{left:518px;bottom:649px;letter-spacing:0.18px;}
#t1a_28{left:112px;bottom:628px;letter-spacing:0.19px;}
#t1b_28{left:613px;bottom:629px;}
#t1c_28{left:95px;bottom:594px;letter-spacing:-0.24px;word-spacing:0.7px;}
#t1d_28{left:567px;bottom:594px;letter-spacing:-0.12px;}
#t1e_28{left:613px;bottom:594px;letter-spacing:-0.16px;word-spacing:0.35px;}
#t1f_28{left:95px;bottom:560px;letter-spacing:-0.24px;word-spacing:1.6px;}
#t1g_28{left:112px;bottom:539px;letter-spacing:-0.21px;word-spacing:0.69px;}
#t1h_28{left:111px;bottom:518px;letter-spacing:-0.18px;word-spacing:0.52px;}
#t1i_28{left:233px;bottom:518px;letter-spacing:-0.2px;}
#t1j_28{left:283px;bottom:518px;letter-spacing:-0.16px;word-spacing:0.47px;}
#t1k_28{left:95px;bottom:484px;letter-spacing:-0.2px;word-spacing:0.7px;}
#t1l_28{left:112px;bottom:463px;letter-spacing:-0.18px;word-spacing:0.54px;}
#t1m_28{left:455px;bottom:463px;letter-spacing:-0.16px;word-spacing:0.48px;}
#t1n_28{left:689px;bottom:463px;letter-spacing:-0.15px;word-spacing:0.47px;}
#t1o_28{left:95px;bottom:429px;letter-spacing:-0.16px;word-spacing:0.77px;}
#t1p_28{left:112px;bottom:408px;letter-spacing:-0.18px;word-spacing:0.56px;}
#t1q_28{left:112px;bottom:388px;letter-spacing:-0.15px;}
#t1r_28{left:131px;bottom:388px;letter-spacing:-0.2px;word-spacing:0.59px;}
#t1s_28{left:112px;bottom:367px;letter-spacing:-0.2px;word-spacing:0.95px;}
#t1t_28{left:812px;bottom:367px;}
#t1u_28{left:111px;bottom:346px;letter-spacing:-0.17px;}
#t1v_28{left:95px;bottom:312px;letter-spacing:-0.16px;word-spacing:0.45px;}
#t1w_28{left:110px;bottom:291px;letter-spacing:-0.24px;word-spacing:1.12px;}
#t1x_28{left:112px;bottom:270px;letter-spacing:-0.21px;word-spacing:0.6px;}
#t1y_28{left:388px;bottom:270px;letter-spacing:-0.16px;word-spacing:0.48px;}
#t1z_28{left:622px;bottom:270px;letter-spacing:-0.15px;word-spacing:0.47px;}
#t20_28{left:95px;bottom:236px;letter-spacing:-0.29px;word-spacing:0.77px;}
#t21_28{left:112px;bottom:215px;letter-spacing:-0.21px;word-spacing:0.79px;}
#t22_28{left:619px;bottom:215px;letter-spacing:-0.27px;word-spacing:0.71px;}
#t23_28{left:728px;bottom:215px;letter-spacing:-0.15px;word-spacing:0.47px;}
#t24_28{left:95px;bottom:181px;letter-spacing:-0.27px;word-spacing:1.19px;}
#t25_28{left:112px;bottom:160px;letter-spacing:-0.15px;word-spacing:2.27px;}
#t26_28{left:193px;bottom:160px;letter-spacing:-0.19px;word-spacing:0.54px;}
#t27_28{left:764px;bottom:160px;letter-spacing:-0.14px;word-spacing:0.45px;}
#t28_28{left:111px;bottom:139px;letter-spacing:-0.17px;word-spacing:0.47px;}
#t29_28{left:801px;bottom:68px;letter-spacing:0.1px;}

.s0_28{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_28{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s2_28{font-size:17px;font-family:XCharter-Italic_6r;color:#000;}
.s3_28{font-size:18px;font-family:LMMono10-Regular_6t;color:#00F;}
.t.v0_28{transform:scaleX(0.979);}
.t.v1_28{transform:scaleX(1.02);}
.t.v2_28{transform:scaleX(1.011);}
.t.v3_28{transform:scaleX(1.008);}
.t.v4_28{transform:scaleX(1.014);}
.t.v5_28{transform:scaleX(1.005);}
.t.v6_28{transform:scaleX(1.01);}
.t.v7_28{transform:scaleX(1.015);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts28" type="text/css" >

@font-face {
	font-family: LMMono10-Regular_6t;
	src: url("fonts/LMMono10-Regular_6t.woff") format("woff");
}

@font-face {
	font-family: XCharter-Italic_6r;
	src: url("fonts/XCharter-Italic_6r.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg28Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg28" style="-webkit-user-select: none;"><object width="909" height="1286" data="28/28.svg" type="image/svg+xml" id="pdf28" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_28" class="t s0_28">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_28" class="t v0_28 s1_28">Martin Popel, and Maja Popović. Findings of the 2022 conference on machine translation (WMT22). </span>
<span id="t3_28" class="t v1_28 s1_28">In </span><span id="t4_28" class="t v1_28 s2_28">Proceedings of the Seventh Conference on Machine Translation (WMT)</span><span id="t5_28" class="t v1_28 s1_28">, December 2022. URL </span>
<span id="t6_28" class="t s3_28">https://aclanthology.org/2022.wmt-1.1</span><span id="t7_28" class="t s1_28">. </span>
<span id="t8_28" class="t v1_28 s1_28">Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. </span><span id="t9_28" class="t v1_28 s1_28">Large </span>
<span id="ta_28" class="t v0_28 s1_28">language models are zero-shot reasoners. </span><span id="tb_28" class="t v0_28 s2_28">NeurIPS</span><span id="tc_28" class="t v0_28 s1_28">, 2022. URL </span><span id="td_28" class="t s3_28">https://arxiv.org/abs/2205. </span>
<span id="te_28" class="t s3_28">11916</span><span id="tf_28" class="t s1_28">. </span>
<span id="tg_28" class="t v1_28 s1_28">Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword </span>
<span id="th_28" class="t s1_28">tokenizer and detokenizer for neural text processing. </span><span id="ti_28" class="t s2_28">EMNLP (System Demonstrations)</span><span id="tj_28" class="t s1_28">, 2018. doi: </span>
<span id="tk_28" class="t s1_28">10.18653/v1/D18-2012. URL </span><span id="tl_28" class="t s3_28">https://aclanthology.org/D18-2012</span><span id="tm_28" class="t s1_28">. </span>
<span id="tn_28" class="t v0_28 s1_28" data-mappings='[[48,"fi"]]'>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, </span>
<span id="to_28" class="t v1_28 s1_28">Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, </span>
<span id="tp_28" class="t v1_28 s1_28">Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. </span>
<span id="tq_28" class="t v2_28 s1_28">Natural questions: A benchmark for question answering research. </span><span id="tr_28" class="t v2_28 s2_28">Transactions of the Association </span>
<span id="ts_28" class="t v1_28 s2_28">for Computational Linguistics</span><span id="tt_28" class="t v1_28 s1_28">, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL </span><span id="tu_28" class="t s3_28">https:// </span>
<span id="tv_28" class="t s3_28">aclanthology.org/Q19-1026</span><span id="tw_28" class="t s1_28">. </span>
<span id="tx_28" class="t s1_28">Faisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: A new benchmark </span>
<span id="ty_28" class="t v0_28 s1_28">dataset for cross-lingual abstractive summarization. In </span><span id="tz_28" class="t v0_28 s2_28">Findings of the Association for Computational </span>
<span id="t10_28" class="t v0_28 s2_28">Linguistics: EMNLP 2020</span><span id="t11_28" class="t v0_28 s1_28">, pages 4034–4048, Online, November 2020. Association for Computational </span>
<span id="t12_28" class="t v1_28 s1_28">Linguistics. </span><span id="t13_28" class="t v1_28 s1_28" data-mappings='[[22,"fi"]]'>doi: 10.18653/v1/2020.ﬁndings-emnlp.360. </span><span id="t14_28" class="t v1_28 s1_28">URL </span><span id="t15_28" class="t s3_28">https://www.aclweb.org/ </span>
<span id="t16_28" class="t s3_28">anthology/2020.findings-emnlp.360</span><span id="t17_28" class="t s1_28">. </span>
<span id="t18_28" class="t v3_28 s1_28">Leblond et al. AlphaCode 2 Technical Report. 2023. URL </span><span id="t19_28" class="t s3_28">https://storage.googleapis.com/ </span>
<span id="t1a_28" class="t s3_28">deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf</span><span id="t1b_28" class="t s1_28">. </span>
<span id="t1c_28" class="t v0_28 s1_28" data-mappings='[[34,"ff"]]'>Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. </span><span id="t1d_28" class="t v0_28 s2_28">nature</span><span id="t1e_28" class="t v0_28 s1_28">, 521(7553):436–444, 2015. </span>
<span id="t1f_28" class="t v1_28 s1_28">Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom </span>
<span id="t1g_28" class="t v4_28 s1_28">Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation </span>
<span id="t1h_28" class="t s1_28">with alphacode. </span><span id="t1i_28" class="t s2_28">Science</span><span id="t1j_28" class="t s1_28">, 378(6624):1092–1097, 2022. </span>
<span id="t1k_28" class="t v5_28 s1_28">Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual </span>
<span id="t1l_28" class="t s1_28">representation by alignment before projection. </span><span id="t1m_28" class="t s2_28">arXiv preprint arXiv:2311.10122</span><span id="t1n_28" class="t s1_28">, 2023. </span>
<span id="t1o_28" class="t v1_28 s1_28">Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. </span>
<span id="t1p_28" class="t s1_28">Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. </span>
<span id="t1q_28" class="t s1_28">In </span><span id="t1r_28" class="t s2_28">The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics </span>
<span id="t1s_28" class="t v1_28 s2_28">and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)</span><span id="t1t_28" class="t v1_28 s1_28">, </span>
<span id="t1u_28" class="t s1_28">2021. </span>
<span id="t1v_28" class="t v6_28 s1_28">Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai- </span>
<span id="t1w_28" class="t v1_28 s1_28">Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of </span>
<span id="t1x_28" class="t s1_28">foundation models in visual contexts. </span><span id="t1y_28" class="t s2_28">arXiv preprint arXiv:2310.02255</span><span id="t1z_28" class="t s1_28">, 2023. </span>
<span id="t20_28" class="t v7_28 s1_28" data-mappings='[[39,"fi"]]'>Ahmed Masry, Do Long, Jia Qing Tan, Shaﬁq Joty, and Enamul Hoque. ChartQA: A benchmark for </span>
<span id="t21_28" class="t s1_28">question answering about charts with visual and logical reasoning. In </span><span id="t22_28" class="t s2_28">Findings of ACL</span><span id="t23_28" class="t s1_28">, 2022. </span>
<span id="t24_28" class="t v1_28 s1_28">Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document </span>
<span id="t25_28" class="t s1_28">images. In </span><span id="t26_28" class="t s2_28">Proceedings of the IEEE/CVF winter conference on applications of computer vision</span><span id="t27_28" class="t s1_28">, pages </span>
<span id="t28_28" class="t s1_28">2200–2209, 2021. </span>
<span id="t29_28" class="t s0_28">28 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
