<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p4" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_4{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_4{left:95px;bottom:830px;letter-spacing:-0.15px;word-spacing:-0.3px;}
#t3_4{left:158px;bottom:830px;}
#t4_4{left:166px;bottom:830px;letter-spacing:-0.19px;word-spacing:-0.16px;}
#t5_4{left:95px;bottom:810px;letter-spacing:-0.19px;word-spacing:0.61px;}
#t6_4{left:95px;bottom:789px;letter-spacing:-0.14px;word-spacing:0.47px;}
#t7_4{left:95px;bottom:735px;letter-spacing:-0.16px;word-spacing:0.5px;}
#t8_4{left:94px;bottom:715px;letter-spacing:-0.23px;word-spacing:0.68px;}
#t9_4{left:420px;bottom:715px;letter-spacing:-0.16px;word-spacing:0.57px;}
#ta_4{left:506px;bottom:715px;}
#tb_4{left:515px;bottom:715px;letter-spacing:-0.19px;}
#tc_4{left:552px;bottom:715px;letter-spacing:-0.21px;word-spacing:0.81px;}
#td_4{left:95px;bottom:694px;letter-spacing:-0.19px;word-spacing:0.52px;}
#te_4{left:95px;bottom:673px;letter-spacing:-0.19px;word-spacing:0.53px;}
#tf_4{left:374px;bottom:673px;letter-spacing:-0.23px;}
#tg_4{left:429px;bottom:673px;letter-spacing:-0.12px;}
#th_4{left:121px;bottom:642px;letter-spacing:-0.19px;word-spacing:0.5px;}
#ti_4{left:95px;bottom:622px;letter-spacing:-0.19px;word-spacing:-0.18px;}
#tj_4{left:95px;bottom:601px;letter-spacing:-0.2px;word-spacing:0.63px;}
#tk_4{left:95px;bottom:580px;letter-spacing:-0.2px;word-spacing:0.55px;}
#tl_4{left:95px;bottom:559px;letter-spacing:-0.18px;word-spacing:0.51px;}
#tm_4{left:95px;bottom:539px;letter-spacing:-0.22px;word-spacing:0.59px;}
#tn_4{left:95px;bottom:482px;letter-spacing:-0.13px;word-spacing:2.95px;}
#to_4{left:95px;bottom:444px;letter-spacing:-0.24px;word-spacing:0.63px;}
#tp_4{left:486px;bottom:444px;letter-spacing:-0.24px;word-spacing:0.68px;}
#tq_4{left:575px;bottom:444px;}
#tr_4{left:584px;bottom:444px;letter-spacing:-0.19px;}
#ts_4{left:621px;bottom:444px;letter-spacing:-0.14px;word-spacing:0.47px;}
#tt_4{left:95px;bottom:424px;letter-spacing:-0.2px;word-spacing:0.78px;}
#tu_4{left:95px;bottom:403px;letter-spacing:-0.2px;word-spacing:1.53px;}
#tv_4{left:95px;bottom:382px;letter-spacing:-0.18px;word-spacing:0.55px;}
#tw_4{left:95px;bottom:362px;letter-spacing:-0.2px;word-spacing:1.52px;}
#tx_4{left:95px;bottom:341px;letter-spacing:-0.19px;word-spacing:2.33px;}
#ty_4{left:95px;bottom:320px;letter-spacing:-0.19px;word-spacing:0.48px;}
#tz_4{left:95px;bottom:299px;letter-spacing:-0.26px;word-spacing:0.69px;}
#t10_4{left:190px;bottom:299px;letter-spacing:-0.23px;word-spacing:0.65px;}
#t11_4{left:296px;bottom:299px;}
#t12_4{left:305px;bottom:299px;letter-spacing:-0.19px;}
#t13_4{left:342px;bottom:299px;letter-spacing:-0.12px;}
#t14_4{left:121px;bottom:268px;letter-spacing:-0.19px;word-spacing:0.43px;}
#t15_4{left:95px;bottom:248px;letter-spacing:-0.17px;word-spacing:0.13px;}
#t16_4{left:95px;bottom:227px;letter-spacing:-0.18px;word-spacing:0.49px;}
#t17_4{left:259px;bottom:227px;letter-spacing:-0.24px;word-spacing:0.64px;}
#t18_4{left:346px;bottom:227px;}
#t19_4{left:355px;bottom:227px;letter-spacing:-0.19px;}
#t1a_4{left:391px;bottom:227px;letter-spacing:-0.21px;word-spacing:0.65px;}
#t1b_4{left:95px;bottom:206px;letter-spacing:-0.19px;word-spacing:0.55px;}
#t1c_4{left:121px;bottom:175px;letter-spacing:-0.2px;word-spacing:2.25px;}
#t1d_4{left:95px;bottom:155px;letter-spacing:-0.18px;word-spacing:0.49px;}
#t1e_4{left:95px;bottom:134px;letter-spacing:-0.15px;word-spacing:0.39px;}
#t1f_4{left:256px;bottom:134px;letter-spacing:-0.25px;word-spacing:0.62px;}
#t1g_4{left:367px;bottom:134px;}
#t1h_4{left:376px;bottom:134px;letter-spacing:-0.19px;}
#t1i_4{left:413px;bottom:134px;}
#t1j_4{left:422px;bottom:134px;letter-spacing:-0.3px;word-spacing:0.71px;}
#t1k_4{left:530px;bottom:134px;}
#t1l_4{left:539px;bottom:134px;letter-spacing:-0.19px;}
#t1m_4{left:575px;bottom:134px;}
#t1n_4{left:585px;bottom:134px;letter-spacing:-0.24px;word-spacing:0.57px;}
#t1o_4{left:692px;bottom:134px;}
#t1p_4{left:701px;bottom:134px;letter-spacing:-0.19px;}
#t1q_4{left:738px;bottom:134px;letter-spacing:-0.16px;word-spacing:1.33px;}
#t1r_4{left:807px;bottom:68px;}

.s0_4{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_4{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s2_4{font-size:17px;font-family:txsys_6y;color:#000;}
.s3_4{font-size:17px;font-family:XCharter-Roman_vh;color:#00F;}
.s4_4{font-size:20px;font-family:XCharter-Bold_vg;color:#000;}
.t.v0_4{transform:scaleX(0.979);}
.t.v1_4{transform:scaleX(0.987);}
.t.v2_4{transform:scaleX(1.011);}
.t.v3_4{transform:scaleX(0.99);}
.t.v4_4{transform:scaleX(1.02);}
.t.v5_4{transform:scaleX(1.015);}
.t.v6_4{transform:scaleX(1.019);}
.t.v7_4{transform:scaleX(1.005);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts4" type="text/css" >

@font-face {
	font-family: XCharter-Bold_vg;
	src: url("fonts/XCharter-Bold_vg.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

@font-face {
	font-family: txsys_6y;
	src: url("fonts/txsys_6y.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg4Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg4" style="-webkit-user-select: none;"><object width="909" height="1286" data="4/4.svg" type="image/svg+xml" id="pdf4" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_4" class="t s0_4">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_4" class="t v0_4 s1_4">Figure 2 </span><span id="t3_4" class="t s2_4">| </span><span id="t4_4" class="t v0_4 s1_4">Gemini supports interleaved sequences of text, image, audio, and video as inputs (illustrated </span>
<span id="t5_4" class="t s1_4" data-mappings='[[15,"ff"]]'>by tokens of diﬀerent colors in the input sequence). It can output responses with interleaved image </span>
<span id="t6_4" class="t s1_4">and text. </span>
<span id="t7_4" class="t v0_4 s1_4" data-mappings='[[19,"fi"]]'>tasks that require ﬁne-grained understanding. In addition, Gemini can directly ingest audio signals at </span>
<span id="t8_4" class="t v1_4 s1_4">16kHz from Universal Speech Model (USM) (</span><span id="t9_4" class="t v1_4 s3_4">Zhang et al.</span><span id="ta_4" class="t v1_4 s1_4">, </span><span id="tb_4" class="t v1_4 s3_4">2023</span><span id="tc_4" class="t v1_4 s1_4">) features. This enables the model to </span>
<span id="td_4" class="t v0_4 s1_4">capture nuances that are typically lost when the audio is naively mapped to a text input (for example, </span>
<span id="te_4" class="t s1_4">see audio understanding demo on the </span><span id="tf_4" class="t s3_4">website</span><span id="tg_4" class="t s1_4">). </span>
<span id="th_4" class="t v2_4 s1_4">Training the Gemini family of models required innovations in training algorithms, dataset, and </span>
<span id="ti_4" class="t v0_4 s1_4">infrastructure. For the Pro model, the inherent scalability of our infrastructure and learning algorithms </span>
<span id="tj_4" class="t v3_4 s1_4">enable us to complete pretraining in a matter of weeks, leveraging a fraction of the Ultra’s resources. </span>
<span id="tk_4" class="t s1_4">The Nano series of models leverage additional advancements in distillation and training algorithms </span>
<span id="tl_4" class="t v0_4 s1_4">to produce the best-in-class small language models for a wide variety of tasks, such as summarization </span>
<span id="tm_4" class="t s1_4">and reading comprehension, which power our next generation on-device experiences. </span>
<span id="tn_4" class="t s4_4">3. Training Infrastructure </span>
<span id="to_4" class="t s1_4">We trained Gemini models using TPUv5e and TPUv4 (</span><span id="tp_4" class="t s3_4">Jouppi et al.</span><span id="tq_4" class="t s1_4">, </span><span id="tr_4" class="t s3_4">2023</span><span id="ts_4" class="t s1_4">), depending on their sizes </span>
<span id="tt_4" class="t v4_4 s1_4" data-mappings='[[7,"fi"],[53,"fl"]]'>and conﬁguration. Training Gemini Ultra used a large ﬂeet of TPUv4 accelerators across multiple </span>
<span id="tu_4" class="t v4_4 s1_4" data-mappings='[[36,"fi"],[75,"fl"]]'>datacenters. This represents a signiﬁcant increase in scale over our prior ﬂagship model PaLM-2 </span>
<span id="tv_4" class="t v4_4 s1_4">which presented new infrastructure challenges. Scaling up the number of accelerators results in a </span>
<span id="tw_4" class="t v4_4 s1_4">proportionate decrease in the mean time between failure of hardware in the overall system. We </span>
<span id="tx_4" class="t v4_4 s1_4">minimized the rate of planned reschedules and preemptions, but genuine machine failures are </span>
<span id="ty_4" class="t v5_4 s1_4">commonplace across all hardware accelerators at such large scales, due to external factors such as </span>
<span id="tz_4" class="t s1_4">cosmic rays (</span><span id="t10_4" class="t s3_4">Michalak et al.</span><span id="t11_4" class="t s1_4">, </span><span id="t12_4" class="t s3_4">2012</span><span id="t13_4" class="t s1_4">). </span>
<span id="t14_4" class="t v6_4 s1_4">TPUv4 accelerators are deployed in “SuperPods” of 4096 chips, each connected to a dedicated </span>
<span id="t15_4" class="t v0_4 s1_4" data-mappings='[[43,"fi"]]'>optical switch, which can dynamically reconﬁgure 4x4x4 chip cubes into arbitrary 3D torus topologies </span>
<span id="t16_4" class="t v0_4 s1_4">in around 10 seconds (</span><span id="t17_4" class="t v0_4 s3_4">Jouppi et al.</span><span id="t18_4" class="t v0_4 s1_4">, </span><span id="t19_4" class="t v0_4 s3_4">2023</span><span id="t1a_4" class="t v0_4 s1_4">). For Gemini Ultra, we decided to retain a small number of </span>
<span id="t1b_4" class="t s1_4">cubes per superpod to allow for hot standbys and rolling maintenance. </span>
<span id="t1c_4" class="t v4_4 s1_4">TPU accelerators primarily communicate over the high speed inter-chip-interconnect, but at </span>
<span id="t1d_4" class="t v7_4 s1_4">Gemini Ultra scale, we combine SuperPods in multiple datacenters using Google’s intra-cluster and </span>
<span id="t1e_4" class="t v0_4 s1_4">inter-cluster network (</span><span id="t1f_4" class="t v0_4 s3_4">Poutievski et al.</span><span id="t1g_4" class="t v0_4 s1_4">, </span><span id="t1h_4" class="t v0_4 s3_4">2022</span><span id="t1i_4" class="t v0_4 s1_4">; </span><span id="t1j_4" class="t v0_4 s3_4">Wetherall et al.</span><span id="t1k_4" class="t v0_4 s1_4">, </span><span id="t1l_4" class="t v0_4 s3_4">2023</span><span id="t1m_4" class="t v0_4 s1_4">; </span><span id="t1n_4" class="t v0_4 s3_4">yao Hong et al.</span><span id="t1o_4" class="t v0_4 s1_4">, </span><span id="t1p_4" class="t v0_4 s3_4">2018</span><span id="t1q_4" class="t v0_4 s1_4">). Google’s </span>
<span id="t1r_4" class="t s0_4">4 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
