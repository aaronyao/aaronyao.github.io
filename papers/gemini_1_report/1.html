<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p1" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_1{left:95px;bottom:1118px;letter-spacing:-0.27px;word-spacing:2.12px;}
#t2_1{left:95px;bottom:1083px;letter-spacing:-0.12px;word-spacing:1.52px;}
#t3_1{left:95px;bottom:1044px;letter-spacing:-0.22px;word-spacing:0.69px;}
#t4_1{left:231px;bottom:1050px;}
#t5_1{left:95px;bottom:987px;letter-spacing:0.05px;word-spacing:0.8px;}
#t6_1{left:95px;bottom:965px;letter-spacing:0.09px;word-spacing:0.78px;}
#t7_1{left:95px;bottom:943px;letter-spacing:0.05px;word-spacing:0.78px;}
#t8_1{left:95px;bottom:921px;letter-spacing:0.02px;word-spacing:0.95px;}
#t9_1{left:95px;bottom:899px;letter-spacing:0.05px;word-spacing:0.79px;}
#ta_1{left:95px;bottom:877px;letter-spacing:0.07px;word-spacing:0.79px;}
#tb_1{left:95px;bottom:855px;letter-spacing:0.01px;word-spacing:0.92px;}
#tc_1{left:95px;bottom:833px;letter-spacing:0.08px;word-spacing:0.71px;}
#td_1{left:95px;bottom:811px;letter-spacing:0.04px;word-spacing:0.75px;}
#te_1{left:95px;bottom:749px;letter-spacing:-0.22px;word-spacing:5.19px;}
#tf_1{left:95px;bottom:711px;letter-spacing:-0.25px;word-spacing:0.73px;}
#tg_1{left:95px;bottom:691px;letter-spacing:-0.18px;word-spacing:0.18px;}
#th_1{left:95px;bottom:670px;letter-spacing:-0.19px;word-spacing:0.6px;}
#ti_1{left:95px;bottom:649px;letter-spacing:-0.2px;word-spacing:0.57px;}
#tj_1{left:121px;bottom:618px;letter-spacing:-0.18px;word-spacing:-0.23px;}
#tk_1{left:95px;bottom:597px;letter-spacing:-0.19px;word-spacing:0.69px;}
#tl_1{left:95px;bottom:577px;letter-spacing:-0.19px;word-spacing:0.54px;}
#tm_1{left:95px;bottom:556px;letter-spacing:-0.2px;word-spacing:0.45px;}
#tn_1{left:95px;bottom:535px;letter-spacing:-0.2px;word-spacing:0.57px;}
#to_1{left:121px;bottom:504px;letter-spacing:-0.17px;word-spacing:0.49px;}
#tp_1{left:602px;bottom:504px;letter-spacing:-0.18px;word-spacing:0.57px;}
#tq_1{left:671px;bottom:504px;}
#tr_1{left:680px;bottom:504px;letter-spacing:-0.19px;}
#ts_1{left:716px;bottom:504px;}
#tt_1{left:726px;bottom:504px;letter-spacing:-0.26px;word-spacing:0.69px;}
#tu_1{left:812px;bottom:504px;}
#tv_1{left:95px;bottom:484px;letter-spacing:-0.19px;}
#tw_1{left:133px;bottom:484px;}
#tx_1{left:143px;bottom:484px;letter-spacing:-0.29px;word-spacing:1.21px;}
#ty_1{left:269px;bottom:484px;}
#tz_1{left:279px;bottom:484px;letter-spacing:-0.19px;}
#t10_1{left:317px;bottom:484px;}
#t11_1{left:328px;bottom:484px;letter-spacing:-0.27px;word-spacing:1.17px;}
#t12_1{left:444px;bottom:484px;}
#t13_1{left:454px;bottom:484px;letter-spacing:-0.19px;}
#t14_1{left:492px;bottom:484px;}
#t15_1{left:503px;bottom:484px;letter-spacing:-0.18px;}
#t16_1{left:559px;bottom:484px;}
#t17_1{left:569px;bottom:484px;letter-spacing:-0.18px;}
#t18_1{left:615px;bottom:484px;}
#t19_1{left:626px;bottom:484px;letter-spacing:-0.21px;word-spacing:1.1px;}
#t1a_1{left:728px;bottom:484px;}
#t1b_1{left:738px;bottom:484px;letter-spacing:-0.19px;}
#t1c_1{left:776px;bottom:484px;}
#t1d_1{left:786px;bottom:484px;letter-spacing:-0.18px;}
#t1e_1{left:95px;bottom:463px;letter-spacing:-0.22px;word-spacing:0.68px;}
#t1f_1{left:131px;bottom:463px;}
#t1g_1{left:140px;bottom:463px;letter-spacing:-0.19px;}
#t1h_1{left:177px;bottom:463px;letter-spacing:-0.13px;word-spacing:0.43px;}
#t1i_1{left:358px;bottom:463px;letter-spacing:-0.18px;word-spacing:0.57px;}
#t1j_1{left:453px;bottom:463px;}
#t1k_1{left:462px;bottom:463px;letter-spacing:-0.19px;}
#t1l_1{left:499px;bottom:463px;}
#t1m_1{left:509px;bottom:463px;letter-spacing:-0.19px;word-spacing:0.58px;}
#t1n_1{left:586px;bottom:463px;}
#t1o_1{left:596px;bottom:463px;letter-spacing:-0.19px;}
#t1p_1{left:633px;bottom:463px;}
#t1q_1{left:642px;bottom:463px;letter-spacing:-0.22px;word-spacing:0.65px;}
#t1r_1{left:764px;bottom:463px;}
#t1s_1{left:774px;bottom:463px;letter-spacing:-0.19px;}
#t1t_1{left:810px;bottom:463px;}
#t1u_1{left:95px;bottom:442px;letter-spacing:-0.18px;}
#t1v_1{left:150px;bottom:442px;}
#t1w_1{left:159px;bottom:442px;letter-spacing:-0.18px;}
#t1x_1{left:205px;bottom:442px;}
#t1y_1{left:215px;bottom:442px;letter-spacing:-0.3px;word-spacing:0.79px;}
#t1z_1{left:292px;bottom:442px;}
#t20_1{left:301px;bottom:442px;letter-spacing:-0.19px;}
#t21_1{left:338px;bottom:442px;}
#t22_1{left:348px;bottom:442px;letter-spacing:-0.37px;word-spacing:0.95px;}
#t23_1{left:408px;bottom:442px;}
#t24_1{left:417px;bottom:442px;letter-spacing:-0.18px;}
#t25_1{left:463px;bottom:442px;letter-spacing:-0.18px;word-spacing:0.54px;}
#t26_1{left:612px;bottom:442px;letter-spacing:-0.22px;word-spacing:0.61px;}
#t27_1{left:712px;bottom:442px;}
#t28_1{left:721px;bottom:442px;letter-spacing:-0.19px;}
#t29_1{left:758px;bottom:442px;}
#t2a_1{left:768px;bottom:442px;letter-spacing:-0.13px;}
#t2b_1{left:95px;bottom:422px;letter-spacing:-0.22px;word-spacing:0.67px;}
#t2c_1{left:131px;bottom:422px;}
#t2d_1{left:140px;bottom:422px;letter-spacing:-0.19px;}
#t2e_1{left:177px;bottom:422px;letter-spacing:-0.14px;word-spacing:0.44px;}
#t2f_1{left:381px;bottom:422px;letter-spacing:-0.18px;word-spacing:0.56px;}
#t2g_1{left:475px;bottom:422px;}
#t2h_1{left:484px;bottom:422px;letter-spacing:-0.19px;}
#t2i_1{left:521px;bottom:422px;}
#t2j_1{left:531px;bottom:422px;letter-spacing:-0.19px;word-spacing:0.58px;}
#t2k_1{left:609px;bottom:422px;}
#t2l_1{left:618px;bottom:422px;letter-spacing:-0.19px;}
#t2m_1{left:655px;bottom:422px;letter-spacing:-0.21px;word-spacing:0.79px;}
#t2n_1{left:95px;bottom:401px;letter-spacing:-0.25px;word-spacing:1.1px;}
#t2o_1{left:298px;bottom:401px;letter-spacing:-0.22px;word-spacing:1.07px;}
#t2p_1{left:412px;bottom:401px;}
#t2q_1{left:422px;bottom:401px;letter-spacing:-0.19px;}
#t2r_1{left:459px;bottom:401px;letter-spacing:-0.21px;word-spacing:1.06px;}
#t2s_1{left:95px;bottom:380px;letter-spacing:-0.21px;word-spacing:0.54px;}
#t2t_1{left:248px;bottom:380px;letter-spacing:-0.13px;word-spacing:0.4px;}
#t2u_1{left:336px;bottom:380px;}
#t2v_1{left:345px;bottom:380px;letter-spacing:-0.19px;}
#t2w_1{left:382px;bottom:380px;letter-spacing:-0.18px;word-spacing:0.48px;}
#t2x_1{left:714px;bottom:380px;letter-spacing:-0.19px;word-spacing:0.5px;}
#t2y_1{left:812px;bottom:380px;}
#t2z_1{left:95px;bottom:359px;letter-spacing:-0.19px;}
#t30_1{left:132px;bottom:359px;}
#t31_1{left:142px;bottom:359px;letter-spacing:-0.29px;word-spacing:0.77px;}
#t32_1{left:249px;bottom:359px;}
#t33_1{left:258px;bottom:359px;letter-spacing:-0.19px;}
#t34_1{left:295px;bottom:359px;}
#t35_1{left:305px;bottom:359px;letter-spacing:-0.19px;word-spacing:0.57px;}
#t36_1{left:383px;bottom:359px;}
#t37_1{left:392px;bottom:359px;letter-spacing:-0.19px;}
#t38_1{left:429px;bottom:359px;letter-spacing:-0.17px;word-spacing:0.54px;}
#t39_1{left:121px;bottom:328px;letter-spacing:-0.21px;word-spacing:-0.6px;}
#t3a_1{left:95px;bottom:308px;letter-spacing:-0.18px;word-spacing:0.35px;}
#t3b_1{left:95px;bottom:287px;letter-spacing:-0.18px;word-spacing:1.34px;}
#t3c_1{left:95px;bottom:266px;letter-spacing:-0.19px;word-spacing:0.74px;}
#t3d_1{left:95px;bottom:246px;letter-spacing:-0.31px;word-spacing:0.71px;}
#t3e_1{left:155px;bottom:246px;letter-spacing:-0.19px;word-spacing:0.57px;}
#t3f_1{left:272px;bottom:246px;}
#t3g_1{left:281px;bottom:246px;letter-spacing:-0.18px;}
#t3h_1{left:327px;bottom:246px;letter-spacing:-0.2px;word-spacing:0.58px;}
#t3i_1{left:95px;bottom:225px;letter-spacing:-0.22px;word-spacing:0.83px;}
#t3j_1{left:95px;bottom:204px;letter-spacing:-0.22px;word-spacing:0.66px;}
#t3k_1{left:745px;bottom:204px;letter-spacing:-0.37px;word-spacing:0.91px;}
#t3l_1{left:812px;bottom:204px;}
#t3m_1{left:95px;bottom:183px;letter-spacing:-0.19px;}
#t3n_1{left:131px;bottom:183px;letter-spacing:-0.19px;word-spacing:0.65px;}
#t3o_1{left:113px;bottom:160px;}
#t3p_1{left:120px;bottom:153px;letter-spacing:-0.18px;word-spacing:2.65px;}
#t3q_1{left:560px;bottom:153px;letter-spacing:-0.18px;word-spacing:2.64px;}
#t3r_1{left:95px;bottom:137px;letter-spacing:-0.17px;}
#t3s_1{left:94px;bottom:68px;letter-spacing:0.06px;word-spacing:0.9px;}

.s0_1{font-size:32px;font-family:XCharter-Bold_vg;color:#000;}
.s1_1{font-size:14px;font-family:XCharter-Bold_vg;color:#000;}
.s2_1{font-size:11px;font-family:XCharter-Roman_vh;color:#00F;}
.s3_1{font-size:15px;font-family:XCharter-Bold_vg;color:#000;}
.s4_1{font-size:20px;font-family:XCharter-Bold_vg;color:#000;}
.s5_1{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s6_1{font-size:17px;font-family:XCharter-Roman_vh;color:#00F;}
.s7_1{font-size:11px;font-family:XCharter-Roman_vh;color:#000;}
.s8_1{font-size:14px;font-family:XCharter-Roman_vh;color:#000;}
.s9_1{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.t.v0_1{transform:scaleX(0.987);}
.t.v1_1{transform:scaleX(0.984);}
.t.v2_1{transform:scaleX(0.99);}
.t.v3_1{transform:scaleX(0.982);}
.t.v4_1{transform:scaleX(0.979);}
.t.v5_1{transform:scaleX(0.983);}
.t.v6_1{transform:scaleX(0.991);}
.t.v7_1{transform:scaleX(0.988);}
.t.v8_1{transform:scaleX(1.016);}
.t.v9_1{transform:scaleX(1.019);}
.t.v10_1{transform:scaleX(1.02);}
.t.v11_1{transform:scaleX(0.995);}
.t.v12_1{transform:scaleX(1.007);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts1" type="text/css" >

@font-face {
	font-family: XCharter-Bold_vg;
	src: url("fonts/XCharter-Bold_vg.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg1Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg1" style="-webkit-user-select: none;"><object width="909" height="1286" data="1/1.svg" type="image/svg+xml" id="pdf1" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_1" class="t s0_1">Gemini: A Family of Highly Capable </span>
<span id="t2_1" class="t s0_1">Multimodal Models </span>
<span id="t3_1" class="t s1_1">Gemini Team, Google </span>
<span id="t4_1" class="t s2_1">1 </span>
<span id="t5_1" class="t v0_1 s3_1">This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities </span>
<span id="t6_1" class="t v1_1 s3_1">across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano </span>
<span id="t7_1" class="t v2_1 s3_1">sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained </span>
<span id="t8_1" class="t v3_1 s3_1">use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model </span>
<span id="t9_1" class="t v4_1 s3_1" data-mappings='[[82,"fi"]]'>advances the state of the art in 30 of 32 of these benchmarks — notably being the ﬁrst model to achieve </span>
<span id="ta_1" class="t v5_1 s3_1">human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the </span>
<span id="tb_1" class="t v5_1 s3_1">art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of </span>
<span id="tc_1" class="t v6_1 s3_1">Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use </span>
<span id="td_1" class="t s3_1">cases and we discuss our approach toward deploying them responsibly to users. </span>
<span id="te_1" class="t s4_1">1. Introduction </span>
<span id="tf_1" class="t s5_1">We present Gemini, a family of highly capable multimodal models developed at Google. We trained </span>
<span id="tg_1" class="t v4_1 s5_1">Gemini jointly across image, audio, video, and text data for the purpose of building a model with both </span>
<span id="th_1" class="t s5_1">strong generalist capabilities across modalities alongside cutting-edge understanding and reasoning </span>
<span id="ti_1" class="t s5_1">performance in each respective domain. </span>
<span id="tj_1" class="t v4_1 s5_1" data-mappings='[[16,"fi"]]'>Gemini 1.0, our ﬁrst version, comes in three sizes: Ultra for highly-complex tasks, Pro for enhanced </span>
<span id="tk_1" class="t v7_1 s5_1" data-mappings='[[95,"fi"]]'>performance and deployability at scale, and Nano for on-device applications. Each size is speciﬁcally </span>
<span id="tl_1" class="t v8_1 s5_1" data-mappings='[[22,"ff"]]'>tailored to address diﬀerent computational limitations and application requirements. We evaluate </span>
<span id="tm_1" class="t v9_1 s5_1">the performance of Gemini models on a comprehensive suite of internal and external benchmarks </span>
<span id="tn_1" class="t s5_1">covering a wide range of language, coding, reasoning, and multimodal tasks. </span>
<span id="to_1" class="t v4_1 s5_1">Gemini advances state-of-the-art in large-scale language modeling (</span><span id="tp_1" class="t v4_1 s6_1">Anil et al.</span><span id="tq_1" class="t v4_1 s5_1">, </span><span id="tr_1" class="t v4_1 s6_1">2023</span><span id="ts_1" class="t v4_1 s5_1">; </span><span id="tt_1" class="t v4_1 s6_1">Brown et al.</span><span id="tu_1" class="t v4_1 s5_1">, </span>
<span id="tv_1" class="t v10_1 s6_1">2020</span><span id="tw_1" class="t v10_1 s5_1">; </span><span id="tx_1" class="t v10_1 s6_1">Chowdhery et al.</span><span id="ty_1" class="t v10_1 s5_1">, </span><span id="tz_1" class="t v10_1 s6_1">2023</span><span id="t10_1" class="t v10_1 s5_1">; </span><span id="t11_1" class="t v10_1 s6_1" data-mappings='[[2,"ff"]]'>Hoﬀmann et al.</span><span id="t12_1" class="t v10_1 s5_1">, </span><span id="t13_1" class="t v10_1 s6_1">2022</span><span id="t14_1" class="t v10_1 s5_1">; </span><span id="t15_1" class="t v10_1 s6_1">OpenAI</span><span id="t16_1" class="t v10_1 s5_1">, </span><span id="t17_1" class="t v10_1 s6_1">2023a</span><span id="t18_1" class="t v10_1 s5_1">; </span><span id="t19_1" class="t v10_1 s6_1">Radford et al.</span><span id="t1a_1" class="t v10_1 s5_1">, </span><span id="t1b_1" class="t v10_1 s6_1">2019</span><span id="t1c_1" class="t v10_1 s5_1">; </span><span id="t1d_1" class="t v10_1 s6_1">Rae </span>
<span id="t1e_1" class="t v11_1 s6_1">et al.</span><span id="t1f_1" class="t v11_1 s5_1">, </span><span id="t1g_1" class="t v11_1 s6_1">2021</span><span id="t1h_1" class="t v11_1 s5_1">), image understanding (</span><span id="t1i_1" class="t v11_1 s6_1">Alayrac et al.</span><span id="t1j_1" class="t v11_1 s5_1">, </span><span id="t1k_1" class="t v11_1 s6_1">2022</span><span id="t1l_1" class="t v11_1 s5_1">; </span><span id="t1m_1" class="t v11_1 s6_1">Chen et al.</span><span id="t1n_1" class="t v11_1 s5_1">, </span><span id="t1o_1" class="t v11_1 s6_1">2022</span><span id="t1p_1" class="t v11_1 s5_1">; </span><span id="t1q_1" class="t v11_1 s6_1">Dosovitskiy et al.</span><span id="t1r_1" class="t v11_1 s5_1">, </span><span id="t1s_1" class="t v11_1 s6_1">2020</span><span id="t1t_1" class="t v11_1 s5_1">; </span>
<span id="t1u_1" class="t s6_1">OpenAI</span><span id="t1v_1" class="t s5_1">, </span><span id="t1w_1" class="t s6_1">2023b</span><span id="t1x_1" class="t s5_1">; </span><span id="t1y_1" class="t s6_1">Reed et al.</span><span id="t1z_1" class="t s5_1">, </span><span id="t20_1" class="t s6_1">2022</span><span id="t21_1" class="t s5_1">; </span><span id="t22_1" class="t s6_1">Yu et al.</span><span id="t23_1" class="t s5_1">, </span><span id="t24_1" class="t s6_1">2022a</span><span id="t25_1" class="t s5_1">), audio processing (</span><span id="t26_1" class="t s6_1">Radford et al.</span><span id="t27_1" class="t s5_1">, </span><span id="t28_1" class="t s6_1">2023</span><span id="t29_1" class="t s5_1">; </span><span id="t2a_1" class="t s6_1">Zhang </span>
<span id="t2b_1" class="t v11_1 s6_1">et al.</span><span id="t2c_1" class="t v11_1 s5_1">, </span><span id="t2d_1" class="t v11_1 s6_1">2023</span><span id="t2e_1" class="t v11_1 s5_1">), and video understanding(</span><span id="t2f_1" class="t v11_1 s6_1">Alayrac et al.</span><span id="t2g_1" class="t v11_1 s5_1">, </span><span id="t2h_1" class="t v11_1 s6_1">2022</span><span id="t2i_1" class="t v11_1 s5_1">; </span><span id="t2j_1" class="t v11_1 s6_1">Chen et al.</span><span id="t2k_1" class="t v11_1 s5_1">, </span><span id="t2l_1" class="t v11_1 s6_1">2023</span><span id="t2m_1" class="t v11_1 s5_1">). It also builds on the </span>
<span id="t2n_1" class="t v10_1 s5_1">work on sequence models (</span><span id="t2o_1" class="t v10_1 s6_1">Sutskever et al.</span><span id="t2p_1" class="t v10_1 s5_1">, </span><span id="t2q_1" class="t v10_1 s6_1">2014</span><span id="t2r_1" class="t v10_1 s5_1">), a long history of work in deep learning based </span>
<span id="t2s_1" class="t v12_1 s5_1">on neural networks (</span><span id="t2t_1" class="t v12_1 s6_1">LeCun et al.</span><span id="t2u_1" class="t v12_1 s5_1">, </span><span id="t2v_1" class="t v12_1 s6_1">2015</span><span id="t2w_1" class="t v12_1 s5_1">), and machine learning distributed systems (</span><span id="t2x_1" class="t v12_1 s6_1">Barham et al.</span><span id="t2y_1" class="t v12_1 s5_1">, </span>
<span id="t2z_1" class="t s6_1">2022</span><span id="t30_1" class="t s5_1">; </span><span id="t31_1" class="t s6_1">Bradbury et al.</span><span id="t32_1" class="t s5_1">, </span><span id="t33_1" class="t s6_1">2018</span><span id="t34_1" class="t s5_1">; </span><span id="t35_1" class="t s6_1">Dean et al.</span><span id="t36_1" class="t s5_1">, </span><span id="t37_1" class="t s6_1">2012</span><span id="t38_1" class="t s5_1">) that enable large-scale training. </span>
<span id="t39_1" class="t v4_1 s5_1">Our most capable model, Gemini Ultra, achieves new state-of-the-art results in 30 of 32 benchmarks </span>
<span id="t3a_1" class="t v4_1 s5_1">we report on, including 10 of 12 popular text and reasoning benchmarks, 9 of 9 image understanding </span>
<span id="t3b_1" class="t v10_1 s5_1">benchmarks, 6 of 6 video understanding benchmarks, and 5 of 5 speech recognition and speech </span>
<span id="t3c_1" class="t v10_1 s5_1" data-mappings='[[44,"fi"]]'>translation benchmarks. Gemini Ultra is the ﬁrst model to achieve human-expert performance on </span>
<span id="t3d_1" class="t s5_1">MMLU (</span><span id="t3e_1" class="t s6_1">Hendrycks et al.</span><span id="t3f_1" class="t s5_1">, </span><span id="t3g_1" class="t s6_1">2021a</span><span id="t3h_1" class="t s5_1">) — a prominent benchmark testing knowledge and reasoning via a </span>
<span id="t3i_1" class="t v10_1 s5_1">suite of exams — with a score above 90%. Beyond text, Gemini Ultra makes notable advances on </span>
<span id="t3j_1" class="t s5_1">challenging multimodal reasoning tasks. For example, on the recent MMMU benchmark (</span><span id="t3k_1" class="t s6_1">Yue et al.</span><span id="t3l_1" class="t s5_1">, </span>
<span id="t3m_1" class="t v4_1 s6_1">2023</span><span id="t3n_1" class="t v4_1 s5_1">), that comprises questions about images on multi-discipline tasks requiring college-level subject </span>
<span id="t3o_1" class="t s7_1">1 </span>
<span id="t3p_1" class="t v10_1 s8_1">See Contributions and Acknowledgments section for full author list. </span><span id="t3q_1" class="t v10_1 s8_1">Please send correspondence to gemini-1- </span>
<span id="t3r_1" class="t s8_1">report@google.com </span>
<span id="t3s_1" class="t s9_1">© 2023 Google. All rights reserved </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
