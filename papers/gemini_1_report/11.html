<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p11" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_11{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_11{left:95px;bottom:1132px;letter-spacing:-0.22px;word-spacing:1.98px;}
#t3_11{left:95px;bottom:1098px;letter-spacing:-0.19px;word-spacing:0.02px;}
#t4_11{left:95px;bottom:1077px;letter-spacing:-0.22px;word-spacing:0.32px;}
#t5_11{left:95px;bottom:1056px;letter-spacing:-0.21px;word-spacing:0.69px;}
#t6_11{left:717px;bottom:1056px;letter-spacing:-0.17px;word-spacing:0.55px;}
#t7_11{left:812px;bottom:1056px;}
#t8_11{left:95px;bottom:1036px;letter-spacing:-0.19px;}
#t9_11{left:132px;bottom:1036px;letter-spacing:-0.19px;word-spacing:0.5px;}
#ta_11{left:608px;bottom:1036px;letter-spacing:-0.15px;}
#tb_11{left:645px;bottom:1036px;letter-spacing:-0.2px;word-spacing:0.92px;}
#tc_11{left:95px;bottom:1015px;letter-spacing:-0.23px;word-spacing:0.58px;}
#td_11{left:95px;bottom:994px;letter-spacing:-0.2px;word-spacing:0.66px;}
#te_11{left:95px;bottom:974px;letter-spacing:-0.18px;word-spacing:0.48px;}
#tf_11{left:95px;bottom:953px;letter-spacing:-0.14px;word-spacing:0.46px;}
#tg_11{left:121px;bottom:922px;letter-spacing:-0.21px;word-spacing:1.32px;}
#th_11{left:95px;bottom:901px;letter-spacing:-0.22px;word-spacing:0.54px;}
#ti_11{left:95px;bottom:880px;letter-spacing:-0.24px;word-spacing:1.01px;}
#tj_11{left:803px;bottom:880px;}
#tk_11{left:812px;bottom:880px;}
#tl_11{left:95px;bottom:860px;letter-spacing:-0.17px;word-spacing:0.52px;}
#tm_11{left:359px;bottom:821px;letter-spacing:-0.07px;}
#tn_11{left:438px;bottom:821px;letter-spacing:-0.25px;word-spacing:0.61px;}
#to_11{left:596px;bottom:821px;letter-spacing:-0.06px;}
#tp_11{left:275px;bottom:796px;letter-spacing:-0.17px;}
#tq_11{left:369px;bottom:796px;letter-spacing:-0.17px;}
#tr_11{left:482px;bottom:796px;letter-spacing:-0.17px;}
#ts_11{left:595px;bottom:796px;letter-spacing:-0.17px;}
#tt_11{left:263px;bottom:782px;letter-spacing:0.01px;word-spacing:0.57px;}
#tu_11{left:356px;bottom:782px;letter-spacing:0.07px;word-spacing:0.26px;}
#tv_11{left:469px;bottom:782px;letter-spacing:0.07px;word-spacing:0.26px;}
#tw_11{left:583px;bottom:782px;letter-spacing:0.07px;word-spacing:0.26px;}
#tx_11{left:110px;bottom:739px;letter-spacing:-0.71px;word-spacing:1.59px;}
#ty_11{left:167px;bottom:739px;}
#tz_11{left:176px;bottom:739px;letter-spacing:-0.21px;word-spacing:0.59px;}
#t10_11{left:95px;bottom:682px;letter-spacing:-0.24px;word-spacing:2.03px;}
#t11_11{left:95px;bottom:648px;letter-spacing:-0.2px;word-spacing:2.36px;}
#t12_11{left:95px;bottom:627px;letter-spacing:-0.21px;word-spacing:0.7px;}
#t13_11{left:95px;bottom:606px;letter-spacing:-0.17px;word-spacing:0.38px;}
#t14_11{left:95px;bottom:586px;letter-spacing:-0.24px;word-spacing:0.62px;}
#t15_11{left:175px;bottom:586px;letter-spacing:-0.24px;word-spacing:0.63px;}
#t16_11{left:271px;bottom:586px;}
#t17_11{left:280px;bottom:586px;letter-spacing:-0.19px;}
#t18_11{left:318px;bottom:586px;letter-spacing:-0.19px;word-spacing:0.61px;}
#t19_11{left:95px;bottom:565px;letter-spacing:-0.15px;word-spacing:0.68px;}
#t1a_11{left:530px;bottom:565px;letter-spacing:-0.2px;word-spacing:0.82px;}
#t1b_11{left:590px;bottom:565px;}
#t1c_11{left:597px;bottom:565px;letter-spacing:-0.19px;}
#t1d_11{left:635px;bottom:565px;letter-spacing:-0.29px;word-spacing:0.96px;}
#t1e_11{left:95px;bottom:544px;letter-spacing:-0.2px;word-spacing:0.85px;}
#t1f_11{left:95px;bottom:523px;letter-spacing:-0.18px;word-spacing:1.51px;}
#t1g_11{left:95px;bottom:503px;letter-spacing:-0.18px;word-spacing:0.43px;}
#t1h_11{left:95px;bottom:482px;letter-spacing:-0.16px;word-spacing:0.48px;}
#t1i_11{left:121px;bottom:451px;letter-spacing:-0.18px;word-spacing:0.55px;}
#t1j_11{left:416px;bottom:459px;}
#t1k_11{left:428px;bottom:451px;letter-spacing:-0.18px;word-spacing:0.56px;}
#t1l_11{left:95px;bottom:430px;letter-spacing:-0.15px;word-spacing:0.1px;}
#t1m_11{left:94px;bottom:410px;letter-spacing:-0.2px;word-spacing:0.33px;}
#t1n_11{left:95px;bottom:389px;letter-spacing:-0.18px;word-spacing:0.64px;}
#t1o_11{left:95px;bottom:368px;letter-spacing:-0.19px;word-spacing:0.69px;}
#t1p_11{left:95px;bottom:347px;letter-spacing:-0.21px;word-spacing:0.57px;}
#t1q_11{left:121px;bottom:316px;letter-spacing:-0.2px;word-spacing:1.21px;}
#t1r_11{left:95px;bottom:296px;letter-spacing:-0.18px;word-spacing:0.63px;}
#t1s_11{left:95px;bottom:275px;letter-spacing:-0.22px;word-spacing:0.62px;}
#t1t_11{left:113px;bottom:130px;}
#t1u_11{left:120px;bottom:123px;letter-spacing:0.04px;}
#t1v_11{left:801px;bottom:68px;letter-spacing:0.1px;}

.s0_11{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_11{font-size:17px;font-family:XCharter-BoldItalic_6q;color:#000;}
.s2_11{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s3_11{font-size:17px;font-family:XCharter-Roman_vh;color:#00F;}
.s4_11{font-size:14px;font-family:XCharter-Roman_vh;color:#000;}
.s5_11{font-size:9px;font-family:XCharter-Roman_vh;color:#000;}
.s6_11{font-size:17px;font-family:txsys_6y;color:#000;}
.s7_11{font-size:12px;font-family:XCharter-Roman_vh;color:#00F;}
.s8_11{font-size:11px;font-family:XCharter-Roman_vh;color:#000;}
.s9_11{font-size:15px;font-family:LMMono9-Regular_6v;color:#00F;}
.t.v0_11{transform:scaleX(0.979);}
.t.v1_11{transform:scaleX(1.013);}
.t.v2_11{transform:scaleX(1.02);}
.t.v3_11{transform:scaleX(0.994);}
.t.v4_11{transform:scaleX(1.011);}
.t.v5_11{transform:scaleX(0.985);}
.t.v6_11{transform:scaleX(1.018);}
.t.v7_11{transform:scaleX(0.993);}
.t.v8_11{transform:scaleX(0.983);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts11" type="text/css" >

@font-face {
	font-family: LMMono9-Regular_6v;
	src: url("fonts/LMMono9-Regular_6v.woff") format("woff");
}

@font-face {
	font-family: XCharter-BoldItalic_6q;
	src: url("fonts/XCharter-BoldItalic_6q.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

@font-face {
	font-family: txsys_6y;
	src: url("fonts/txsys_6y.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg11Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg11" style="-webkit-user-select: none;"><object width="909" height="1286" data="11/11.svg" type="image/svg+xml" id="pdf11" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_11" class="t s0_11">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_11" class="t s1_11">5.1.6. Human Preference Evaluations </span>
<span id="t3_11" class="t v0_11 s2_11">Human preference of the model outputs provides an important indication of quality that complements </span>
<span id="t4_11" class="t v0_11 s2_11">automated evaluations. We have evaluated the Gemini models in side-by-side blind evaluations where </span>
<span id="t5_11" class="t v0_11 s2_11">human raters judge responses of two models to the same prompt. We instruction tune (</span><span id="t6_11" class="t v0_11 s3_11">Ouyang et al.</span><span id="t7_11" class="t v0_11 s2_11">, </span>
<span id="t8_11" class="t v1_11 s3_11">2022</span><span id="t9_11" class="t v1_11 s2_11">) the pretrained model using techniques discussed in the section </span><span id="ta_11" class="t v1_11 s3_11">6.4.2</span><span id="tb_11" class="t v1_11 s2_11">. The instruction-tuned </span>
<span id="tc_11" class="t v2_11 s2_11" data-mappings='[[53,"fi"]]'>version of the model is evaluated on a range of speciﬁc capabilities, such as following instructions, </span>
<span id="td_11" class="t v3_11 s2_11">creative writing, multimodal understanding, long-context understanding, and safety. These capabili- </span>
<span id="te_11" class="t v1_11 s2_11">ties encompass a range of use cases inspired by current user needs and research-inspired potential </span>
<span id="tf_11" class="t s2_11">future use cases. </span>
<span id="tg_11" class="t v2_11 s2_11">Instruction-tuned Gemini Pro models provide a large improvement on a range of capabilities, </span>
<span id="th_11" class="t v4_11 s2_11">including preference for the Gemini Pro model over the PaLM 2 model API, 65.0% time in creative </span>
<span id="ti_11" class="t v2_11 s2_11">writing, 59.2% in following instructions, and 68.5% time for safer responses as shown in Table </span><span id="tj_11" class="t v2_11 s3_11">6</span><span id="tk_11" class="t v2_11 s2_11">. </span>
<span id="tl_11" class="t s2_11">These improvements directly translate into a more helpful and safer user experience. </span>
<span id="tm_11" class="t s4_11">Creativity </span><span id="tn_11" class="t s4_11">Instruction Following </span><span id="to_11" class="t s4_11">Safety </span>
<span id="tp_11" class="t s4_11">Win-rate </span><span id="tq_11" class="t s4_11">65.0% </span><span id="tr_11" class="t s4_11">59.2% </span><span id="ts_11" class="t s4_11">68.5% </span>
<span id="tt_11" class="t s5_11">95% Conf. Interval </span><span id="tu_11" class="t s5_11">[62.9%, 67.1%] </span><span id="tv_11" class="t s5_11">[57.6%, 60.8%] </span><span id="tw_11" class="t s5_11">[66.0%, 70.8%] </span>
<span id="tx_11" class="t s2_11">Table 6 </span><span id="ty_11" class="t s6_11">| </span><span id="tz_11" class="t s2_11" data-mappings='[[64,"fi"]]'>Win rate of Gemini Pro over PaLM 2 (text-bison@001) with 95% conﬁdence intervals. </span>
<span id="t10_11" class="t s1_11">5.1.7. Complex Reasoning Systems </span>
<span id="t11_11" class="t v2_11 s2_11">Gemini can also be combined with additional techniques such as search and tool-use to create </span>
<span id="t12_11" class="t v5_11 s2_11">powerful reasoning systems that can tackle more complex multi-step problems. One example of such </span>
<span id="t13_11" class="t v0_11 s2_11">a system is AlphaCode 2, a new state-of-the-art agent that excels at solving competitive programming </span>
<span id="t14_11" class="t v2_11 s2_11">problems (</span><span id="t15_11" class="t v2_11 s3_11">Leblond et al</span><span id="t16_11" class="t v2_11 s2_11">, </span><span id="t17_11" class="t v2_11 s3_11">2023</span><span id="t18_11" class="t v2_11 s2_11">). AlphaCode 2 uses a specialized version of Gemini Pro – tuned on </span>
<span id="t19_11" class="t v2_11 s2_11">competitive programming data similar to the data used in </span><span id="t1a_11" class="t v2_11 s3_11">Li et al. </span><span id="t1b_11" class="t v2_11 s2_11">(</span><span id="t1c_11" class="t v2_11 s3_11">2022</span><span id="t1d_11" class="t v2_11 s2_11">) – to conduct a massive </span>
<span id="t1e_11" class="t v2_11 s2_11" data-mappings='[[75,"fi"]]'>search over the space of possible programs. This is followed by a tailored ﬁltering, clustering and </span>
<span id="t1f_11" class="t v2_11 s2_11" data-mappings='[[35,"fi"]]'>reranking mechanism. Gemini Pro is ﬁne-tuned both to be a coding model to generate proposal </span>
<span id="t1g_11" class="t v6_11 s2_11">solution candidates, and to be a reward model that is leveraged to recognize and extract the most </span>
<span id="t1h_11" class="t s2_11">promising code candidates. </span>
<span id="t1i_11" class="t v7_11 s2_11">AlphaCode 2 is evaluated on Codeforces, </span>
<span id="t1j_11" class="t s7_11">5 </span>
<span id="t1k_11" class="t v7_11 s2_11">the same platform as AlphaCode, on 12 contests from </span>
<span id="t1l_11" class="t v0_11 s2_11">division 1 and 2, fora total of 77 problems. AlphaCode 2 solved 43% of these competition problems, a </span>
<span id="t1m_11" class="t v0_11 s2_11">1.7x improvement over the prior record-setting AlphaCode system which solved 25%. Mapping this to </span>
<span id="t1n_11" class="t v0_11 s2_11">competition rankings, AlphaCode 2 built on top of Gemini Pro sits at an estimated 85th percentile on </span>
<span id="t1o_11" class="t s2_11" data-mappings='[[71,"fi"]]'>average – i.e. it performs better than 85% of entrants. This is a signiﬁcant advance over AlphaCode, </span>
<span id="t1p_11" class="t s2_11">which only outperformed 50% of competitors. </span>
<span id="t1q_11" class="t v2_11 s2_11">The composition of powerful pretrained models with search and reasoning mechanisms is an </span>
<span id="t1r_11" class="t v8_11 s2_11">exciting direction towards more general agents; another key ingredient is deep understanding across </span>
<span id="t1s_11" class="t s2_11">a range of modalities which we discuss in the next section. </span>
<span id="t1t_11" class="t s8_11">5 </span>
<span id="t1u_11" class="t s9_11">http://codeforces.com/ </span>
<span id="t1v_11" class="t s0_11">11 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
