<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p3" style="overflow: hidden; position: relative; background-color: white; width: 909px; height: 1286px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_3{left:307px;bottom:1186px;letter-spacing:0.01px;word-spacing:0.57px;}
#t2_3{left:121px;bottom:1132px;letter-spacing:-0.22px;word-spacing:1.61px;}
#t3_3{left:95px;bottom:1111px;letter-spacing:-0.18px;word-spacing:1.86px;}
#t4_3{left:95px;bottom:1090px;letter-spacing:-0.19px;word-spacing:0.08px;}
#t5_3{left:95px;bottom:1070px;letter-spacing:-0.18px;word-spacing:0.55px;}
#t6_3{left:121px;bottom:1039px;letter-spacing:-0.21px;word-spacing:0.62px;}
#t7_3{left:95px;bottom:1018px;letter-spacing:-0.18px;word-spacing:1.7px;}
#t8_3{left:522px;bottom:1018px;letter-spacing:-0.23px;}
#t9_3{left:608px;bottom:1018px;letter-spacing:-0.31px;word-spacing:1.55px;}
#ta_3{left:95px;bottom:997px;letter-spacing:-0.2px;word-spacing:0.65px;}
#tb_3{left:95px;bottom:977px;letter-spacing:-0.21px;word-spacing:0.68px;}
#tc_3{left:95px;bottom:956px;letter-spacing:-0.21px;word-spacing:0.5px;}
#td_3{left:217px;bottom:956px;letter-spacing:-0.23px;word-spacing:0.55px;}
#te_3{left:379px;bottom:956px;}
#tf_3{left:384px;bottom:964px;}
#tg_3{left:396px;bottom:956px;letter-spacing:-0.19px;word-spacing:0.47px;}
#th_3{left:95px;bottom:935px;letter-spacing:-0.23px;word-spacing:0.81px;}
#ti_3{left:95px;bottom:914px;letter-spacing:-0.19px;word-spacing:0.54px;}
#tj_3{left:95px;bottom:894px;letter-spacing:-0.26px;word-spacing:0.69px;}
#tk_3{left:95px;bottom:837px;letter-spacing:-0.11px;word-spacing:2.89px;}
#tl_3{left:95px;bottom:799px;letter-spacing:-0.23px;word-spacing:0.63px;}
#tm_3{left:489px;bottom:799px;letter-spacing:-0.38px;word-spacing:0.95px;}
#tn_3{left:588px;bottom:799px;}
#to_3{left:598px;bottom:799px;letter-spacing:-0.19px;}
#tp_3{left:635px;bottom:799px;letter-spacing:-0.16px;word-spacing:0.5px;}
#tq_3{left:95px;bottom:779px;letter-spacing:-0.19px;word-spacing:0.18px;}
#tr_3{left:95px;bottom:758px;letter-spacing:-0.2px;word-spacing:2.25px;}
#ts_3{left:95px;bottom:737px;letter-spacing:-0.21px;word-spacing:0.69px;}
#tt_3{left:627px;bottom:737px;letter-spacing:-0.19px;}
#tu_3{left:684px;bottom:737px;}
#tv_3{left:693px;bottom:737px;letter-spacing:-0.19px;}
#tw_3{left:730px;bottom:737px;letter-spacing:-0.12px;word-spacing:0.83px;}
#tx_3{left:95px;bottom:717px;letter-spacing:-0.19px;word-spacing:0.61px;}
#ty_3{left:95px;bottom:696px;letter-spacing:-0.59px;word-spacing:1.37px;}
#tz_3{left:157px;bottom:696px;}
#t10_3{left:166px;bottom:696px;}
#t11_3{left:104px;bottom:657px;letter-spacing:-0.12px;word-spacing:0.6px;}
#t12_3{left:205px;bottom:657px;letter-spacing:-0.2px;word-spacing:0.76px;}
#t13_3{left:104px;bottom:628px;letter-spacing:-0.15px;}
#t14_3{left:205px;bottom:628px;letter-spacing:-0.22px;word-spacing:1.36px;}
#t15_3{left:205px;bottom:607px;letter-spacing:-0.21px;word-spacing:2.41px;}
#t16_3{left:776px;bottom:607px;letter-spacing:-0.11px;word-spacing:2.25px;}
#t17_3{left:205px;bottom:587px;letter-spacing:-0.21px;word-spacing:0.6px;}
#t18_3{left:104px;bottom:558px;letter-spacing:-0.16px;}
#t19_3{left:205px;bottom:558px;letter-spacing:-0.2px;word-spacing:1.69px;}
#t1a_3{left:205px;bottom:537px;letter-spacing:-0.19px;word-spacing:1.45px;}
#t1b_3{left:205px;bottom:516px;letter-spacing:-0.22px;word-spacing:0.61px;}
#t1c_3{left:104px;bottom:487px;letter-spacing:-0.35px;}
#t1d_3{left:205px;bottom:487px;letter-spacing:-0.24px;word-spacing:1.44px;}
#t1e_3{left:205px;bottom:466px;letter-spacing:-0.19px;word-spacing:0.55px;}
#t1f_3{left:205px;bottom:446px;letter-spacing:-0.21px;word-spacing:0.84px;}
#t1g_3{left:205px;bottom:425px;letter-spacing:-0.17px;word-spacing:0.51px;}
#t1h_3{left:95px;bottom:385px;letter-spacing:-0.71px;word-spacing:1.59px;}
#t1i_3{left:152px;bottom:385px;}
#t1j_3{left:161px;bottom:385px;letter-spacing:-0.29px;word-spacing:0.75px;}
#t1k_3{left:121px;bottom:340px;letter-spacing:-0.2px;word-spacing:0.5px;}
#t1l_3{left:95px;bottom:319px;letter-spacing:-0.18px;word-spacing:0.07px;}
#t1m_3{left:95px;bottom:298px;letter-spacing:-0.16px;word-spacing:0.53px;}
#t1n_3{left:350px;bottom:298px;}
#t1o_3{left:360px;bottom:298px;letter-spacing:-0.2px;word-spacing:0.7px;}
#t1p_3{left:95px;bottom:278px;letter-spacing:-0.23px;word-spacing:0.68px;}
#t1q_3{left:331px;bottom:278px;letter-spacing:-0.18px;word-spacing:0.62px;}
#t1r_3{left:424px;bottom:278px;}
#t1s_3{left:433px;bottom:278px;letter-spacing:-0.19px;}
#t1t_3{left:469px;bottom:278px;letter-spacing:-0.12px;word-spacing:0.47px;}
#t1u_3{left:534px;bottom:278px;letter-spacing:-0.38px;word-spacing:1.02px;}
#t1v_3{left:592px;bottom:278px;}
#t1w_3{left:601px;bottom:278px;letter-spacing:-0.18px;}
#t1x_3{left:646px;bottom:278px;letter-spacing:-0.25px;word-spacing:0.75px;}
#t1y_3{left:735px;bottom:278px;letter-spacing:-0.19px;word-spacing:0.62px;}
#t1z_3{left:812px;bottom:278px;}
#t20_3{left:95px;bottom:257px;letter-spacing:-0.19px;}
#t21_3{left:132px;bottom:257px;letter-spacing:-0.18px;word-spacing:0.45px;}
#t22_3{left:95px;bottom:236px;letter-spacing:-0.17px;word-spacing:0.5px;}
#t23_3{left:478px;bottom:236px;letter-spacing:-0.21px;word-spacing:0.58px;}
#t24_3{left:575px;bottom:236px;}
#t25_3{left:585px;bottom:236px;letter-spacing:-0.19px;}
#t26_3{left:622px;bottom:236px;}
#t27_3{left:632px;bottom:236px;letter-spacing:-0.37px;word-spacing:0.95px;}
#t28_3{left:691px;bottom:236px;}
#t29_3{left:700px;bottom:236px;letter-spacing:-0.18px;}
#t2a_3{left:746px;bottom:236px;letter-spacing:-0.12px;}
#t2b_3{left:121px;bottom:205px;letter-spacing:-0.17px;word-spacing:0.55px;}
#t2c_3{left:95px;bottom:185px;letter-spacing:-0.23px;word-spacing:0.57px;}
#t2d_3{left:95px;bottom:164px;letter-spacing:-0.22px;word-spacing:0.57px;}
#t2e_3{left:113px;bottom:140px;}
#t2f_3{left:120px;bottom:134px;letter-spacing:-0.17px;word-spacing:0.46px;}
#t2g_3{left:807px;bottom:68px;}

.s0_3{font-size:12px;font-family:XCharter-Roman_vh;color:#000;}
.s1_3{font-size:17px;font-family:XCharter-Roman_vh;color:#000;}
.s2_3{font-size:17px;font-family:XCharter-Italic_6r;color:#000;}
.s3_3{font-size:12px;font-family:XCharter-Roman_vh;color:#00F;}
.s4_3{font-size:20px;font-family:XCharter-Bold_vg;color:#000;}
.s5_3{font-size:17px;font-family:XCharter-Roman_vh;color:#00F;}
.s6_3{font-size:17px;font-family:XCharter-Bold_vg;color:#000;}
.s7_3{font-size:17px;font-family:txsys_6y;color:#000;}
.s8_3{font-size:11px;font-family:XCharter-Roman_vh;color:#000;}
.s9_3{font-size:14px;font-family:XCharter-Roman_vh;color:#000;}
.t.v0_3{transform:scaleX(1.02);}
.t.v1_3{transform:scaleX(0.979);}
.t.v2_3{transform:scaleX(0.983);}
.t.v3_3{transform:scaleX(1.011);}
.t.v4_3{transform:scaleX(0.98);}
.t.v5_3{transform:scaleX(0.993);}
.t.v6_3{transform:scaleX(0.991);}
.t.v7_3{transform:scaleX(0.987);}
.t.v8_3{transform:scaleX(1.015);}
.t.v9_3{transform:scaleX(0.992);}
.t.v10_3{transform:scaleX(1.016);}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts3" type="text/css" >

@font-face {
	font-family: XCharter-Bold_vg;
	src: url("fonts/XCharter-Bold_vg.woff") format("woff");
}

@font-face {
	font-family: XCharter-Italic_6r;
	src: url("fonts/XCharter-Italic_6r.woff") format("woff");
}

@font-face {
	font-family: XCharter-Roman_vh;
	src: url("fonts/XCharter-Roman_vh.woff") format("woff");
}

@font-face {
	font-family: txsys_6y;
	src: url("fonts/txsys_6y.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg3Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg3" style="-webkit-user-select: none;"><object width="909" height="1286" data="3/3.svg" type="image/svg+xml" id="pdf3" style="width:909px; height:1286px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_3" class="t s0_3">Gemini: A Family of Highly Capable Multimodal Models </span>
<span id="t2_3" class="t v0_3 s1_3" data-mappings='[[39,"ffi"]]'>In tandem, we advance the frontier of eﬃciency with Gemini Nano, a series of small models </span>
<span id="t3_3" class="t v0_3 s1_3">targeting on-device deployment. These models excel in on-device tasks, such as summarization, </span>
<span id="t4_3" class="t v1_3 s1_3">reading comprehension, text completion tasks, and exhibit impressive capabilities in reasoning, STEM, </span>
<span id="t5_3" class="t s1_3">coding, multimodal, and multilingual tasks relative to their sizes. </span>
<span id="t6_3" class="t s1_3" data-mappings='[[30,"fi"]]'>In the following sections, we ﬁrst provide an overview of the model architecture, training infras- </span>
<span id="t7_3" class="t v0_3 s1_3">tructure, and training dataset. We then present detailed </span><span id="t8_3" class="t v0_3 s2_3">evaluations </span><span id="t9_3" class="t v0_3 s1_3">of the Gemini model family, </span>
<span id="ta_3" class="t v2_3 s1_3">covering well-studied benchmarks and human-preference evaluations across text, code, image, audio </span>
<span id="tb_3" class="t s1_3">and video — which include both English performance and multilingual capabilities. We also discuss </span>
<span id="tc_3" class="t v3_3 s1_3">our approach to </span><span id="td_3" class="t v3_3 s2_3">responsible deployment</span><span id="te_3" class="t v3_3 s1_3">, </span>
<span id="tf_3" class="t s3_3">2 </span>
<span id="tg_3" class="t v3_3 s1_3">including our process for impact assessments, developing </span>
<span id="th_3" class="t v4_3 s1_3">model policies, evaluations, and mitigations of harm before deployment decisions. Finally, we discuss </span>
<span id="ti_3" class="t s1_3">the broader implications of Gemini, its limitations alongside its potential applications — paving the </span>
<span id="tj_3" class="t s1_3">way for a new era of research and innovation in AI. </span>
<span id="tk_3" class="t s4_3">2. Model Architecture </span>
<span id="tl_3" class="t s1_3">Gemini models build on top of Transformer decoders (</span><span id="tm_3" class="t s5_3">Vaswani et al.</span><span id="tn_3" class="t s1_3">, </span><span id="to_3" class="t s5_3">2017</span><span id="tp_3" class="t s1_3">) that are enhanced with </span>
<span id="tq_3" class="t v1_3 s1_3">improvements in architecture and model optimization to enable stable training at scale and optimized </span>
<span id="tr_3" class="t v0_3 s1_3">inference on Google’s Tensor Processing Units. They are trained to support 32k context length, </span>
<span id="ts_3" class="t v5_3 s1_3" data-mappings='[[11,"ffi"]]'>employing eﬃcient attention mechanisms (for e.g. multi-query attention (</span><span id="tt_3" class="t v5_3 s5_3">Shazeer</span><span id="tu_3" class="t v5_3 s1_3">, </span><span id="tv_3" class="t v5_3 s5_3">2019</span><span id="tw_3" class="t v5_3 s1_3" data-mappings='[[8,"fi"]]'>)). Our ﬁrst </span>
<span id="tx_3" class="t v6_3 s1_3">version, Gemini 1.0, comprises three main sizes to support a wide range of applications as discussed </span>
<span id="ty_3" class="t s1_3">in Table </span><span id="tz_3" class="t s5_3">1</span><span id="t10_3" class="t s1_3">. </span>
<span id="t11_3" class="t s6_3">Model size </span><span id="t12_3" class="t s6_3">Model description </span>
<span id="t13_3" class="t s1_3">Ultra </span><span id="t14_3" class="t v0_3 s1_3">Our most capable model that delivers state-of-the-art performance across a wide </span>
<span id="t15_3" class="t v0_3 s1_3">range of highly complex tasks, including reasoning and multimodal tasks. </span><span id="t16_3" class="t v0_3 s1_3">It is </span>
<span id="t17_3" class="t s1_3" data-mappings='[[1,"ffi"]]'>eﬃciently serveable at scale on TPU accelerators due to the Gemini architecture. </span>
<span id="t18_3" class="t s1_3">Pro </span><span id="t19_3" class="t v0_3 s1_3">A performance-optimized model in terms of cost as well as latency that delivers </span>
<span id="t1a_3" class="t v0_3 s1_3" data-mappings='[[5,"fi"]]'>signiﬁcant performance across a wide range of tasks. This model exhibits strong </span>
<span id="t1b_3" class="t s1_3">reasoning performance and broad multimodal capabilities. </span>
<span id="t1c_3" class="t s1_3">Nano </span><span id="t1d_3" class="t v0_3 s1_3" data-mappings='[[10,"ffi"]]'>Our most eﬃcient model, designed to run on-device. We trained two versions of </span>
<span id="t1e_3" class="t s1_3">Nano, with 1.8B (Nano-1) and 3.25B (Nano-2) parameters, targeting low and high </span>
<span id="t1f_3" class="t v2_3 s1_3">memory devices respectively. It is trained by distilling from larger Gemini models. It </span>
<span id="t1g_3" class="t s1_3">is 4-bit quantized for deployment and provides best-in-class performance. </span>
<span id="t1h_3" class="t s1_3">Table 1 </span><span id="t1i_3" class="t s7_3">| </span><span id="t1j_3" class="t s1_3">An overview of the Gemini 1.0 model family. </span>
<span id="t1k_3" class="t v1_3 s1_3">Gemini models are trained to accommodate textual input interleaved with a wide variety of audio </span>
<span id="t1l_3" class="t v1_3 s1_3">and visual inputs, such as natural images, charts, screenshots, PDFs, and videos, and they can produce </span>
<span id="t1m_3" class="t v7_3 s1_3">text and image outputs (see Figure </span><span id="t1n_3" class="t v7_3 s5_3">2</span><span id="t1o_3" class="t v7_3 s1_3">). The visual encoding of Gemini models is inspired by our own </span>
<span id="t1p_3" class="t v1_3 s1_3">foundational work on Flamingo (</span><span id="t1q_3" class="t v1_3 s5_3">Alayrac et al.</span><span id="t1r_3" class="t v1_3 s1_3">, </span><span id="t1s_3" class="t v1_3 s5_3">2022</span><span id="t1t_3" class="t v1_3 s1_3">), CoCa (</span><span id="t1u_3" class="t v1_3 s5_3">Yu et al.</span><span id="t1v_3" class="t v1_3 s1_3">, </span><span id="t1w_3" class="t v1_3 s5_3">2022a</span><span id="t1x_3" class="t v1_3 s1_3">), and PaLI (</span><span id="t1y_3" class="t v1_3 s5_3">Chen et al.</span><span id="t1z_3" class="t v1_3 s1_3">, </span>
<span id="t20_3" class="t v8_3 s5_3">2022</span><span id="t21_3" class="t v8_3 s1_3">), with the important distinction that the models are multimodal from the beginning and can </span>
<span id="t22_3" class="t s1_3">natively output images using discrete image tokens (</span><span id="t23_3" class="t s5_3">Ramesh et al.</span><span id="t24_3" class="t s1_3">, </span><span id="t25_3" class="t s5_3">2021</span><span id="t26_3" class="t s1_3">; </span><span id="t27_3" class="t s5_3">Yu et al.</span><span id="t28_3" class="t s1_3">, </span><span id="t29_3" class="t s5_3">2022b</span><span id="t2a_3" class="t s1_3">). </span>
<span id="t2b_3" class="t v9_3 s1_3">Video understanding is accomplished by encoding the video as a sequence of frames in the large </span>
<span id="t2c_3" class="t v1_3 s1_3">context window. Video frames or images can be interleaved naturally with text or audio as part of the </span>
<span id="t2d_3" class="t v10_3 s1_3">model input. The models can handle variable input resolution in order to spend more compute on </span>
<span id="t2e_3" class="t s8_3">2 </span>
<span id="t2f_3" class="t s9_3">We plan to update this report with more details ahead of the general availability of the Gemini Ultra model. </span>
<span id="t2g_3" class="t s0_3">3 </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
